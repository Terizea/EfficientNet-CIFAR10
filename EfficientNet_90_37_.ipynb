{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EfficientNet_90.37%",
      "provenance": [],
      "mount_file_id": "1hKKRBC21sp2l99M4Q2tzIK5svbeZv1Q0",
      "authorship_tag": "ABX9TyOoHyYm2qc0xMltQ1J5Das8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "764423620c4d45da88efeb861ed287f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab0be1cc16db4fc181efca4376185828",
              "IPY_MODEL_f366d024ff3544a8ae5e48edb512f5e5",
              "IPY_MODEL_1dca0807cca54161be7a5dc2b04cab6b"
            ],
            "layout": "IPY_MODEL_836bbdbeabda49ae9b56a40cb19ceee4"
          }
        },
        "ab0be1cc16db4fc181efca4376185828": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45b4cf64e82c44bda816a3906074db28",
            "placeholder": "​",
            "style": "IPY_MODEL_18a5b8f0ce394fd18999b25dff444e6c",
            "value": ""
          }
        },
        "f366d024ff3544a8ae5e48edb512f5e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b49ea332df3b404da0cc7fa34ec25d00",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_93e3b680c9784cf6b8d58eb9cb721612",
            "value": 170498071
          }
        },
        "1dca0807cca54161be7a5dc2b04cab6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f91672b852f0499e885a6390bc99cbac",
            "placeholder": "​",
            "style": "IPY_MODEL_7d1d07eac9a74926b65bb1d1eb581400",
            "value": " 170499072/? [00:03&lt;00:00, 46847950.61it/s]"
          }
        },
        "836bbdbeabda49ae9b56a40cb19ceee4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45b4cf64e82c44bda816a3906074db28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18a5b8f0ce394fd18999b25dff444e6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b49ea332df3b404da0cc7fa34ec25d00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93e3b680c9784cf6b8d58eb9cb721612": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f91672b852f0499e885a6390bc99cbac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d1d07eac9a74926b65bb1d1eb581400": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Terizea/EfficientNet-CIFAR10/blob/main/EfficientNet_90_37_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "764423620c4d45da88efeb861ed287f4",
            "ab0be1cc16db4fc181efca4376185828",
            "f366d024ff3544a8ae5e48edb512f5e5",
            "1dca0807cca54161be7a5dc2b04cab6b",
            "836bbdbeabda49ae9b56a40cb19ceee4",
            "45b4cf64e82c44bda816a3906074db28",
            "18a5b8f0ce394fd18999b25dff444e6c",
            "b49ea332df3b404da0cc7fa34ec25d00",
            "93e3b680c9784cf6b8d58eb9cb721612",
            "f91672b852f0499e885a6390bc99cbac",
            "7d1d07eac9a74926b65bb1d1eb581400"
          ]
        },
        "id": "8G1SVL3gptY6",
        "outputId": "2e327b5b-1079-40d2-d33e-435d72beada7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to drive/app/cifar10/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "764423620c4d45da88efeb861ed287f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting drive/app/cifar10/cifar-10-python.tar.gz to drive/app/cifar10/\n",
            "Files already downloaded and verified\n",
            "\n",
            "Epoch: 0\n",
            "train: [39/391] Loss: 2.265 | Acc: 21.406% (1096/5120)\n",
            "train: [79/391] Loss: 2.054 | Acc: 26.475% (2711/10240)\n",
            "train: [119/391] Loss: 1.948 | Acc: 29.688% (4560/15360)\n",
            "train: [159/391] Loss: 1.874 | Acc: 32.178% (6590/20480)\n",
            "train: [199/391] Loss: 1.821 | Acc: 34.070% (8722/25600)\n",
            "train: [239/391] Loss: 1.780 | Acc: 35.511% (10909/30720)\n",
            "train: [279/391] Loss: 1.745 | Acc: 36.749% (13171/35840)\n",
            "train: [319/391] Loss: 1.712 | Acc: 38.098% (15605/40960)\n",
            "train: [359/391] Loss: 1.681 | Acc: 39.191% (18059/46080)\n",
            "val: [39/79] Loss: 1.302 | Acc: 53.984% (2764/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 1\n",
            "train: [39/391] Loss: 1.385 | Acc: 49.414% (2530/5120)\n",
            "train: [79/391] Loss: 1.364 | Acc: 50.508% (5172/10240)\n",
            "train: [119/391] Loss: 1.358 | Acc: 50.944% (7825/15360)\n",
            "train: [159/391] Loss: 1.338 | Acc: 51.660% (10580/20480)\n",
            "train: [199/391] Loss: 1.320 | Acc: 52.445% (13426/25600)\n",
            "train: [239/391] Loss: 1.308 | Acc: 52.796% (16219/30720)\n",
            "train: [279/391] Loss: 1.294 | Acc: 53.351% (19121/35840)\n",
            "train: [319/391] Loss: 1.280 | Acc: 53.867% (22064/40960)\n",
            "train: [359/391] Loss: 1.268 | Acc: 54.329% (25035/46080)\n",
            "val: [39/79] Loss: 1.094 | Acc: 61.582% (3153/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 2\n",
            "train: [39/391] Loss: 1.132 | Acc: 58.809% (3011/5120)\n",
            "train: [79/391] Loss: 1.115 | Acc: 59.902% (6134/10240)\n",
            "train: [119/391] Loss: 1.107 | Acc: 60.417% (9280/15360)\n",
            "train: [159/391] Loss: 1.100 | Acc: 60.781% (12448/20480)\n",
            "train: [199/391] Loss: 1.095 | Acc: 61.047% (15628/25600)\n",
            "train: [239/391] Loss: 1.094 | Acc: 61.019% (18745/30720)\n",
            "train: [279/391] Loss: 1.086 | Acc: 61.356% (21990/35840)\n",
            "train: [319/391] Loss: 1.078 | Acc: 61.592% (25228/40960)\n",
            "train: [359/391] Loss: 1.070 | Acc: 61.866% (28508/46080)\n",
            "val: [39/79] Loss: 0.905 | Acc: 68.086% (3486/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 3\n",
            "train: [39/391] Loss: 1.006 | Acc: 64.219% (3288/5120)\n",
            "train: [79/391] Loss: 0.999 | Acc: 64.463% (6601/10240)\n",
            "train: [119/391] Loss: 0.987 | Acc: 64.837% (9959/15360)\n",
            "train: [159/391] Loss: 0.973 | Acc: 65.166% (13346/20480)\n",
            "train: [199/391] Loss: 0.968 | Acc: 65.207% (16693/25600)\n",
            "train: [239/391] Loss: 0.958 | Acc: 65.723% (20190/30720)\n",
            "train: [279/391] Loss: 0.955 | Acc: 65.932% (23630/35840)\n",
            "train: [319/391] Loss: 0.950 | Acc: 66.130% (27087/40960)\n",
            "train: [359/391] Loss: 0.944 | Acc: 66.404% (30599/46080)\n",
            "val: [39/79] Loss: 0.793 | Acc: 71.699% (3671/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 4\n",
            "train: [39/391] Loss: 0.881 | Acc: 68.066% (3485/5120)\n",
            "train: [79/391] Loss: 0.884 | Acc: 68.271% (6991/10240)\n",
            "train: [119/391] Loss: 0.873 | Acc: 68.874% (10579/15360)\n",
            "train: [159/391] Loss: 0.863 | Acc: 69.292% (14191/20480)\n",
            "train: [199/391] Loss: 0.857 | Acc: 69.484% (17788/25600)\n",
            "train: [239/391] Loss: 0.852 | Acc: 69.600% (21381/30720)\n",
            "train: [279/391] Loss: 0.850 | Acc: 69.696% (24979/35840)\n",
            "train: [319/391] Loss: 0.846 | Acc: 69.841% (28607/40960)\n",
            "train: [359/391] Loss: 0.844 | Acc: 70.022% (32266/46080)\n",
            "val: [39/79] Loss: 0.717 | Acc: 74.648% (3822/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 5\n",
            "train: [39/391] Loss: 0.796 | Acc: 71.934% (3683/5120)\n",
            "train: [79/391] Loss: 0.787 | Acc: 72.051% (7378/10240)\n",
            "train: [119/391] Loss: 0.781 | Acc: 72.454% (11129/15360)\n",
            "train: [159/391] Loss: 0.779 | Acc: 72.544% (14857/20480)\n",
            "train: [199/391] Loss: 0.776 | Acc: 72.762% (18627/25600)\n",
            "train: [239/391] Loss: 0.773 | Acc: 72.891% (22392/30720)\n",
            "train: [279/391] Loss: 0.770 | Acc: 72.966% (26151/35840)\n",
            "train: [319/391] Loss: 0.766 | Acc: 73.079% (29933/40960)\n",
            "train: [359/391] Loss: 0.766 | Acc: 73.077% (33674/46080)\n",
            "val: [39/79] Loss: 0.663 | Acc: 76.777% (3931/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 6\n",
            "train: [39/391] Loss: 0.714 | Acc: 75.254% (3853/5120)\n",
            "train: [79/391] Loss: 0.707 | Acc: 75.547% (7736/10240)\n",
            "train: [119/391] Loss: 0.698 | Acc: 75.658% (11621/15360)\n",
            "train: [159/391] Loss: 0.697 | Acc: 75.620% (15487/20480)\n",
            "train: [199/391] Loss: 0.696 | Acc: 75.680% (19374/25600)\n",
            "train: [239/391] Loss: 0.696 | Acc: 75.677% (23248/30720)\n",
            "train: [279/391] Loss: 0.696 | Acc: 75.614% (27100/35840)\n",
            "train: [319/391] Loss: 0.696 | Acc: 75.562% (30950/40960)\n",
            "train: [359/391] Loss: 0.693 | Acc: 75.684% (34875/46080)\n",
            "val: [39/79] Loss: 0.612 | Acc: 78.145% (4001/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 7\n",
            "train: [39/391] Loss: 0.644 | Acc: 78.008% (3994/5120)\n",
            "train: [79/391] Loss: 0.644 | Acc: 78.164% (8004/10240)\n",
            "train: [119/391] Loss: 0.643 | Acc: 77.923% (11969/15360)\n",
            "train: [159/391] Loss: 0.640 | Acc: 77.871% (15948/20480)\n",
            "train: [199/391] Loss: 0.636 | Acc: 77.852% (19930/25600)\n",
            "train: [239/391] Loss: 0.638 | Acc: 77.760% (23888/30720)\n",
            "train: [279/391] Loss: 0.639 | Acc: 77.653% (27831/35840)\n",
            "train: [319/391] Loss: 0.638 | Acc: 77.649% (31805/40960)\n",
            "train: [359/391] Loss: 0.639 | Acc: 77.611% (35763/46080)\n",
            "val: [39/79] Loss: 0.579 | Acc: 79.980% (4095/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 8\n",
            "train: [39/391] Loss: 0.618 | Acc: 77.832% (3985/5120)\n",
            "train: [79/391] Loss: 0.612 | Acc: 77.861% (7973/10240)\n",
            "train: [119/391] Loss: 0.611 | Acc: 78.060% (11990/15360)\n",
            "train: [159/391] Loss: 0.601 | Acc: 78.506% (16078/20480)\n",
            "train: [199/391] Loss: 0.601 | Acc: 78.555% (20110/25600)\n",
            "train: [239/391] Loss: 0.599 | Acc: 78.649% (24161/30720)\n",
            "train: [279/391] Loss: 0.601 | Acc: 78.574% (28161/35840)\n",
            "train: [319/391] Loss: 0.597 | Acc: 78.740% (32252/40960)\n",
            "train: [359/391] Loss: 0.593 | Acc: 78.863% (36340/46080)\n",
            "val: [39/79] Loss: 0.543 | Acc: 80.977% (4146/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 9\n",
            "train: [39/391] Loss: 0.557 | Acc: 80.469% (4120/5120)\n",
            "train: [79/391] Loss: 0.566 | Acc: 80.244% (8217/10240)\n",
            "train: [119/391] Loss: 0.570 | Acc: 80.039% (12294/15360)\n",
            "train: [159/391] Loss: 0.569 | Acc: 79.976% (16379/20480)\n",
            "train: [199/391] Loss: 0.567 | Acc: 80.180% (20526/25600)\n",
            "train: [239/391] Loss: 0.566 | Acc: 80.104% (24608/30720)\n",
            "train: [279/391] Loss: 0.566 | Acc: 80.140% (28722/35840)\n",
            "train: [319/391] Loss: 0.569 | Acc: 80.007% (32771/40960)\n",
            "train: [359/391] Loss: 0.571 | Acc: 79.970% (36850/46080)\n",
            "val: [39/79] Loss: 0.533 | Acc: 81.543% (4175/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 10\n",
            "train: [39/391] Loss: 0.720 | Acc: 74.863% (3833/5120)\n",
            "train: [79/391] Loss: 0.735 | Acc: 73.809% (7558/10240)\n",
            "train: [119/391] Loss: 0.741 | Acc: 73.691% (11319/15360)\n",
            "train: [159/391] Loss: 0.743 | Acc: 73.550% (15063/20480)\n",
            "train: [199/391] Loss: 0.750 | Acc: 73.414% (18794/25600)\n",
            "train: [239/391] Loss: 0.749 | Acc: 73.402% (22549/30720)\n",
            "train: [279/391] Loss: 0.751 | Acc: 73.309% (26274/35840)\n",
            "train: [319/391] Loss: 0.754 | Acc: 73.372% (30053/40960)\n",
            "train: [359/391] Loss: 0.752 | Acc: 73.457% (33849/46080)\n",
            "val: [39/79] Loss: 0.634 | Acc: 77.402% (3963/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 11\n",
            "train: [39/391] Loss: 0.708 | Acc: 75.625% (3872/5120)\n",
            "train: [79/391] Loss: 0.711 | Acc: 75.117% (7692/10240)\n",
            "train: [119/391] Loss: 0.716 | Acc: 74.980% (11517/15360)\n",
            "train: [159/391] Loss: 0.713 | Acc: 75.044% (15369/20480)\n",
            "train: [199/391] Loss: 0.706 | Acc: 75.367% (19294/25600)\n",
            "train: [239/391] Loss: 0.703 | Acc: 75.420% (23169/30720)\n",
            "train: [279/391] Loss: 0.700 | Acc: 75.497% (27058/35840)\n",
            "train: [319/391] Loss: 0.698 | Acc: 75.415% (30890/40960)\n",
            "train: [359/391] Loss: 0.699 | Acc: 75.399% (34744/46080)\n",
            "val: [39/79] Loss: 0.591 | Acc: 79.258% (4058/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 12\n",
            "train: [39/391] Loss: 0.644 | Acc: 77.539% (3970/5120)\n",
            "train: [79/391] Loss: 0.646 | Acc: 77.588% (7945/10240)\n",
            "train: [119/391] Loss: 0.658 | Acc: 77.090% (11841/15360)\n",
            "train: [159/391] Loss: 0.660 | Acc: 77.002% (15770/20480)\n",
            "train: [199/391] Loss: 0.654 | Acc: 77.117% (19742/25600)\n",
            "train: [239/391] Loss: 0.653 | Acc: 77.194% (23714/30720)\n",
            "train: [279/391] Loss: 0.652 | Acc: 77.165% (27656/35840)\n",
            "train: [319/391] Loss: 0.651 | Acc: 77.202% (31622/40960)\n",
            "train: [359/391] Loss: 0.648 | Acc: 77.313% (35626/46080)\n",
            "val: [39/79] Loss: 0.579 | Acc: 80.469% (4120/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 13\n",
            "train: [39/391] Loss: 0.590 | Acc: 79.355% (4063/5120)\n",
            "train: [79/391] Loss: 0.601 | Acc: 79.189% (8109/10240)\n",
            "train: [119/391] Loss: 0.609 | Acc: 78.867% (12114/15360)\n",
            "train: [159/391] Loss: 0.612 | Acc: 78.721% (16122/20480)\n",
            "train: [199/391] Loss: 0.613 | Acc: 78.574% (20115/25600)\n",
            "train: [239/391] Loss: 0.610 | Acc: 78.737% (24188/30720)\n",
            "train: [279/391] Loss: 0.607 | Acc: 78.756% (28226/35840)\n",
            "train: [319/391] Loss: 0.607 | Acc: 78.801% (32277/40960)\n",
            "train: [359/391] Loss: 0.605 | Acc: 78.817% (36319/46080)\n",
            "val: [39/79] Loss: 0.535 | Acc: 81.484% (4172/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 14\n",
            "train: [39/391] Loss: 0.561 | Acc: 80.508% (4122/5120)\n",
            "train: [79/391] Loss: 0.559 | Acc: 80.547% (8248/10240)\n",
            "train: [119/391] Loss: 0.557 | Acc: 80.684% (12393/15360)\n",
            "train: [159/391] Loss: 0.556 | Acc: 80.630% (16513/20480)\n",
            "train: [199/391] Loss: 0.556 | Acc: 80.500% (20608/25600)\n",
            "train: [239/391] Loss: 0.559 | Acc: 80.378% (24692/30720)\n",
            "train: [279/391] Loss: 0.558 | Acc: 80.455% (28835/35840)\n",
            "train: [319/391] Loss: 0.556 | Acc: 80.559% (32997/40960)\n",
            "train: [359/391] Loss: 0.555 | Acc: 80.560% (37122/46080)\n",
            "val: [39/79] Loss: 0.513 | Acc: 82.051% (4201/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 15\n",
            "train: [39/391] Loss: 0.538 | Acc: 80.840% (4139/5120)\n",
            "train: [79/391] Loss: 0.513 | Acc: 81.689% (8365/10240)\n",
            "train: [119/391] Loss: 0.521 | Acc: 81.615% (12536/15360)\n",
            "train: [159/391] Loss: 0.518 | Acc: 81.626% (16717/20480)\n",
            "train: [199/391] Loss: 0.520 | Acc: 81.637% (20899/25600)\n",
            "train: [239/391] Loss: 0.518 | Acc: 81.813% (25133/30720)\n",
            "train: [279/391] Loss: 0.515 | Acc: 81.895% (29351/35840)\n",
            "train: [319/391] Loss: 0.514 | Acc: 81.958% (33570/40960)\n",
            "train: [359/391] Loss: 0.516 | Acc: 81.910% (37744/46080)\n",
            "val: [39/79] Loss: 0.476 | Acc: 83.535% (4277/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 16\n",
            "train: [39/391] Loss: 0.487 | Acc: 83.027% (4251/5120)\n",
            "train: [79/391] Loss: 0.479 | Acc: 83.252% (8525/10240)\n",
            "train: [119/391] Loss: 0.480 | Acc: 83.333% (12800/15360)\n",
            "train: [159/391] Loss: 0.481 | Acc: 83.228% (17045/20480)\n",
            "train: [199/391] Loss: 0.475 | Acc: 83.477% (21370/25600)\n",
            "train: [239/391] Loss: 0.475 | Acc: 83.363% (25609/30720)\n",
            "train: [279/391] Loss: 0.479 | Acc: 83.292% (29852/35840)\n",
            "train: [319/391] Loss: 0.478 | Acc: 83.293% (34117/40960)\n",
            "train: [359/391] Loss: 0.477 | Acc: 83.377% (38420/46080)\n",
            "val: [39/79] Loss: 0.464 | Acc: 83.887% (4295/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 17\n",
            "train: [39/391] Loss: 0.450 | Acc: 83.535% (4277/5120)\n",
            "train: [79/391] Loss: 0.459 | Acc: 83.594% (8560/10240)\n",
            "train: [119/391] Loss: 0.446 | Acc: 84.329% (12953/15360)\n",
            "train: [159/391] Loss: 0.446 | Acc: 84.331% (17271/20480)\n",
            "train: [199/391] Loss: 0.446 | Acc: 84.340% (21591/25600)\n",
            "train: [239/391] Loss: 0.446 | Acc: 84.342% (25910/30720)\n",
            "train: [279/391] Loss: 0.445 | Acc: 84.358% (30234/35840)\n",
            "train: [319/391] Loss: 0.440 | Acc: 84.490% (34607/40960)\n",
            "train: [359/391] Loss: 0.440 | Acc: 84.518% (38946/46080)\n",
            "val: [39/79] Loss: 0.436 | Acc: 85.234% (4364/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 18\n",
            "train: [39/391] Loss: 0.410 | Acc: 86.035% (4405/5120)\n",
            "train: [79/391] Loss: 0.420 | Acc: 85.361% (8741/10240)\n",
            "train: [119/391] Loss: 0.417 | Acc: 85.371% (13113/15360)\n",
            "train: [159/391] Loss: 0.420 | Acc: 85.190% (17447/20480)\n",
            "train: [199/391] Loss: 0.416 | Acc: 85.336% (21846/25600)\n",
            "train: [239/391] Loss: 0.414 | Acc: 85.449% (26250/30720)\n",
            "train: [279/391] Loss: 0.413 | Acc: 85.522% (30651/35840)\n",
            "train: [319/391] Loss: 0.413 | Acc: 85.544% (35039/40960)\n",
            "train: [359/391] Loss: 0.409 | Acc: 85.658% (39471/46080)\n",
            "val: [39/79] Loss: 0.429 | Acc: 85.156% (4360/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 19\n",
            "train: [39/391] Loss: 0.403 | Acc: 85.996% (4403/5120)\n",
            "train: [79/391] Loss: 0.395 | Acc: 86.270% (8834/10240)\n",
            "train: [119/391] Loss: 0.394 | Acc: 86.178% (13237/15360)\n",
            "train: [159/391] Loss: 0.390 | Acc: 86.318% (17678/20480)\n",
            "train: [199/391] Loss: 0.389 | Acc: 86.316% (22097/25600)\n",
            "train: [239/391] Loss: 0.392 | Acc: 86.136% (26461/30720)\n",
            "train: [279/391] Loss: 0.390 | Acc: 86.186% (30889/35840)\n",
            "train: [319/391] Loss: 0.390 | Acc: 86.211% (35312/40960)\n",
            "train: [359/391] Loss: 0.391 | Acc: 86.207% (39724/46080)\n",
            "val: [39/79] Loss: 0.422 | Acc: 85.625% (4384/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 20\n",
            "train: [39/391] Loss: 0.517 | Acc: 81.582% (4177/5120)\n",
            "train: [79/391] Loss: 0.536 | Acc: 81.084% (8303/10240)\n",
            "train: [119/391] Loss: 0.542 | Acc: 80.970% (12437/15360)\n",
            "train: [159/391] Loss: 0.546 | Acc: 80.947% (16578/20480)\n",
            "train: [199/391] Loss: 0.550 | Acc: 80.820% (20690/25600)\n",
            "train: [239/391] Loss: 0.555 | Acc: 80.599% (24760/30720)\n",
            "train: [279/391] Loss: 0.559 | Acc: 80.488% (28847/35840)\n",
            "train: [319/391] Loss: 0.559 | Acc: 80.510% (32977/40960)\n",
            "train: [359/391] Loss: 0.560 | Acc: 80.497% (37093/46080)\n",
            "val: [39/79] Loss: 0.522 | Acc: 82.480% (4223/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 21\n",
            "train: [39/391] Loss: 0.522 | Acc: 81.367% (4166/5120)\n",
            "train: [79/391] Loss: 0.527 | Acc: 81.445% (8340/10240)\n",
            "train: [119/391] Loss: 0.528 | Acc: 81.556% (12527/15360)\n",
            "train: [159/391] Loss: 0.542 | Acc: 81.123% (16614/20480)\n",
            "train: [199/391] Loss: 0.557 | Acc: 80.586% (20630/25600)\n",
            "train: [239/391] Loss: 0.562 | Acc: 80.420% (24705/30720)\n",
            "train: [279/391] Loss: 0.558 | Acc: 80.547% (28868/35840)\n",
            "train: [319/391] Loss: 0.556 | Acc: 80.588% (33009/40960)\n",
            "train: [359/391] Loss: 0.554 | Acc: 80.640% (37159/46080)\n",
            "val: [39/79] Loss: 0.521 | Acc: 82.324% (4215/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 22\n",
            "train: [39/391] Loss: 0.505 | Acc: 82.578% (4228/5120)\n",
            "train: [79/391] Loss: 0.505 | Acc: 82.383% (8436/10240)\n",
            "train: [119/391] Loss: 0.499 | Acc: 82.604% (12688/15360)\n",
            "train: [159/391] Loss: 0.503 | Acc: 82.422% (16880/20480)\n",
            "train: [199/391] Loss: 0.499 | Acc: 82.465% (21111/25600)\n",
            "train: [239/391] Loss: 0.506 | Acc: 82.188% (25248/30720)\n",
            "train: [279/391] Loss: 0.506 | Acc: 82.196% (29459/35840)\n",
            "train: [319/391] Loss: 0.504 | Acc: 82.231% (33682/40960)\n",
            "train: [359/391] Loss: 0.504 | Acc: 82.270% (37910/46080)\n",
            "val: [39/79] Loss: 0.466 | Acc: 83.848% (4293/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 23\n",
            "train: [39/391] Loss: 0.482 | Acc: 82.852% (4242/5120)\n",
            "train: [79/391] Loss: 0.465 | Acc: 83.857% (8587/10240)\n",
            "train: [119/391] Loss: 0.468 | Acc: 83.711% (12858/15360)\n",
            "train: [159/391] Loss: 0.477 | Acc: 83.379% (17076/20480)\n",
            "train: [199/391] Loss: 0.480 | Acc: 83.297% (21324/25600)\n",
            "train: [239/391] Loss: 0.477 | Acc: 83.359% (25608/30720)\n",
            "train: [279/391] Loss: 0.477 | Acc: 83.318% (29861/35840)\n",
            "train: [319/391] Loss: 0.479 | Acc: 83.271% (34108/40960)\n",
            "train: [359/391] Loss: 0.479 | Acc: 83.331% (38399/46080)\n",
            "val: [39/79] Loss: 0.462 | Acc: 84.160% (4309/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 24\n",
            "train: [39/391] Loss: 0.440 | Acc: 84.727% (4338/5120)\n",
            "train: [79/391] Loss: 0.441 | Acc: 84.824% (8686/10240)\n",
            "train: [119/391] Loss: 0.440 | Acc: 84.837% (13031/15360)\n",
            "train: [159/391] Loss: 0.440 | Acc: 84.722% (17351/20480)\n",
            "train: [199/391] Loss: 0.442 | Acc: 84.621% (21663/25600)\n",
            "train: [239/391] Loss: 0.437 | Acc: 84.785% (26046/30720)\n",
            "train: [279/391] Loss: 0.443 | Acc: 84.481% (30278/35840)\n",
            "train: [319/391] Loss: 0.446 | Acc: 84.421% (34579/40960)\n",
            "train: [359/391] Loss: 0.447 | Acc: 84.382% (38883/46080)\n",
            "val: [39/79] Loss: 0.446 | Acc: 84.316% (4317/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 25\n",
            "train: [39/391] Loss: 0.407 | Acc: 85.938% (4400/5120)\n",
            "train: [79/391] Loss: 0.406 | Acc: 85.850% (8791/10240)\n",
            "train: [119/391] Loss: 0.393 | Acc: 86.276% (13252/15360)\n",
            "train: [159/391] Loss: 0.400 | Acc: 85.986% (17610/20480)\n",
            "train: [199/391] Loss: 0.397 | Acc: 86.062% (22032/25600)\n",
            "train: [239/391] Loss: 0.404 | Acc: 85.902% (26389/30720)\n",
            "train: [279/391] Loss: 0.404 | Acc: 85.868% (30775/35840)\n",
            "train: [319/391] Loss: 0.403 | Acc: 85.781% (35136/40960)\n",
            "train: [359/391] Loss: 0.406 | Acc: 85.712% (39496/46080)\n",
            "val: [39/79] Loss: 0.432 | Acc: 85.449% (4375/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 26\n",
            "train: [39/391] Loss: 0.386 | Acc: 86.211% (4414/5120)\n",
            "train: [79/391] Loss: 0.384 | Acc: 86.250% (8832/10240)\n",
            "train: [119/391] Loss: 0.388 | Acc: 86.341% (13262/15360)\n",
            "train: [159/391] Loss: 0.384 | Acc: 86.465% (17708/20480)\n",
            "train: [199/391] Loss: 0.381 | Acc: 86.590% (22167/25600)\n",
            "train: [239/391] Loss: 0.383 | Acc: 86.523% (26580/30720)\n",
            "train: [279/391] Loss: 0.380 | Acc: 86.627% (31047/35840)\n",
            "train: [319/391] Loss: 0.380 | Acc: 86.643% (35489/40960)\n",
            "train: [359/391] Loss: 0.382 | Acc: 86.584% (39898/46080)\n",
            "val: [39/79] Loss: 0.400 | Acc: 86.465% (4427/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 27\n",
            "train: [39/391] Loss: 0.351 | Acc: 87.949% (4503/5120)\n",
            "train: [79/391] Loss: 0.349 | Acc: 87.705% (8981/10240)\n",
            "train: [119/391] Loss: 0.348 | Acc: 87.819% (13489/15360)\n",
            "train: [159/391] Loss: 0.343 | Acc: 87.881% (17998/20480)\n",
            "train: [199/391] Loss: 0.346 | Acc: 87.781% (22472/25600)\n",
            "train: [239/391] Loss: 0.347 | Acc: 87.812% (26976/30720)\n",
            "train: [279/391] Loss: 0.347 | Acc: 87.829% (31478/35840)\n",
            "train: [319/391] Loss: 0.344 | Acc: 87.898% (36003/40960)\n",
            "train: [359/391] Loss: 0.343 | Acc: 87.969% (40536/46080)\n",
            "val: [39/79] Loss: 0.404 | Acc: 86.758% (4442/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 28\n",
            "train: [39/391] Loss: 0.331 | Acc: 88.477% (4530/5120)\n",
            "train: [79/391] Loss: 0.318 | Acc: 88.838% (9097/10240)\n",
            "train: [119/391] Loss: 0.310 | Acc: 89.069% (13681/15360)\n",
            "train: [159/391] Loss: 0.308 | Acc: 89.189% (18266/20480)\n",
            "train: [199/391] Loss: 0.314 | Acc: 88.992% (22782/25600)\n",
            "train: [239/391] Loss: 0.318 | Acc: 88.870% (27301/30720)\n",
            "train: [279/391] Loss: 0.320 | Acc: 88.828% (31836/35840)\n",
            "train: [319/391] Loss: 0.321 | Acc: 88.779% (36364/40960)\n",
            "train: [359/391] Loss: 0.321 | Acc: 88.785% (40912/46080)\n",
            "val: [39/79] Loss: 0.375 | Acc: 87.227% (4466/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 29\n",
            "train: [39/391] Loss: 0.311 | Acc: 88.867% (4550/5120)\n",
            "train: [79/391] Loss: 0.311 | Acc: 88.848% (9098/10240)\n",
            "train: [119/391] Loss: 0.303 | Acc: 89.342% (13723/15360)\n",
            "train: [159/391] Loss: 0.305 | Acc: 89.268% (18282/20480)\n",
            "train: [199/391] Loss: 0.304 | Acc: 89.234% (22844/25600)\n",
            "train: [239/391] Loss: 0.305 | Acc: 89.219% (27408/30720)\n",
            "train: [279/391] Loss: 0.303 | Acc: 89.289% (32001/35840)\n",
            "train: [319/391] Loss: 0.303 | Acc: 89.329% (36589/40960)\n",
            "train: [359/391] Loss: 0.305 | Acc: 89.156% (41083/46080)\n",
            "val: [39/79] Loss: 0.377 | Acc: 87.285% (4469/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 30\n",
            "train: [39/391] Loss: 0.399 | Acc: 85.820% (4394/5120)\n",
            "train: [79/391] Loss: 0.421 | Acc: 84.941% (8698/10240)\n",
            "train: [119/391] Loss: 0.440 | Acc: 84.401% (12964/15360)\n",
            "train: [159/391] Loss: 0.448 | Acc: 84.194% (17243/20480)\n",
            "train: [199/391] Loss: 0.447 | Acc: 84.250% (21568/25600)\n",
            "train: [239/391] Loss: 0.450 | Acc: 84.115% (25840/30720)\n",
            "train: [279/391] Loss: 0.455 | Acc: 84.040% (30120/35840)\n",
            "train: [319/391] Loss: 0.456 | Acc: 84.033% (34420/40960)\n",
            "train: [359/391] Loss: 0.460 | Acc: 83.947% (38683/46080)\n",
            "val: [39/79] Loss: 0.528 | Acc: 81.953% (4196/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 31\n",
            "train: [39/391] Loss: 0.475 | Acc: 82.754% (4237/5120)\n",
            "train: [79/391] Loss: 0.451 | Acc: 83.848% (8586/10240)\n",
            "train: [119/391] Loss: 0.456 | Acc: 83.672% (12852/15360)\n",
            "train: [159/391] Loss: 0.453 | Acc: 84.004% (17204/20480)\n",
            "train: [199/391] Loss: 0.450 | Acc: 84.086% (21526/25600)\n",
            "train: [239/391] Loss: 0.450 | Acc: 84.062% (25824/30720)\n",
            "train: [279/391] Loss: 0.449 | Acc: 84.082% (30135/35840)\n",
            "train: [319/391] Loss: 0.451 | Acc: 84.006% (34409/40960)\n",
            "train: [359/391] Loss: 0.450 | Acc: 84.056% (38733/46080)\n",
            "val: [39/79] Loss: 0.474 | Acc: 83.711% (4286/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 32\n",
            "train: [39/391] Loss: 0.402 | Acc: 86.016% (4404/5120)\n",
            "train: [79/391] Loss: 0.405 | Acc: 85.830% (8789/10240)\n",
            "train: [119/391] Loss: 0.407 | Acc: 85.846% (13186/15360)\n",
            "train: [159/391] Loss: 0.411 | Acc: 85.645% (17540/20480)\n",
            "train: [199/391] Loss: 0.414 | Acc: 85.535% (21897/25600)\n",
            "train: [239/391] Loss: 0.414 | Acc: 85.550% (26281/30720)\n",
            "train: [279/391] Loss: 0.419 | Acc: 85.410% (30611/35840)\n",
            "train: [319/391] Loss: 0.418 | Acc: 85.486% (35015/40960)\n",
            "train: [359/391] Loss: 0.419 | Acc: 85.438% (39370/46080)\n",
            "val: [39/79] Loss: 0.429 | Acc: 85.352% (4370/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 33\n",
            "train: [39/391] Loss: 0.391 | Acc: 85.879% (4397/5120)\n",
            "train: [79/391] Loss: 0.396 | Acc: 86.055% (8812/10240)\n",
            "train: [119/391] Loss: 0.394 | Acc: 86.120% (13228/15360)\n",
            "train: [159/391] Loss: 0.395 | Acc: 85.986% (17610/20480)\n",
            "train: [199/391] Loss: 0.396 | Acc: 86.031% (22024/25600)\n",
            "train: [239/391] Loss: 0.396 | Acc: 86.019% (26425/30720)\n",
            "train: [279/391] Loss: 0.393 | Acc: 86.122% (30866/35840)\n",
            "train: [319/391] Loss: 0.395 | Acc: 86.108% (35270/40960)\n",
            "train: [359/391] Loss: 0.397 | Acc: 86.024% (39640/46080)\n",
            "val: [39/79] Loss: 0.444 | Acc: 84.824% (4343/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 34\n",
            "train: [39/391] Loss: 0.371 | Acc: 87.168% (4463/5120)\n",
            "train: [79/391] Loss: 0.378 | Acc: 86.797% (8888/10240)\n",
            "train: [119/391] Loss: 0.384 | Acc: 86.641% (13308/15360)\n",
            "train: [159/391] Loss: 0.385 | Acc: 86.641% (17744/20480)\n",
            "train: [199/391] Loss: 0.384 | Acc: 86.617% (22174/25600)\n",
            "train: [239/391] Loss: 0.380 | Acc: 86.686% (26630/30720)\n",
            "train: [279/391] Loss: 0.377 | Acc: 86.738% (31087/35840)\n",
            "train: [319/391] Loss: 0.377 | Acc: 86.748% (35532/40960)\n",
            "train: [359/391] Loss: 0.378 | Acc: 86.654% (39930/46080)\n",
            "val: [39/79] Loss: 0.413 | Acc: 86.348% (4421/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 35\n",
            "train: [39/391] Loss: 0.344 | Acc: 87.852% (4498/5120)\n",
            "train: [79/391] Loss: 0.337 | Acc: 88.193% (9031/10240)\n",
            "train: [119/391] Loss: 0.341 | Acc: 87.917% (13504/15360)\n",
            "train: [159/391] Loss: 0.339 | Acc: 87.974% (18017/20480)\n",
            "train: [199/391] Loss: 0.342 | Acc: 87.883% (22498/25600)\n",
            "train: [239/391] Loss: 0.340 | Acc: 87.985% (27029/30720)\n",
            "train: [279/391] Loss: 0.339 | Acc: 88.030% (31550/35840)\n",
            "train: [319/391] Loss: 0.338 | Acc: 88.091% (36082/40960)\n",
            "train: [359/391] Loss: 0.338 | Acc: 88.066% (40581/46080)\n",
            "val: [39/79] Loss: 0.403 | Acc: 86.582% (4433/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 36\n",
            "train: [39/391] Loss: 0.314 | Acc: 89.160% (4565/5120)\n",
            "train: [79/391] Loss: 0.308 | Acc: 89.287% (9143/10240)\n",
            "train: [119/391] Loss: 0.302 | Acc: 89.460% (13741/15360)\n",
            "train: [159/391] Loss: 0.304 | Acc: 89.492% (18328/20480)\n",
            "train: [199/391] Loss: 0.306 | Acc: 89.355% (22875/25600)\n",
            "train: [239/391] Loss: 0.306 | Acc: 89.349% (27448/30720)\n",
            "train: [279/391] Loss: 0.303 | Acc: 89.364% (32028/35840)\n",
            "train: [319/391] Loss: 0.305 | Acc: 89.333% (36591/40960)\n",
            "train: [359/391] Loss: 0.305 | Acc: 89.342% (41169/46080)\n",
            "val: [39/79] Loss: 0.375 | Acc: 87.891% (4500/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 37\n",
            "train: [39/391] Loss: 0.278 | Acc: 89.590% (4587/5120)\n",
            "train: [79/391] Loss: 0.276 | Acc: 89.980% (9214/10240)\n",
            "train: [119/391] Loss: 0.275 | Acc: 90.078% (13836/15360)\n",
            "train: [159/391] Loss: 0.279 | Acc: 89.951% (18422/20480)\n",
            "train: [199/391] Loss: 0.279 | Acc: 90.066% (23057/25600)\n",
            "train: [239/391] Loss: 0.276 | Acc: 90.117% (27684/30720)\n",
            "train: [279/391] Loss: 0.279 | Acc: 90.017% (32262/35840)\n",
            "train: [319/391] Loss: 0.277 | Acc: 90.112% (36910/40960)\n",
            "train: [359/391] Loss: 0.278 | Acc: 90.072% (41505/46080)\n",
            "val: [39/79] Loss: 0.371 | Acc: 87.988% (4505/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 38\n",
            "train: [39/391] Loss: 0.268 | Acc: 90.801% (4649/5120)\n",
            "train: [79/391] Loss: 0.259 | Acc: 91.055% (9324/10240)\n",
            "train: [119/391] Loss: 0.262 | Acc: 90.996% (13977/15360)\n",
            "train: [159/391] Loss: 0.261 | Acc: 91.006% (18638/20480)\n",
            "train: [199/391] Loss: 0.257 | Acc: 91.059% (23311/25600)\n",
            "train: [239/391] Loss: 0.256 | Acc: 91.087% (27982/30720)\n",
            "train: [279/391] Loss: 0.256 | Acc: 91.060% (32636/35840)\n",
            "train: [319/391] Loss: 0.257 | Acc: 90.981% (37266/40960)\n",
            "train: [359/391] Loss: 0.256 | Acc: 90.985% (41926/46080)\n",
            "val: [39/79] Loss: 0.376 | Acc: 88.105% (4511/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 39\n",
            "train: [39/391] Loss: 0.249 | Acc: 91.035% (4661/5120)\n",
            "train: [79/391] Loss: 0.255 | Acc: 90.898% (9308/10240)\n",
            "train: [119/391] Loss: 0.249 | Acc: 90.957% (13971/15360)\n",
            "train: [159/391] Loss: 0.241 | Acc: 91.250% (18688/20480)\n",
            "train: [199/391] Loss: 0.245 | Acc: 91.172% (23340/25600)\n",
            "train: [239/391] Loss: 0.247 | Acc: 91.133% (27996/30720)\n",
            "train: [279/391] Loss: 0.247 | Acc: 91.133% (32662/35840)\n",
            "train: [319/391] Loss: 0.246 | Acc: 91.191% (37352/40960)\n",
            "train: [359/391] Loss: 0.247 | Acc: 91.148% (42001/46080)\n",
            "val: [39/79] Loss: 0.364 | Acc: 88.574% (4535/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 40\n",
            "train: [39/391] Loss: 0.315 | Acc: 88.828% (4548/5120)\n",
            "train: [79/391] Loss: 0.349 | Acc: 87.705% (8981/10240)\n",
            "train: [119/391] Loss: 0.363 | Acc: 87.220% (13397/15360)\n",
            "train: [159/391] Loss: 0.369 | Acc: 87.065% (17831/20480)\n",
            "train: [199/391] Loss: 0.374 | Acc: 86.863% (22237/25600)\n",
            "train: [239/391] Loss: 0.379 | Acc: 86.654% (26620/30720)\n",
            "train: [279/391] Loss: 0.379 | Acc: 86.562% (31024/35840)\n",
            "train: [319/391] Loss: 0.379 | Acc: 86.604% (35473/40960)\n",
            "train: [359/391] Loss: 0.379 | Acc: 86.586% (39899/46080)\n",
            "val: [39/79] Loss: 0.462 | Acc: 84.805% (4342/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 41\n",
            "train: [39/391] Loss: 0.366 | Acc: 86.680% (4438/5120)\n",
            "train: [79/391] Loss: 0.367 | Acc: 87.021% (8911/10240)\n",
            "train: [119/391] Loss: 0.372 | Acc: 86.803% (13333/15360)\n",
            "train: [159/391] Loss: 0.371 | Acc: 86.851% (17787/20480)\n",
            "train: [199/391] Loss: 0.374 | Acc: 86.703% (22196/25600)\n",
            "train: [239/391] Loss: 0.375 | Acc: 86.722% (26641/30720)\n",
            "train: [279/391] Loss: 0.378 | Acc: 86.554% (31021/35840)\n",
            "train: [319/391] Loss: 0.379 | Acc: 86.543% (35448/40960)\n",
            "train: [359/391] Loss: 0.379 | Acc: 86.597% (39904/46080)\n",
            "val: [39/79] Loss: 0.404 | Acc: 86.348% (4421/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 42\n",
            "train: [39/391] Loss: 0.349 | Acc: 87.949% (4503/5120)\n",
            "train: [79/391] Loss: 0.347 | Acc: 88.105% (9022/10240)\n",
            "train: [119/391] Loss: 0.353 | Acc: 87.754% (13479/15360)\n",
            "train: [159/391] Loss: 0.358 | Acc: 87.607% (17942/20480)\n",
            "train: [199/391] Loss: 0.354 | Acc: 87.641% (22436/25600)\n",
            "train: [239/391] Loss: 0.357 | Acc: 87.673% (26933/30720)\n",
            "train: [279/391] Loss: 0.357 | Acc: 87.561% (31382/35840)\n",
            "train: [319/391] Loss: 0.357 | Acc: 87.500% (35840/40960)\n",
            "train: [359/391] Loss: 0.359 | Acc: 87.407% (40277/46080)\n",
            "val: [39/79] Loss: 0.395 | Acc: 86.562% (4432/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 43\n",
            "train: [39/391] Loss: 0.329 | Acc: 88.652% (4539/5120)\n",
            "train: [79/391] Loss: 0.335 | Acc: 88.164% (9028/10240)\n",
            "train: [119/391] Loss: 0.341 | Acc: 87.910% (13503/15360)\n",
            "train: [159/391] Loss: 0.340 | Acc: 87.969% (18016/20480)\n",
            "train: [199/391] Loss: 0.340 | Acc: 88.035% (22537/25600)\n",
            "train: [239/391] Loss: 0.343 | Acc: 87.910% (27006/30720)\n",
            "train: [279/391] Loss: 0.339 | Acc: 88.061% (31561/35840)\n",
            "train: [319/391] Loss: 0.340 | Acc: 88.025% (36055/40960)\n",
            "train: [359/391] Loss: 0.340 | Acc: 88.014% (40557/46080)\n",
            "val: [39/79] Loss: 0.408 | Acc: 86.387% (4423/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 44\n",
            "train: [39/391] Loss: 0.303 | Acc: 89.355% (4575/5120)\n",
            "train: [79/391] Loss: 0.301 | Acc: 89.277% (9142/10240)\n",
            "train: [119/391] Loss: 0.298 | Acc: 89.538% (13753/15360)\n",
            "train: [159/391] Loss: 0.305 | Acc: 89.302% (18289/20480)\n",
            "train: [199/391] Loss: 0.306 | Acc: 89.230% (22843/25600)\n",
            "train: [239/391] Loss: 0.308 | Acc: 89.137% (27383/30720)\n",
            "train: [279/391] Loss: 0.309 | Acc: 89.138% (31947/35840)\n",
            "train: [319/391] Loss: 0.308 | Acc: 89.128% (36507/40960)\n",
            "train: [359/391] Loss: 0.308 | Acc: 89.156% (41083/46080)\n",
            "val: [39/79] Loss: 0.382 | Acc: 87.500% (4480/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 45\n",
            "train: [39/391] Loss: 0.269 | Acc: 90.117% (4614/5120)\n",
            "train: [79/391] Loss: 0.269 | Acc: 90.068% (9223/10240)\n",
            "train: [119/391] Loss: 0.276 | Acc: 89.974% (13820/15360)\n",
            "train: [159/391] Loss: 0.278 | Acc: 89.966% (18425/20480)\n",
            "train: [199/391] Loss: 0.278 | Acc: 89.973% (23033/25600)\n",
            "train: [239/391] Loss: 0.274 | Acc: 90.189% (27706/30720)\n",
            "train: [279/391] Loss: 0.276 | Acc: 90.126% (32301/35840)\n",
            "train: [319/391] Loss: 0.277 | Acc: 90.122% (36914/40960)\n",
            "train: [359/391] Loss: 0.278 | Acc: 90.074% (41506/46080)\n",
            "val: [39/79] Loss: 0.385 | Acc: 87.500% (4480/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 46\n",
            "train: [39/391] Loss: 0.242 | Acc: 91.094% (4664/5120)\n",
            "train: [79/391] Loss: 0.240 | Acc: 91.201% (9339/10240)\n",
            "train: [119/391] Loss: 0.243 | Acc: 91.237% (14014/15360)\n",
            "train: [159/391] Loss: 0.246 | Acc: 91.221% (18682/20480)\n",
            "train: [199/391] Loss: 0.248 | Acc: 91.086% (23318/25600)\n",
            "train: [239/391] Loss: 0.249 | Acc: 91.061% (27974/30720)\n",
            "train: [279/391] Loss: 0.249 | Acc: 90.988% (32610/35840)\n",
            "train: [319/391] Loss: 0.248 | Acc: 91.025% (37284/40960)\n",
            "train: [359/391] Loss: 0.250 | Acc: 90.968% (41918/46080)\n",
            "val: [39/79] Loss: 0.370 | Acc: 88.438% (4528/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 47\n",
            "train: [39/391] Loss: 0.234 | Acc: 91.855% (4703/5120)\n",
            "train: [79/391] Loss: 0.230 | Acc: 91.807% (9401/10240)\n",
            "train: [119/391] Loss: 0.231 | Acc: 91.842% (14107/15360)\n",
            "train: [159/391] Loss: 0.230 | Acc: 91.851% (18811/20480)\n",
            "train: [199/391] Loss: 0.231 | Acc: 91.785% (23497/25600)\n",
            "train: [239/391] Loss: 0.230 | Acc: 91.855% (28218/30720)\n",
            "train: [279/391] Loss: 0.233 | Acc: 91.766% (32889/35840)\n",
            "train: [319/391] Loss: 0.234 | Acc: 91.704% (37562/40960)\n",
            "train: [359/391] Loss: 0.233 | Acc: 91.701% (42256/46080)\n",
            "val: [39/79] Loss: 0.362 | Acc: 89.004% (4557/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 48\n",
            "train: [39/391] Loss: 0.207 | Acc: 92.949% (4759/5120)\n",
            "train: [79/391] Loss: 0.210 | Acc: 92.617% (9484/10240)\n",
            "train: [119/391] Loss: 0.217 | Acc: 92.428% (14197/15360)\n",
            "train: [159/391] Loss: 0.214 | Acc: 92.490% (18942/20480)\n",
            "train: [199/391] Loss: 0.212 | Acc: 92.586% (23702/25600)\n",
            "train: [239/391] Loss: 0.212 | Acc: 92.572% (28438/30720)\n",
            "train: [279/391] Loss: 0.211 | Acc: 92.628% (33198/35840)\n",
            "train: [319/391] Loss: 0.211 | Acc: 92.590% (37925/40960)\n",
            "train: [359/391] Loss: 0.210 | Acc: 92.652% (42694/46080)\n",
            "val: [39/79] Loss: 0.365 | Acc: 88.730% (4543/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 49\n",
            "train: [39/391] Loss: 0.190 | Acc: 93.066% (4765/5120)\n",
            "train: [79/391] Loss: 0.189 | Acc: 93.232% (9547/10240)\n",
            "train: [119/391] Loss: 0.197 | Acc: 92.982% (14282/15360)\n",
            "train: [159/391] Loss: 0.202 | Acc: 92.734% (18992/20480)\n",
            "train: [199/391] Loss: 0.201 | Acc: 92.836% (23766/25600)\n",
            "train: [239/391] Loss: 0.201 | Acc: 92.848% (28523/30720)\n",
            "train: [279/391] Loss: 0.201 | Acc: 92.854% (33279/35840)\n",
            "train: [319/391] Loss: 0.199 | Acc: 92.927% (38063/40960)\n",
            "train: [359/391] Loss: 0.199 | Acc: 92.943% (42828/46080)\n",
            "val: [39/79] Loss: 0.353 | Acc: 89.062% (4560/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 50\n",
            "train: [39/391] Loss: 0.282 | Acc: 89.922% (4604/5120)\n",
            "train: [79/391] Loss: 0.306 | Acc: 89.111% (9125/10240)\n",
            "train: [119/391] Loss: 0.318 | Acc: 88.581% (13606/15360)\n",
            "train: [159/391] Loss: 0.322 | Acc: 88.486% (18122/20480)\n",
            "train: [199/391] Loss: 0.327 | Acc: 88.355% (22619/25600)\n",
            "train: [239/391] Loss: 0.327 | Acc: 88.382% (27151/30720)\n",
            "train: [279/391] Loss: 0.328 | Acc: 88.345% (31663/35840)\n",
            "train: [319/391] Loss: 0.328 | Acc: 88.323% (36177/40960)\n",
            "train: [359/391] Loss: 0.331 | Acc: 88.264% (40672/46080)\n",
            "val: [39/79] Loss: 0.395 | Acc: 86.777% (4443/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 51\n",
            "train: [39/391] Loss: 0.309 | Acc: 88.965% (4555/5120)\n",
            "train: [79/391] Loss: 0.313 | Acc: 88.955% (9109/10240)\n",
            "train: [119/391] Loss: 0.311 | Acc: 89.056% (13679/15360)\n",
            "train: [159/391] Loss: 0.317 | Acc: 88.799% (18186/20480)\n",
            "train: [199/391] Loss: 0.316 | Acc: 88.895% (22757/25600)\n",
            "train: [239/391] Loss: 0.316 | Acc: 88.903% (27311/30720)\n",
            "train: [279/391] Loss: 0.317 | Acc: 88.836% (31839/35840)\n",
            "train: [319/391] Loss: 0.315 | Acc: 88.960% (36438/40960)\n",
            "train: [359/391] Loss: 0.318 | Acc: 88.863% (40948/46080)\n",
            "val: [39/79] Loss: 0.390 | Acc: 87.109% (4460/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 52\n",
            "train: [39/391] Loss: 0.313 | Acc: 89.102% (4562/5120)\n",
            "train: [79/391] Loss: 0.302 | Acc: 89.512% (9166/10240)\n",
            "train: [119/391] Loss: 0.306 | Acc: 89.245% (13708/15360)\n",
            "train: [159/391] Loss: 0.306 | Acc: 89.097% (18247/20480)\n",
            "train: [199/391] Loss: 0.309 | Acc: 89.023% (22790/25600)\n",
            "train: [239/391] Loss: 0.328 | Acc: 88.639% (27230/30720)\n",
            "train: [279/391] Loss: 0.356 | Acc: 87.715% (31437/35840)\n",
            "train: [319/391] Loss: 0.359 | Acc: 87.603% (35882/40960)\n",
            "train: [359/391] Loss: 0.362 | Acc: 87.448% (40296/46080)\n",
            "val: [39/79] Loss: 0.484 | Acc: 84.941% (4349/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 53\n",
            "train: [39/391] Loss: 0.522 | Acc: 82.988% (4249/5120)\n",
            "train: [79/391] Loss: 0.523 | Acc: 82.988% (8498/10240)\n",
            "train: [119/391] Loss: 0.498 | Acc: 83.483% (12823/15360)\n",
            "train: [159/391] Loss: 0.471 | Acc: 84.170% (17238/20480)\n",
            "train: [199/391] Loss: 0.450 | Acc: 84.734% (21692/25600)\n",
            "train: [239/391] Loss: 0.432 | Acc: 85.391% (26232/30720)\n",
            "train: [279/391] Loss: 0.418 | Acc: 85.924% (30795/35840)\n",
            "train: [319/391] Loss: 0.407 | Acc: 86.204% (35309/40960)\n",
            "train: [359/391] Loss: 0.398 | Acc: 86.502% (39860/46080)\n",
            "val: [39/79] Loss: 0.384 | Acc: 87.090% (4459/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 54\n",
            "train: [39/391] Loss: 0.300 | Acc: 89.375% (4576/5120)\n",
            "train: [79/391] Loss: 0.298 | Acc: 89.668% (9182/10240)\n",
            "train: [119/391] Loss: 0.295 | Acc: 89.740% (13784/15360)\n",
            "train: [159/391] Loss: 0.294 | Acc: 89.668% (18364/20480)\n",
            "train: [199/391] Loss: 0.289 | Acc: 89.816% (22993/25600)\n",
            "train: [239/391] Loss: 0.290 | Acc: 89.798% (27586/30720)\n",
            "train: [279/391] Loss: 0.288 | Acc: 89.880% (32213/35840)\n",
            "train: [319/391] Loss: 0.290 | Acc: 89.802% (36783/40960)\n",
            "train: [359/391] Loss: 0.290 | Acc: 89.770% (41366/46080)\n",
            "val: [39/79] Loss: 0.367 | Acc: 88.086% (4510/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 55\n",
            "train: [39/391] Loss: 0.259 | Acc: 90.859% (4652/5120)\n",
            "train: [79/391] Loss: 0.261 | Acc: 90.830% (9301/10240)\n",
            "train: [119/391] Loss: 0.262 | Acc: 90.801% (13947/15360)\n",
            "train: [159/391] Loss: 0.258 | Acc: 90.835% (18603/20480)\n",
            "train: [199/391] Loss: 0.256 | Acc: 90.914% (23274/25600)\n",
            "train: [239/391] Loss: 0.256 | Acc: 90.788% (27890/30720)\n",
            "train: [279/391] Loss: 0.255 | Acc: 90.778% (32535/35840)\n",
            "train: [319/391] Loss: 0.257 | Acc: 90.732% (37164/40960)\n",
            "train: [359/391] Loss: 0.257 | Acc: 90.751% (41818/46080)\n",
            "val: [39/79] Loss: 0.368 | Acc: 88.223% (4517/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 56\n",
            "train: [39/391] Loss: 0.234 | Acc: 91.504% (4685/5120)\n",
            "train: [79/391] Loss: 0.227 | Acc: 91.777% (9398/10240)\n",
            "train: [119/391] Loss: 0.227 | Acc: 91.810% (14102/15360)\n",
            "train: [159/391] Loss: 0.229 | Acc: 91.816% (18804/20480)\n",
            "train: [199/391] Loss: 0.229 | Acc: 91.758% (23490/25600)\n",
            "train: [239/391] Loss: 0.230 | Acc: 91.868% (28222/30720)\n",
            "train: [279/391] Loss: 0.230 | Acc: 91.892% (32934/35840)\n",
            "train: [319/391] Loss: 0.230 | Acc: 91.909% (37646/40960)\n",
            "train: [359/391] Loss: 0.228 | Acc: 91.944% (42368/46080)\n",
            "val: [39/79] Loss: 0.372 | Acc: 88.594% (4536/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 57\n",
            "train: [39/391] Loss: 0.215 | Acc: 92.539% (4738/5120)\n",
            "train: [79/391] Loss: 0.210 | Acc: 92.725% (9495/10240)\n",
            "train: [119/391] Loss: 0.212 | Acc: 92.559% (14217/15360)\n",
            "train: [159/391] Loss: 0.208 | Acc: 92.578% (18960/20480)\n",
            "train: [199/391] Loss: 0.205 | Acc: 92.648% (23718/25600)\n",
            "train: [239/391] Loss: 0.206 | Acc: 92.705% (28479/30720)\n",
            "train: [279/391] Loss: 0.206 | Acc: 92.729% (33234/35840)\n",
            "train: [319/391] Loss: 0.205 | Acc: 92.761% (37995/40960)\n",
            "train: [359/391] Loss: 0.204 | Acc: 92.808% (42766/46080)\n",
            "val: [39/79] Loss: 0.352 | Acc: 88.926% (4553/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 58\n",
            "train: [39/391] Loss: 0.174 | Acc: 93.848% (4805/5120)\n",
            "train: [79/391] Loss: 0.180 | Acc: 93.584% (9583/10240)\n",
            "train: [119/391] Loss: 0.181 | Acc: 93.633% (14382/15360)\n",
            "train: [159/391] Loss: 0.183 | Acc: 93.521% (19153/20480)\n",
            "train: [199/391] Loss: 0.187 | Acc: 93.406% (23912/25600)\n",
            "train: [239/391] Loss: 0.187 | Acc: 93.382% (28687/30720)\n",
            "train: [279/391] Loss: 0.187 | Acc: 93.384% (33469/35840)\n",
            "train: [319/391] Loss: 0.185 | Acc: 93.496% (38296/40960)\n",
            "train: [359/391] Loss: 0.185 | Acc: 93.518% (43093/46080)\n",
            "val: [39/79] Loss: 0.354 | Acc: 89.082% (4561/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 59\n",
            "train: [39/391] Loss: 0.188 | Acc: 93.262% (4775/5120)\n",
            "train: [79/391] Loss: 0.180 | Acc: 93.467% (9571/10240)\n",
            "train: [119/391] Loss: 0.178 | Acc: 93.470% (14357/15360)\n",
            "train: [159/391] Loss: 0.181 | Acc: 93.496% (19148/20480)\n",
            "train: [199/391] Loss: 0.182 | Acc: 93.457% (23925/25600)\n",
            "train: [239/391] Loss: 0.181 | Acc: 93.561% (28742/30720)\n",
            "train: [279/391] Loss: 0.181 | Acc: 93.599% (33546/35840)\n",
            "train: [319/391] Loss: 0.180 | Acc: 93.643% (38356/40960)\n",
            "train: [359/391] Loss: 0.180 | Acc: 93.715% (43184/46080)\n",
            "val: [39/79] Loss: 0.350 | Acc: 89.277% (4571/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 60\n",
            "train: [39/391] Loss: 0.246 | Acc: 91.191% (4669/5120)\n",
            "train: [79/391] Loss: 0.271 | Acc: 90.303% (9247/10240)\n",
            "train: [119/391] Loss: 0.274 | Acc: 90.182% (13852/15360)\n",
            "train: [159/391] Loss: 0.283 | Acc: 89.883% (18408/20480)\n",
            "train: [199/391] Loss: 0.288 | Acc: 89.746% (22975/25600)\n",
            "train: [239/391] Loss: 0.290 | Acc: 89.678% (27549/30720)\n",
            "train: [279/391] Loss: 0.291 | Acc: 89.721% (32156/35840)\n",
            "train: [319/391] Loss: 0.292 | Acc: 89.658% (36724/40960)\n",
            "train: [359/391] Loss: 0.291 | Acc: 89.701% (41334/46080)\n",
            "val: [39/79] Loss: 0.406 | Acc: 86.875% (4448/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 61\n",
            "train: [39/391] Loss: 0.280 | Acc: 89.844% (4600/5120)\n",
            "train: [79/391] Loss: 0.276 | Acc: 89.902% (9206/10240)\n",
            "train: [119/391] Loss: 0.287 | Acc: 89.564% (13757/15360)\n",
            "train: [159/391] Loss: 0.284 | Acc: 89.609% (18352/20480)\n",
            "train: [199/391] Loss: 0.284 | Acc: 89.676% (22957/25600)\n",
            "train: [239/391] Loss: 0.287 | Acc: 89.648% (27540/30720)\n",
            "train: [279/391] Loss: 0.289 | Acc: 89.581% (32106/35840)\n",
            "train: [319/391] Loss: 0.290 | Acc: 89.639% (36716/40960)\n",
            "train: [359/391] Loss: 0.291 | Acc: 89.603% (41289/46080)\n",
            "val: [39/79] Loss: 0.372 | Acc: 87.480% (4479/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 62\n",
            "train: [39/391] Loss: 0.278 | Acc: 90.156% (4616/5120)\n",
            "train: [79/391] Loss: 0.270 | Acc: 90.312% (9248/10240)\n",
            "train: [119/391] Loss: 0.279 | Acc: 90.000% (13824/15360)\n",
            "train: [159/391] Loss: 0.278 | Acc: 90.015% (18435/20480)\n",
            "train: [199/391] Loss: 0.276 | Acc: 90.020% (23045/25600)\n",
            "train: [239/391] Loss: 0.274 | Acc: 90.085% (27674/30720)\n",
            "train: [279/391] Loss: 0.278 | Acc: 90.064% (32279/35840)\n",
            "train: [319/391] Loss: 0.280 | Acc: 89.976% (36854/40960)\n",
            "train: [359/391] Loss: 0.284 | Acc: 89.818% (41388/46080)\n",
            "val: [39/79] Loss: 0.375 | Acc: 87.598% (4485/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 63\n",
            "train: [39/391] Loss: 0.267 | Acc: 90.488% (4633/5120)\n",
            "train: [79/391] Loss: 0.262 | Acc: 90.615% (9279/10240)\n",
            "train: [119/391] Loss: 0.262 | Acc: 90.592% (13915/15360)\n",
            "train: [159/391] Loss: 0.259 | Acc: 90.708% (18577/20480)\n",
            "train: [199/391] Loss: 0.257 | Acc: 90.863% (23261/25600)\n",
            "train: [239/391] Loss: 0.258 | Acc: 90.801% (27894/30720)\n",
            "train: [279/391] Loss: 0.258 | Acc: 90.798% (32542/35840)\n",
            "train: [319/391] Loss: 0.261 | Acc: 90.679% (37142/40960)\n",
            "train: [359/391] Loss: 0.263 | Acc: 90.586% (41742/46080)\n",
            "val: [39/79] Loss: 0.371 | Acc: 88.105% (4511/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 64\n",
            "train: [39/391] Loss: 0.253 | Acc: 91.445% (4682/5120)\n",
            "train: [79/391] Loss: 0.243 | Acc: 91.543% (9374/10240)\n",
            "train: [119/391] Loss: 0.239 | Acc: 91.582% (14067/15360)\n",
            "train: [159/391] Loss: 0.239 | Acc: 91.558% (18751/20480)\n",
            "train: [199/391] Loss: 0.237 | Acc: 91.633% (23458/25600)\n",
            "train: [239/391] Loss: 0.237 | Acc: 91.562% (28128/30720)\n",
            "train: [279/391] Loss: 0.237 | Acc: 91.549% (32811/35840)\n",
            "train: [319/391] Loss: 0.237 | Acc: 91.519% (37486/40960)\n",
            "train: [359/391] Loss: 0.237 | Acc: 91.541% (42182/46080)\n",
            "val: [39/79] Loss: 0.390 | Acc: 88.047% (4508/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 65\n",
            "train: [39/391] Loss: 0.207 | Acc: 92.441% (4733/5120)\n",
            "train: [79/391] Loss: 0.207 | Acc: 92.568% (9479/10240)\n",
            "train: [119/391] Loss: 0.209 | Acc: 92.539% (14214/15360)\n",
            "train: [159/391] Loss: 0.208 | Acc: 92.627% (18970/20480)\n",
            "train: [199/391] Loss: 0.209 | Acc: 92.566% (23697/25600)\n",
            "train: [239/391] Loss: 0.211 | Acc: 92.490% (28413/30720)\n",
            "train: [279/391] Loss: 0.211 | Acc: 92.436% (33129/35840)\n",
            "train: [319/391] Loss: 0.212 | Acc: 92.395% (37845/40960)\n",
            "train: [359/391] Loss: 0.211 | Acc: 92.439% (42596/46080)\n",
            "val: [39/79] Loss: 0.371 | Acc: 89.062% (4560/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 66\n",
            "train: [39/391] Loss: 0.182 | Acc: 93.711% (4798/5120)\n",
            "train: [79/391] Loss: 0.189 | Acc: 93.477% (9572/10240)\n",
            "train: [119/391] Loss: 0.186 | Acc: 93.574% (14373/15360)\n",
            "train: [159/391] Loss: 0.185 | Acc: 93.574% (19164/20480)\n",
            "train: [199/391] Loss: 0.188 | Acc: 93.387% (23907/25600)\n",
            "train: [239/391] Loss: 0.188 | Acc: 93.376% (28685/30720)\n",
            "train: [279/391] Loss: 0.187 | Acc: 93.432% (33486/35840)\n",
            "train: [319/391] Loss: 0.185 | Acc: 93.457% (38280/40960)\n",
            "train: [359/391] Loss: 0.185 | Acc: 93.459% (43066/46080)\n",
            "val: [39/79] Loss: 0.373 | Acc: 89.023% (4558/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 67\n",
            "train: [39/391] Loss: 0.186 | Acc: 93.496% (4787/5120)\n",
            "train: [79/391] Loss: 0.182 | Acc: 93.535% (9578/10240)\n",
            "train: [119/391] Loss: 0.177 | Acc: 93.678% (14389/15360)\n",
            "train: [159/391] Loss: 0.172 | Acc: 93.804% (19211/20480)\n",
            "train: [199/391] Loss: 0.173 | Acc: 93.789% (24010/25600)\n",
            "train: [239/391] Loss: 0.173 | Acc: 93.796% (28814/30720)\n",
            "train: [279/391] Loss: 0.173 | Acc: 93.809% (33621/35840)\n",
            "train: [319/391] Loss: 0.173 | Acc: 93.867% (38448/40960)\n",
            "train: [359/391] Loss: 0.173 | Acc: 93.809% (43227/46080)\n",
            "val: [39/79] Loss: 0.366 | Acc: 89.336% (4574/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 68\n",
            "train: [39/391] Loss: 0.148 | Acc: 94.570% (4842/5120)\n",
            "train: [79/391] Loss: 0.148 | Acc: 94.561% (9683/10240)\n",
            "train: [119/391] Loss: 0.148 | Acc: 94.557% (14524/15360)\n",
            "train: [159/391] Loss: 0.149 | Acc: 94.531% (19360/20480)\n",
            "train: [199/391] Loss: 0.149 | Acc: 94.531% (24200/25600)\n",
            "train: [239/391] Loss: 0.148 | Acc: 94.567% (29051/30720)\n",
            "train: [279/391] Loss: 0.150 | Acc: 94.528% (33879/35840)\n",
            "train: [319/391] Loss: 0.151 | Acc: 94.478% (38698/40960)\n",
            "train: [359/391] Loss: 0.153 | Acc: 94.410% (43504/46080)\n",
            "val: [39/79] Loss: 0.357 | Acc: 89.707% (4593/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 69\n",
            "train: [39/391] Loss: 0.134 | Acc: 95.000% (4864/5120)\n",
            "train: [79/391] Loss: 0.142 | Acc: 94.795% (9707/10240)\n",
            "train: [119/391] Loss: 0.145 | Acc: 94.805% (14562/15360)\n",
            "train: [159/391] Loss: 0.145 | Acc: 94.805% (19416/20480)\n",
            "train: [199/391] Loss: 0.142 | Acc: 94.914% (24298/25600)\n",
            "train: [239/391] Loss: 0.142 | Acc: 94.902% (29154/30720)\n",
            "train: [279/391] Loss: 0.143 | Acc: 94.888% (34008/35840)\n",
            "train: [319/391] Loss: 0.143 | Acc: 94.880% (38863/40960)\n",
            "train: [359/391] Loss: 0.142 | Acc: 94.941% (43749/46080)\n",
            "val: [39/79] Loss: 0.361 | Acc: 90.098% (4613/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 70\n",
            "train: [39/391] Loss: 0.201 | Acc: 92.734% (4748/5120)\n",
            "train: [79/391] Loss: 0.237 | Acc: 91.602% (9380/10240)\n",
            "train: [119/391] Loss: 0.242 | Acc: 91.393% (14038/15360)\n",
            "train: [159/391] Loss: 0.244 | Acc: 91.299% (18698/20480)\n",
            "train: [199/391] Loss: 0.249 | Acc: 90.953% (23284/25600)\n",
            "train: [239/391] Loss: 0.252 | Acc: 90.918% (27930/30720)\n",
            "train: [279/391] Loss: 0.253 | Acc: 90.854% (32562/35840)\n",
            "train: [319/391] Loss: 0.259 | Acc: 90.718% (37158/40960)\n",
            "train: [359/391] Loss: 0.259 | Acc: 90.684% (41787/46080)\n",
            "val: [39/79] Loss: 0.414 | Acc: 86.953% (4452/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 71\n",
            "train: [39/391] Loss: 0.246 | Acc: 91.270% (4673/5120)\n",
            "train: [79/391] Loss: 0.250 | Acc: 91.318% (9351/10240)\n",
            "train: [119/391] Loss: 0.248 | Acc: 91.211% (14010/15360)\n",
            "train: [159/391] Loss: 0.251 | Acc: 90.991% (18635/20480)\n",
            "train: [199/391] Loss: 0.254 | Acc: 90.891% (23268/25600)\n",
            "train: [239/391] Loss: 0.255 | Acc: 90.859% (27912/30720)\n",
            "train: [279/391] Loss: 0.256 | Acc: 90.884% (32573/35840)\n",
            "train: [319/391] Loss: 0.255 | Acc: 90.945% (37251/40960)\n",
            "train: [359/391] Loss: 0.257 | Acc: 90.820% (41850/46080)\n",
            "val: [39/79] Loss: 0.412 | Acc: 86.758% (4442/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 72\n",
            "train: [39/391] Loss: 0.240 | Acc: 91.523% (4686/5120)\n",
            "train: [79/391] Loss: 0.238 | Acc: 91.650% (9385/10240)\n",
            "train: [119/391] Loss: 0.241 | Acc: 91.419% (14042/15360)\n",
            "train: [159/391] Loss: 0.247 | Acc: 91.216% (18681/20480)\n",
            "train: [199/391] Loss: 0.244 | Acc: 91.316% (23377/25600)\n",
            "train: [239/391] Loss: 0.247 | Acc: 91.237% (28028/30720)\n",
            "train: [279/391] Loss: 0.247 | Acc: 91.247% (32703/35840)\n",
            "train: [319/391] Loss: 0.246 | Acc: 91.289% (37392/40960)\n",
            "train: [359/391] Loss: 0.249 | Acc: 91.241% (42044/46080)\n",
            "val: [39/79] Loss: 0.411 | Acc: 87.168% (4463/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 73\n",
            "train: [39/391] Loss: 0.232 | Acc: 91.699% (4695/5120)\n",
            "train: [79/391] Loss: 0.221 | Acc: 92.061% (9427/10240)\n",
            "train: [119/391] Loss: 0.220 | Acc: 92.018% (14134/15360)\n",
            "train: [159/391] Loss: 0.220 | Acc: 91.997% (18841/20480)\n",
            "train: [199/391] Loss: 0.224 | Acc: 91.910% (23529/25600)\n",
            "train: [239/391] Loss: 0.221 | Acc: 91.992% (28260/30720)\n",
            "train: [279/391] Loss: 0.224 | Acc: 91.872% (32927/35840)\n",
            "train: [319/391] Loss: 0.226 | Acc: 91.821% (37610/40960)\n",
            "train: [359/391] Loss: 0.226 | Acc: 91.832% (42316/46080)\n",
            "val: [39/79] Loss: 0.378 | Acc: 88.477% (4530/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 74\n",
            "train: [39/391] Loss: 0.205 | Acc: 92.676% (4745/5120)\n",
            "train: [79/391] Loss: 0.205 | Acc: 92.744% (9497/10240)\n",
            "train: [119/391] Loss: 0.199 | Acc: 92.891% (14268/15360)\n",
            "train: [159/391] Loss: 0.228 | Acc: 92.021% (18846/20480)\n",
            "train: [199/391] Loss: 0.237 | Acc: 91.711% (23478/25600)\n",
            "train: [239/391] Loss: 0.256 | Acc: 91.061% (27974/30720)\n",
            "train: [279/391] Loss: 0.258 | Acc: 90.979% (32607/35840)\n",
            "train: [319/391] Loss: 0.258 | Acc: 90.984% (37267/40960)\n",
            "train: [359/391] Loss: 0.256 | Acc: 91.018% (41941/46080)\n",
            "val: [39/79] Loss: 0.377 | Acc: 88.027% (4507/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 75\n",
            "train: [39/391] Loss: 0.222 | Acc: 92.070% (4714/5120)\n",
            "train: [79/391] Loss: 0.203 | Acc: 92.871% (9510/10240)\n",
            "train: [119/391] Loss: 0.202 | Acc: 92.812% (14256/15360)\n",
            "train: [159/391] Loss: 0.201 | Acc: 92.871% (19020/20480)\n",
            "train: [199/391] Loss: 0.202 | Acc: 92.781% (23752/25600)\n",
            "train: [239/391] Loss: 0.202 | Acc: 92.812% (28512/30720)\n",
            "train: [279/391] Loss: 0.201 | Acc: 92.840% (33274/35840)\n",
            "train: [319/391] Loss: 0.200 | Acc: 92.856% (38034/40960)\n",
            "train: [359/391] Loss: 0.200 | Acc: 92.891% (42804/46080)\n",
            "val: [39/79] Loss: 0.361 | Acc: 89.043% (4559/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 76\n",
            "train: [39/391] Loss: 0.175 | Acc: 93.594% (4792/5120)\n",
            "train: [79/391] Loss: 0.167 | Acc: 94.023% (9628/10240)\n",
            "train: [119/391] Loss: 0.164 | Acc: 94.180% (14466/15360)\n",
            "train: [159/391] Loss: 0.165 | Acc: 94.155% (19283/20480)\n",
            "train: [199/391] Loss: 0.168 | Acc: 94.070% (24082/25600)\n",
            "train: [239/391] Loss: 0.166 | Acc: 94.089% (28904/30720)\n",
            "train: [279/391] Loss: 0.167 | Acc: 93.993% (33687/35840)\n",
            "train: [319/391] Loss: 0.167 | Acc: 94.031% (38515/40960)\n",
            "train: [359/391] Loss: 0.167 | Acc: 94.041% (43334/46080)\n",
            "val: [39/79] Loss: 0.358 | Acc: 89.277% (4571/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 77\n",
            "train: [39/391] Loss: 0.157 | Acc: 94.316% (4829/5120)\n",
            "train: [79/391] Loss: 0.154 | Acc: 94.424% (9669/10240)\n",
            "train: [119/391] Loss: 0.154 | Acc: 94.440% (14506/15360)\n",
            "train: [159/391] Loss: 0.158 | Acc: 94.360% (19325/20480)\n",
            "train: [199/391] Loss: 0.157 | Acc: 94.418% (24171/25600)\n",
            "train: [239/391] Loss: 0.155 | Acc: 94.469% (29021/30720)\n",
            "train: [279/391] Loss: 0.154 | Acc: 94.537% (33882/35840)\n",
            "train: [319/391] Loss: 0.152 | Acc: 94.558% (38731/40960)\n",
            "train: [359/391] Loss: 0.152 | Acc: 94.581% (43583/46080)\n",
            "val: [39/79] Loss: 0.353 | Acc: 89.336% (4574/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 78\n",
            "train: [39/391] Loss: 0.143 | Acc: 95.020% (4865/5120)\n",
            "train: [79/391] Loss: 0.134 | Acc: 95.381% (9767/10240)\n",
            "train: [119/391] Loss: 0.134 | Acc: 95.293% (14637/15360)\n",
            "train: [159/391] Loss: 0.132 | Acc: 95.249% (19507/20480)\n",
            "train: [199/391] Loss: 0.134 | Acc: 95.191% (24369/25600)\n",
            "train: [239/391] Loss: 0.134 | Acc: 95.156% (29232/30720)\n",
            "train: [279/391] Loss: 0.133 | Acc: 95.206% (34122/35840)\n",
            "train: [319/391] Loss: 0.135 | Acc: 95.164% (38979/40960)\n",
            "train: [359/391] Loss: 0.135 | Acc: 95.139% (43840/46080)\n",
            "val: [39/79] Loss: 0.356 | Acc: 89.688% (4592/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 79\n",
            "train: [39/391] Loss: 0.127 | Acc: 95.645% (4897/5120)\n",
            "train: [79/391] Loss: 0.124 | Acc: 95.801% (9810/10240)\n",
            "train: [119/391] Loss: 0.127 | Acc: 95.592% (14683/15360)\n",
            "train: [159/391] Loss: 0.124 | Acc: 95.601% (19579/20480)\n",
            "train: [199/391] Loss: 0.126 | Acc: 95.590% (24471/25600)\n",
            "train: [239/391] Loss: 0.128 | Acc: 95.501% (29338/30720)\n",
            "train: [279/391] Loss: 0.128 | Acc: 95.474% (34218/35840)\n",
            "train: [319/391] Loss: 0.128 | Acc: 95.508% (39120/40960)\n",
            "train: [359/391] Loss: 0.130 | Acc: 95.434% (43976/46080)\n",
            "val: [39/79] Loss: 0.354 | Acc: 89.961% (4606/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 80\n",
            "train: [39/391] Loss: 0.177 | Acc: 93.711% (4798/5120)\n",
            "train: [79/391] Loss: 0.195 | Acc: 93.193% (9543/10240)\n",
            "train: [119/391] Loss: 0.211 | Acc: 92.630% (14228/15360)\n",
            "train: [159/391] Loss: 0.219 | Acc: 92.300% (18903/20480)\n",
            "train: [199/391] Loss: 0.220 | Acc: 92.285% (23625/25600)\n",
            "train: [239/391] Loss: 0.221 | Acc: 92.233% (28334/30720)\n",
            "train: [279/391] Loss: 0.224 | Acc: 92.068% (32997/35840)\n",
            "train: [319/391] Loss: 0.228 | Acc: 91.875% (37632/40960)\n",
            "train: [359/391] Loss: 0.231 | Acc: 91.758% (42282/46080)\n",
            "val: [39/79] Loss: 0.368 | Acc: 88.398% (4526/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 81\n",
            "train: [39/391] Loss: 0.212 | Acc: 92.363% (4729/5120)\n",
            "train: [79/391] Loss: 0.221 | Acc: 91.943% (9415/10240)\n",
            "train: [119/391] Loss: 0.222 | Acc: 92.018% (14134/15360)\n",
            "train: [159/391] Loss: 0.230 | Acc: 91.841% (18809/20480)\n",
            "train: [199/391] Loss: 0.231 | Acc: 91.875% (23520/25600)\n",
            "train: [239/391] Loss: 0.231 | Acc: 91.849% (28216/30720)\n",
            "train: [279/391] Loss: 0.232 | Acc: 91.783% (32895/35840)\n",
            "train: [319/391] Loss: 0.231 | Acc: 91.812% (37606/40960)\n",
            "train: [359/391] Loss: 0.231 | Acc: 91.806% (42304/46080)\n",
            "val: [39/79] Loss: 0.390 | Acc: 87.480% (4479/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 82\n",
            "train: [39/391] Loss: 0.209 | Acc: 92.402% (4731/5120)\n",
            "train: [79/391] Loss: 0.212 | Acc: 92.617% (9484/10240)\n",
            "train: [119/391] Loss: 0.214 | Acc: 92.461% (14202/15360)\n",
            "train: [159/391] Loss: 0.217 | Acc: 92.275% (18898/20480)\n",
            "train: [199/391] Loss: 0.218 | Acc: 92.219% (23608/25600)\n",
            "train: [239/391] Loss: 0.217 | Acc: 92.253% (28340/30720)\n",
            "train: [279/391] Loss: 0.220 | Acc: 92.087% (33004/35840)\n",
            "train: [319/391] Loss: 0.221 | Acc: 92.061% (37708/40960)\n",
            "train: [359/391] Loss: 0.222 | Acc: 91.999% (42393/46080)\n",
            "val: [39/79] Loss: 0.370 | Acc: 88.184% (4515/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 83\n",
            "train: [39/391] Loss: 0.184 | Acc: 93.340% (4779/5120)\n",
            "train: [79/391] Loss: 0.189 | Acc: 93.184% (9542/10240)\n",
            "train: [119/391] Loss: 0.195 | Acc: 93.021% (14288/15360)\n",
            "train: [159/391] Loss: 0.201 | Acc: 92.744% (18994/20480)\n",
            "train: [199/391] Loss: 0.201 | Acc: 92.750% (23744/25600)\n",
            "train: [239/391] Loss: 0.202 | Acc: 92.721% (28484/30720)\n",
            "train: [279/391] Loss: 0.202 | Acc: 92.785% (33254/35840)\n",
            "train: [319/391] Loss: 0.202 | Acc: 92.749% (37990/40960)\n",
            "train: [359/391] Loss: 0.204 | Acc: 92.676% (42705/46080)\n",
            "val: [39/79] Loss: 0.370 | Acc: 88.711% (4542/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 84\n",
            "train: [39/391] Loss: 0.196 | Acc: 93.398% (4782/5120)\n",
            "train: [79/391] Loss: 0.181 | Acc: 93.613% (9586/10240)\n",
            "train: [119/391] Loss: 0.179 | Acc: 93.613% (14379/15360)\n",
            "train: [159/391] Loss: 0.181 | Acc: 93.628% (19175/20480)\n",
            "train: [199/391] Loss: 0.180 | Acc: 93.691% (23985/25600)\n",
            "train: [239/391] Loss: 0.182 | Acc: 93.610% (28757/30720)\n",
            "train: [279/391] Loss: 0.183 | Acc: 93.524% (33519/35840)\n",
            "train: [319/391] Loss: 0.182 | Acc: 93.552% (38319/40960)\n",
            "train: [359/391] Loss: 0.185 | Acc: 93.435% (43055/46080)\n",
            "val: [39/79] Loss: 0.372 | Acc: 88.574% (4535/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 85\n",
            "train: [39/391] Loss: 0.156 | Acc: 94.473% (4837/5120)\n",
            "train: [79/391] Loss: 0.159 | Acc: 94.268% (9653/10240)\n",
            "train: [119/391] Loss: 0.160 | Acc: 94.245% (14476/15360)\n",
            "train: [159/391] Loss: 0.159 | Acc: 94.282% (19309/20480)\n",
            "train: [199/391] Loss: 0.157 | Acc: 94.340% (24151/25600)\n",
            "train: [239/391] Loss: 0.159 | Acc: 94.264% (28958/30720)\n",
            "train: [279/391] Loss: 0.159 | Acc: 94.247% (33778/35840)\n",
            "train: [319/391] Loss: 0.160 | Acc: 94.241% (38601/40960)\n",
            "train: [359/391] Loss: 0.162 | Acc: 94.204% (43409/46080)\n",
            "val: [39/79] Loss: 0.374 | Acc: 88.711% (4542/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 86\n",
            "train: [39/391] Loss: 0.154 | Acc: 94.668% (4847/5120)\n",
            "train: [79/391] Loss: 0.148 | Acc: 94.922% (9720/10240)\n",
            "train: [119/391] Loss: 0.143 | Acc: 95.078% (14604/15360)\n",
            "train: [159/391] Loss: 0.141 | Acc: 95.098% (19476/20480)\n",
            "train: [199/391] Loss: 0.142 | Acc: 95.020% (24325/25600)\n",
            "train: [239/391] Loss: 0.142 | Acc: 95.029% (29193/30720)\n",
            "train: [279/391] Loss: 0.142 | Acc: 95.006% (34050/35840)\n",
            "train: [319/391] Loss: 0.142 | Acc: 95.007% (38915/40960)\n",
            "train: [359/391] Loss: 0.143 | Acc: 94.996% (43774/46080)\n",
            "val: [39/79] Loss: 0.392 | Acc: 88.789% (4546/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 87\n",
            "train: [39/391] Loss: 0.122 | Acc: 95.586% (4894/5120)\n",
            "train: [79/391] Loss: 0.124 | Acc: 95.449% (9774/10240)\n",
            "train: [119/391] Loss: 0.121 | Acc: 95.586% (14682/15360)\n",
            "train: [159/391] Loss: 0.123 | Acc: 95.498% (19558/20480)\n",
            "train: [199/391] Loss: 0.124 | Acc: 95.547% (24460/25600)\n",
            "train: [239/391] Loss: 0.124 | Acc: 95.609% (29371/30720)\n",
            "train: [279/391] Loss: 0.125 | Acc: 95.527% (34237/35840)\n",
            "train: [319/391] Loss: 0.123 | Acc: 95.569% (39145/40960)\n",
            "train: [359/391] Loss: 0.124 | Acc: 95.560% (44034/46080)\n",
            "val: [39/79] Loss: 0.382 | Acc: 89.473% (4581/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 88\n",
            "train: [39/391] Loss: 0.106 | Acc: 96.484% (4940/5120)\n",
            "train: [79/391] Loss: 0.108 | Acc: 96.367% (9868/10240)\n",
            "train: [119/391] Loss: 0.113 | Acc: 96.061% (14755/15360)\n",
            "train: [159/391] Loss: 0.110 | Acc: 96.094% (19680/20480)\n",
            "train: [199/391] Loss: 0.111 | Acc: 96.066% (24593/25600)\n",
            "train: [239/391] Loss: 0.110 | Acc: 96.139% (29534/30720)\n",
            "train: [279/391] Loss: 0.112 | Acc: 96.105% (34444/35840)\n",
            "train: [319/391] Loss: 0.110 | Acc: 96.116% (39369/40960)\n",
            "train: [359/391] Loss: 0.110 | Acc: 96.124% (44294/46080)\n",
            "val: [39/79] Loss: 0.382 | Acc: 89.648% (4590/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 89\n",
            "train: [39/391] Loss: 0.102 | Acc: 96.445% (4938/5120)\n",
            "train: [79/391] Loss: 0.108 | Acc: 96.299% (9861/10240)\n",
            "train: [119/391] Loss: 0.108 | Acc: 96.237% (14782/15360)\n",
            "train: [159/391] Loss: 0.105 | Acc: 96.304% (19723/20480)\n",
            "train: [199/391] Loss: 0.106 | Acc: 96.293% (24651/25600)\n",
            "train: [239/391] Loss: 0.106 | Acc: 96.296% (29582/30720)\n",
            "train: [279/391] Loss: 0.107 | Acc: 96.217% (34484/35840)\n",
            "train: [319/391] Loss: 0.107 | Acc: 96.213% (39409/40960)\n",
            "train: [359/391] Loss: 0.107 | Acc: 96.215% (44336/46080)\n",
            "val: [39/79] Loss: 0.375 | Acc: 89.883% (4602/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 90\n",
            "train: [39/391] Loss: 0.150 | Acc: 94.590% (4843/5120)\n",
            "train: [79/391] Loss: 0.185 | Acc: 93.291% (9553/10240)\n",
            "train: [119/391] Loss: 0.200 | Acc: 92.780% (14251/15360)\n",
            "train: [159/391] Loss: 0.204 | Acc: 92.690% (18983/20480)\n",
            "train: [199/391] Loss: 0.209 | Acc: 92.547% (23692/25600)\n",
            "train: [239/391] Loss: 0.210 | Acc: 92.539% (28428/30720)\n",
            "train: [279/391] Loss: 0.211 | Acc: 92.427% (33126/35840)\n",
            "train: [319/391] Loss: 0.215 | Acc: 92.275% (37796/40960)\n",
            "train: [359/391] Loss: 0.216 | Acc: 92.246% (42507/46080)\n",
            "val: [39/79] Loss: 0.405 | Acc: 87.852% (4498/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 91\n",
            "train: [39/391] Loss: 0.198 | Acc: 92.793% (4751/5120)\n",
            "train: [79/391] Loss: 0.200 | Acc: 92.695% (9492/10240)\n",
            "train: [119/391] Loss: 0.201 | Acc: 92.689% (14237/15360)\n",
            "train: [159/391] Loss: 0.205 | Acc: 92.480% (18940/20480)\n",
            "train: [199/391] Loss: 0.206 | Acc: 92.441% (23665/25600)\n",
            "train: [239/391] Loss: 0.209 | Acc: 92.337% (28366/30720)\n",
            "train: [279/391] Loss: 0.208 | Acc: 92.408% (33119/35840)\n",
            "train: [319/391] Loss: 0.207 | Acc: 92.390% (37843/40960)\n",
            "train: [359/391] Loss: 0.209 | Acc: 92.350% (42555/46080)\n",
            "val: [39/79] Loss: 0.402 | Acc: 88.066% (4509/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 92\n",
            "train: [39/391] Loss: 0.180 | Acc: 93.613% (4793/5120)\n",
            "train: [79/391] Loss: 0.182 | Acc: 93.496% (9574/10240)\n",
            "train: [119/391] Loss: 0.185 | Acc: 93.392% (14345/15360)\n",
            "train: [159/391] Loss: 0.188 | Acc: 93.267% (19101/20480)\n",
            "train: [199/391] Loss: 0.189 | Acc: 93.219% (23864/25600)\n",
            "train: [239/391] Loss: 0.192 | Acc: 93.112% (28604/30720)\n",
            "train: [279/391] Loss: 0.194 | Acc: 93.100% (33367/35840)\n",
            "train: [319/391] Loss: 0.195 | Acc: 93.059% (38117/40960)\n",
            "train: [359/391] Loss: 0.200 | Acc: 92.912% (42814/46080)\n",
            "val: [39/79] Loss: 0.462 | Acc: 85.703% (4388/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 93\n",
            "train: [39/391] Loss: 0.220 | Acc: 91.953% (4708/5120)\n",
            "train: [79/391] Loss: 0.214 | Acc: 92.314% (9453/10240)\n",
            "train: [119/391] Loss: 0.214 | Acc: 92.279% (14174/15360)\n",
            "train: [159/391] Loss: 0.208 | Acc: 92.534% (18951/20480)\n",
            "train: [199/391] Loss: 0.207 | Acc: 92.562% (23696/25600)\n",
            "train: [239/391] Loss: 0.207 | Acc: 92.578% (28440/30720)\n",
            "train: [279/391] Loss: 0.207 | Acc: 92.612% (33192/35840)\n",
            "train: [319/391] Loss: 0.204 | Acc: 92.678% (37961/40960)\n",
            "train: [359/391] Loss: 0.202 | Acc: 92.728% (42729/46080)\n",
            "val: [39/79] Loss: 0.386 | Acc: 88.652% (4539/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 94\n",
            "train: [39/391] Loss: 0.167 | Acc: 94.004% (4813/5120)\n",
            "train: [79/391] Loss: 0.165 | Acc: 94.199% (9646/10240)\n",
            "train: [119/391] Loss: 0.162 | Acc: 94.173% (14465/15360)\n",
            "train: [159/391] Loss: 0.163 | Acc: 94.175% (19287/20480)\n",
            "train: [199/391] Loss: 0.162 | Acc: 94.176% (24109/25600)\n",
            "train: [239/391] Loss: 0.162 | Acc: 94.167% (28928/30720)\n",
            "train: [279/391] Loss: 0.165 | Acc: 94.079% (33718/35840)\n",
            "train: [319/391] Loss: 0.166 | Acc: 94.084% (38537/40960)\n",
            "train: [359/391] Loss: 0.167 | Acc: 94.091% (43357/46080)\n",
            "val: [39/79] Loss: 0.382 | Acc: 88.926% (4553/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 95\n",
            "train: [39/391] Loss: 0.134 | Acc: 95.234% (4876/5120)\n",
            "train: [79/391] Loss: 0.137 | Acc: 95.127% (9741/10240)\n",
            "train: [119/391] Loss: 0.141 | Acc: 94.857% (14570/15360)\n",
            "train: [159/391] Loss: 0.142 | Acc: 94.888% (19433/20480)\n",
            "train: [199/391] Loss: 0.141 | Acc: 95.027% (24327/25600)\n",
            "train: [239/391] Loss: 0.143 | Acc: 94.935% (29164/30720)\n",
            "train: [279/391] Loss: 0.143 | Acc: 94.925% (34021/35840)\n",
            "train: [319/391] Loss: 0.144 | Acc: 94.878% (38862/40960)\n",
            "train: [359/391] Loss: 0.144 | Acc: 94.850% (43707/46080)\n",
            "val: [39/79] Loss: 0.370 | Acc: 89.902% (4603/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 96\n",
            "train: [39/391] Loss: 0.135 | Acc: 94.941% (4861/5120)\n",
            "train: [79/391] Loss: 0.128 | Acc: 95.186% (9747/10240)\n",
            "train: [119/391] Loss: 0.124 | Acc: 95.404% (14654/15360)\n",
            "train: [159/391] Loss: 0.123 | Acc: 95.498% (19558/20480)\n",
            "train: [199/391] Loss: 0.123 | Acc: 95.543% (24459/25600)\n",
            "train: [239/391] Loss: 0.123 | Acc: 95.521% (29344/30720)\n",
            "train: [279/391] Loss: 0.123 | Acc: 95.539% (34241/35840)\n",
            "train: [319/391] Loss: 0.124 | Acc: 95.544% (39135/40960)\n",
            "train: [359/391] Loss: 0.124 | Acc: 95.534% (44022/46080)\n",
            "val: [39/79] Loss: 0.394 | Acc: 89.355% (4575/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 97\n",
            "train: [39/391] Loss: 0.104 | Acc: 96.621% (4947/5120)\n",
            "train: [79/391] Loss: 0.106 | Acc: 96.445% (9876/10240)\n",
            "train: [119/391] Loss: 0.109 | Acc: 96.341% (14798/15360)\n",
            "train: [159/391] Loss: 0.110 | Acc: 96.162% (19694/20480)\n",
            "train: [199/391] Loss: 0.110 | Acc: 96.137% (24611/25600)\n",
            "train: [239/391] Loss: 0.110 | Acc: 96.094% (29520/30720)\n",
            "train: [279/391] Loss: 0.112 | Acc: 95.960% (34392/35840)\n",
            "train: [319/391] Loss: 0.111 | Acc: 95.984% (39315/40960)\n",
            "train: [359/391] Loss: 0.111 | Acc: 96.026% (44249/46080)\n",
            "val: [39/79] Loss: 0.383 | Acc: 89.746% (4595/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 98\n",
            "train: [39/391] Loss: 0.099 | Acc: 96.680% (4950/5120)\n",
            "train: [79/391] Loss: 0.097 | Acc: 96.768% (9909/10240)\n",
            "train: [119/391] Loss: 0.098 | Acc: 96.556% (14831/15360)\n",
            "train: [159/391] Loss: 0.101 | Acc: 96.436% (19750/20480)\n",
            "train: [199/391] Loss: 0.101 | Acc: 96.453% (24692/25600)\n",
            "train: [239/391] Loss: 0.100 | Acc: 96.439% (29626/30720)\n",
            "train: [279/391] Loss: 0.101 | Acc: 96.406% (34552/35840)\n",
            "train: [319/391] Loss: 0.099 | Acc: 96.450% (39506/40960)\n",
            "train: [359/391] Loss: 0.099 | Acc: 96.465% (44451/46080)\n",
            "val: [39/79] Loss: 0.393 | Acc: 90.059% (4611/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 99\n",
            "train: [39/391] Loss: 0.084 | Acc: 97.207% (4977/5120)\n",
            "train: [79/391] Loss: 0.090 | Acc: 97.012% (9934/10240)\n",
            "train: [119/391] Loss: 0.088 | Acc: 97.038% (14905/15360)\n",
            "train: [159/391] Loss: 0.091 | Acc: 96.904% (19846/20480)\n",
            "train: [199/391] Loss: 0.092 | Acc: 96.859% (24796/25600)\n",
            "train: [239/391] Loss: 0.093 | Acc: 96.794% (29735/30720)\n",
            "train: [279/391] Loss: 0.091 | Acc: 96.814% (34698/35840)\n",
            "train: [319/391] Loss: 0.092 | Acc: 96.802% (39650/40960)\n",
            "train: [359/391] Loss: 0.091 | Acc: 96.786% (44599/46080)\n",
            "val: [39/79] Loss: 0.384 | Acc: 90.039% (4610/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 100\n",
            "train: [39/391] Loss: 0.136 | Acc: 95.352% (4882/5120)\n",
            "train: [79/391] Loss: 0.161 | Acc: 94.453% (9672/10240)\n",
            "train: [119/391] Loss: 0.168 | Acc: 94.128% (14458/15360)\n",
            "train: [159/391] Loss: 0.173 | Acc: 93.843% (19219/20480)\n",
            "train: [199/391] Loss: 0.178 | Acc: 93.676% (23981/25600)\n",
            "train: [239/391] Loss: 0.183 | Acc: 93.509% (28726/30720)\n",
            "train: [279/391] Loss: 0.186 | Acc: 93.379% (33467/35840)\n",
            "train: [319/391] Loss: 0.186 | Acc: 93.379% (38248/40960)\n",
            "train: [359/391] Loss: 0.188 | Acc: 93.309% (42997/46080)\n",
            "val: [39/79] Loss: 0.415 | Acc: 87.578% (4484/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 101\n",
            "train: [39/391] Loss: 0.194 | Acc: 93.027% (4763/5120)\n",
            "train: [79/391] Loss: 0.188 | Acc: 93.223% (9546/10240)\n",
            "train: [119/391] Loss: 0.188 | Acc: 93.177% (14312/15360)\n",
            "train: [159/391] Loss: 0.186 | Acc: 93.223% (19092/20480)\n",
            "train: [199/391] Loss: 0.187 | Acc: 93.223% (23865/25600)\n",
            "train: [239/391] Loss: 0.188 | Acc: 93.190% (28628/30720)\n",
            "train: [279/391] Loss: 0.187 | Acc: 93.253% (33422/35840)\n",
            "train: [319/391] Loss: 0.189 | Acc: 93.154% (38156/40960)\n",
            "train: [359/391] Loss: 0.192 | Acc: 93.064% (42884/46080)\n",
            "val: [39/79] Loss: 0.393 | Acc: 88.301% (4521/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 102\n",
            "train: [39/391] Loss: 0.165 | Acc: 93.730% (4799/5120)\n",
            "train: [79/391] Loss: 0.171 | Acc: 93.799% (9605/10240)\n",
            "train: [119/391] Loss: 0.172 | Acc: 93.737% (14398/15360)\n",
            "train: [159/391] Loss: 0.176 | Acc: 93.687% (19187/20480)\n",
            "train: [199/391] Loss: 0.177 | Acc: 93.594% (23960/25600)\n",
            "train: [239/391] Loss: 0.178 | Acc: 93.604% (28755/30720)\n",
            "train: [279/391] Loss: 0.177 | Acc: 93.616% (33552/35840)\n",
            "train: [319/391] Loss: 0.177 | Acc: 93.623% (38348/40960)\n",
            "train: [359/391] Loss: 0.180 | Acc: 93.553% (43109/46080)\n",
            "val: [39/79] Loss: 0.416 | Acc: 88.691% (4541/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 103\n",
            "train: [39/391] Loss: 0.170 | Acc: 93.750% (4800/5120)\n",
            "train: [79/391] Loss: 0.163 | Acc: 94.248% (9651/10240)\n",
            "train: [119/391] Loss: 0.166 | Acc: 94.232% (14474/15360)\n",
            "train: [159/391] Loss: 0.170 | Acc: 94.023% (19256/20480)\n",
            "train: [199/391] Loss: 0.167 | Acc: 94.105% (24091/25600)\n",
            "train: [239/391] Loss: 0.167 | Acc: 94.102% (28908/30720)\n",
            "train: [279/391] Loss: 0.167 | Acc: 94.174% (33752/35840)\n",
            "train: [319/391] Loss: 0.167 | Acc: 94.126% (38554/40960)\n",
            "train: [359/391] Loss: 0.168 | Acc: 94.102% (43362/46080)\n",
            "val: [39/79] Loss: 0.405 | Acc: 88.359% (4524/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 104\n",
            "train: [39/391] Loss: 0.154 | Acc: 94.336% (4830/5120)\n",
            "train: [79/391] Loss: 0.151 | Acc: 94.434% (9670/10240)\n",
            "train: [119/391] Loss: 0.144 | Acc: 94.720% (14549/15360)\n",
            "train: [159/391] Loss: 0.146 | Acc: 94.761% (19407/20480)\n",
            "train: [199/391] Loss: 0.145 | Acc: 94.785% (24265/25600)\n",
            "train: [239/391] Loss: 0.148 | Acc: 94.694% (29090/30720)\n",
            "train: [279/391] Loss: 0.147 | Acc: 94.721% (33948/35840)\n",
            "train: [319/391] Loss: 0.146 | Acc: 94.702% (38790/40960)\n",
            "train: [359/391] Loss: 0.147 | Acc: 94.692% (43634/46080)\n",
            "val: [39/79] Loss: 0.419 | Acc: 88.516% (4532/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 105\n",
            "train: [39/391] Loss: 0.137 | Acc: 94.961% (4862/5120)\n",
            "train: [79/391] Loss: 0.136 | Acc: 95.156% (9744/10240)\n",
            "train: [119/391] Loss: 0.134 | Acc: 95.221% (14626/15360)\n",
            "train: [159/391] Loss: 0.133 | Acc: 95.171% (19491/20480)\n",
            "train: [199/391] Loss: 0.131 | Acc: 95.293% (24395/25600)\n",
            "train: [239/391] Loss: 0.131 | Acc: 95.345% (29290/30720)\n",
            "train: [279/391] Loss: 0.130 | Acc: 95.371% (34181/35840)\n",
            "train: [319/391] Loss: 0.130 | Acc: 95.364% (39061/40960)\n",
            "train: [359/391] Loss: 0.129 | Acc: 95.397% (43959/46080)\n",
            "val: [39/79] Loss: 0.406 | Acc: 88.965% (4555/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 106\n",
            "train: [39/391] Loss: 0.117 | Acc: 96.016% (4916/5120)\n",
            "train: [79/391] Loss: 0.113 | Acc: 96.143% (9845/10240)\n",
            "train: [119/391] Loss: 0.112 | Acc: 96.107% (14762/15360)\n",
            "train: [159/391] Loss: 0.110 | Acc: 96.157% (19693/20480)\n",
            "train: [199/391] Loss: 0.112 | Acc: 96.051% (24589/25600)\n",
            "train: [239/391] Loss: 0.111 | Acc: 96.068% (29512/30720)\n",
            "train: [279/391] Loss: 0.111 | Acc: 96.110% (34446/35840)\n",
            "train: [319/391] Loss: 0.110 | Acc: 96.128% (39374/40960)\n",
            "train: [359/391] Loss: 0.110 | Acc: 96.152% (44307/46080)\n",
            "val: [39/79] Loss: 0.404 | Acc: 89.766% (4596/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 107\n",
            "train: [39/391] Loss: 0.097 | Acc: 96.738% (4953/5120)\n",
            "train: [79/391] Loss: 0.099 | Acc: 96.611% (9893/10240)\n",
            "train: [119/391] Loss: 0.096 | Acc: 96.712% (14855/15360)\n",
            "train: [159/391] Loss: 0.097 | Acc: 96.577% (19779/20480)\n",
            "train: [199/391] Loss: 0.096 | Acc: 96.621% (24735/25600)\n",
            "train: [239/391] Loss: 0.096 | Acc: 96.615% (29680/30720)\n",
            "train: [279/391] Loss: 0.095 | Acc: 96.602% (34622/35840)\n",
            "train: [319/391] Loss: 0.096 | Acc: 96.575% (39557/40960)\n",
            "train: [359/391] Loss: 0.096 | Acc: 96.558% (44494/46080)\n",
            "val: [39/79] Loss: 0.417 | Acc: 89.258% (4570/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 108\n",
            "train: [39/391] Loss: 0.085 | Acc: 97.188% (4976/5120)\n",
            "train: [79/391] Loss: 0.085 | Acc: 97.070% (9940/10240)\n",
            "train: [119/391] Loss: 0.086 | Acc: 96.973% (14895/15360)\n",
            "train: [159/391] Loss: 0.086 | Acc: 96.929% (19851/20480)\n",
            "train: [199/391] Loss: 0.088 | Acc: 96.887% (24803/25600)\n",
            "train: [239/391] Loss: 0.087 | Acc: 96.921% (29774/30720)\n",
            "train: [279/391] Loss: 0.086 | Acc: 96.948% (34746/35840)\n",
            "train: [319/391] Loss: 0.086 | Acc: 96.975% (39721/40960)\n",
            "train: [359/391] Loss: 0.086 | Acc: 96.968% (44683/46080)\n",
            "val: [39/79] Loss: 0.416 | Acc: 89.590% (4587/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 109\n",
            "train: [39/391] Loss: 0.087 | Acc: 96.797% (4956/5120)\n",
            "train: [79/391] Loss: 0.082 | Acc: 97.100% (9943/10240)\n",
            "train: [119/391] Loss: 0.080 | Acc: 97.220% (14933/15360)\n",
            "train: [159/391] Loss: 0.080 | Acc: 97.241% (19915/20480)\n",
            "train: [199/391] Loss: 0.080 | Acc: 97.176% (24877/25600)\n",
            "train: [239/391] Loss: 0.080 | Acc: 97.210% (29863/30720)\n",
            "train: [279/391] Loss: 0.079 | Acc: 97.193% (34834/35840)\n",
            "train: [319/391] Loss: 0.079 | Acc: 97.219% (39821/40960)\n",
            "train: [359/391] Loss: 0.079 | Acc: 97.214% (44796/46080)\n",
            "val: [39/79] Loss: 0.410 | Acc: 89.570% (4586/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 110\n",
            "train: [39/391] Loss: 0.114 | Acc: 96.191% (4925/5120)\n",
            "train: [79/391] Loss: 0.142 | Acc: 94.951% (9723/10240)\n",
            "train: [119/391] Loss: 0.156 | Acc: 94.401% (14500/15360)\n",
            "train: [159/391] Loss: 0.162 | Acc: 94.282% (19309/20480)\n",
            "train: [199/391] Loss: 0.168 | Acc: 94.000% (24064/25600)\n",
            "train: [239/391] Loss: 0.170 | Acc: 93.929% (28855/30720)\n",
            "train: [279/391] Loss: 0.176 | Acc: 93.708% (33585/35840)\n",
            "train: [319/391] Loss: 0.177 | Acc: 93.674% (38369/40960)\n",
            "train: [359/391] Loss: 0.177 | Acc: 93.696% (43175/46080)\n",
            "val: [39/79] Loss: 0.417 | Acc: 88.066% (4509/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 111\n",
            "train: [39/391] Loss: 0.172 | Acc: 93.809% (4803/5120)\n",
            "train: [79/391] Loss: 0.169 | Acc: 93.936% (9619/10240)\n",
            "train: [119/391] Loss: 0.174 | Acc: 93.717% (14395/15360)\n",
            "train: [159/391] Loss: 0.173 | Acc: 93.804% (19211/20480)\n",
            "train: [199/391] Loss: 0.174 | Acc: 93.789% (24010/25600)\n",
            "train: [239/391] Loss: 0.175 | Acc: 93.737% (28796/30720)\n",
            "train: [279/391] Loss: 0.176 | Acc: 93.711% (33586/35840)\n",
            "train: [319/391] Loss: 0.177 | Acc: 93.677% (38370/40960)\n",
            "train: [359/391] Loss: 0.177 | Acc: 93.674% (43165/46080)\n",
            "val: [39/79] Loss: 0.402 | Acc: 89.004% (4557/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 112\n",
            "train: [39/391] Loss: 0.146 | Acc: 94.961% (4862/5120)\n",
            "train: [79/391] Loss: 0.156 | Acc: 94.707% (9698/10240)\n",
            "train: [119/391] Loss: 0.159 | Acc: 94.486% (14513/15360)\n",
            "train: [159/391] Loss: 0.158 | Acc: 94.507% (19355/20480)\n",
            "train: [199/391] Loss: 0.158 | Acc: 94.508% (24194/25600)\n",
            "train: [239/391] Loss: 0.159 | Acc: 94.447% (29014/30720)\n",
            "train: [279/391] Loss: 0.159 | Acc: 94.411% (33837/35840)\n",
            "train: [319/391] Loss: 0.160 | Acc: 94.407% (38669/40960)\n",
            "train: [359/391] Loss: 0.161 | Acc: 94.390% (43495/46080)\n",
            "val: [39/79] Loss: 0.413 | Acc: 88.359% (4524/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 113\n",
            "train: [39/391] Loss: 0.147 | Acc: 94.551% (4841/5120)\n",
            "train: [79/391] Loss: 0.149 | Acc: 94.561% (9683/10240)\n",
            "train: [119/391] Loss: 0.149 | Acc: 94.609% (14532/15360)\n",
            "train: [159/391] Loss: 0.150 | Acc: 94.502% (19354/20480)\n",
            "train: [199/391] Loss: 0.150 | Acc: 94.551% (24205/25600)\n",
            "train: [239/391] Loss: 0.149 | Acc: 94.603% (29062/30720)\n",
            "train: [279/391] Loss: 0.152 | Acc: 94.512% (33873/35840)\n",
            "train: [319/391] Loss: 0.154 | Acc: 94.441% (38683/40960)\n",
            "train: [359/391] Loss: 0.155 | Acc: 94.416% (43507/46080)\n",
            "val: [39/79] Loss: 0.415 | Acc: 88.262% (4519/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 114\n",
            "train: [39/391] Loss: 0.112 | Acc: 96.133% (4922/5120)\n",
            "train: [79/391] Loss: 0.124 | Acc: 95.654% (9795/10240)\n",
            "train: [119/391] Loss: 0.126 | Acc: 95.514% (14671/15360)\n",
            "train: [159/391] Loss: 0.128 | Acc: 95.444% (19547/20480)\n",
            "train: [199/391] Loss: 0.131 | Acc: 95.316% (24401/25600)\n",
            "train: [239/391] Loss: 0.131 | Acc: 95.332% (29286/30720)\n",
            "train: [279/391] Loss: 0.130 | Acc: 95.385% (34186/35840)\n",
            "train: [319/391] Loss: 0.132 | Acc: 95.310% (39039/40960)\n",
            "train: [359/391] Loss: 0.132 | Acc: 95.291% (43910/46080)\n",
            "val: [39/79] Loss: 0.403 | Acc: 89.102% (4562/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 115\n",
            "train: [39/391] Loss: 0.115 | Acc: 96.113% (4921/5120)\n",
            "train: [79/391] Loss: 0.119 | Acc: 95.967% (9827/10240)\n",
            "train: [119/391] Loss: 0.118 | Acc: 96.003% (14746/15360)\n",
            "train: [159/391] Loss: 0.114 | Acc: 96.113% (19684/20480)\n",
            "train: [199/391] Loss: 0.114 | Acc: 96.062% (24592/25600)\n",
            "train: [239/391] Loss: 0.114 | Acc: 96.068% (29512/30720)\n",
            "train: [279/391] Loss: 0.113 | Acc: 96.097% (34441/35840)\n",
            "train: [319/391] Loss: 0.113 | Acc: 96.118% (39370/40960)\n",
            "train: [359/391] Loss: 0.113 | Acc: 96.068% (44268/46080)\n",
            "val: [39/79] Loss: 0.411 | Acc: 89.238% (4569/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 116\n",
            "train: [39/391] Loss: 0.102 | Acc: 96.094% (4920/5120)\n",
            "train: [79/391] Loss: 0.098 | Acc: 96.240% (9855/10240)\n",
            "train: [119/391] Loss: 0.101 | Acc: 96.354% (14800/15360)\n",
            "train: [159/391] Loss: 0.102 | Acc: 96.348% (19732/20480)\n",
            "train: [199/391] Loss: 0.102 | Acc: 96.336% (24662/25600)\n",
            "train: [239/391] Loss: 0.102 | Acc: 96.361% (29602/30720)\n",
            "train: [279/391] Loss: 0.101 | Acc: 96.398% (34549/35840)\n",
            "train: [319/391] Loss: 0.099 | Acc: 96.445% (39504/40960)\n",
            "train: [359/391] Loss: 0.099 | Acc: 96.476% (44456/46080)\n",
            "val: [39/79] Loss: 0.416 | Acc: 89.277% (4571/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 117\n",
            "train: [39/391] Loss: 0.099 | Acc: 96.504% (4941/5120)\n",
            "train: [79/391] Loss: 0.095 | Acc: 96.660% (9898/10240)\n",
            "train: [119/391] Loss: 0.090 | Acc: 96.882% (14881/15360)\n",
            "train: [159/391] Loss: 0.091 | Acc: 96.851% (19835/20480)\n",
            "train: [199/391] Loss: 0.088 | Acc: 96.891% (24804/25600)\n",
            "train: [239/391] Loss: 0.089 | Acc: 96.852% (29753/30720)\n",
            "train: [279/391] Loss: 0.089 | Acc: 96.833% (34705/35840)\n",
            "train: [319/391] Loss: 0.088 | Acc: 96.846% (39668/40960)\n",
            "train: [359/391] Loss: 0.088 | Acc: 96.834% (44621/46080)\n",
            "val: [39/79] Loss: 0.419 | Acc: 89.609% (4588/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 118\n",
            "train: [39/391] Loss: 0.071 | Acc: 97.520% (4993/5120)\n",
            "train: [79/391] Loss: 0.070 | Acc: 97.520% (9986/10240)\n",
            "train: [119/391] Loss: 0.070 | Acc: 97.526% (14980/15360)\n",
            "train: [159/391] Loss: 0.071 | Acc: 97.534% (19975/20480)\n",
            "train: [199/391] Loss: 0.072 | Acc: 97.465% (24951/25600)\n",
            "train: [239/391] Loss: 0.071 | Acc: 97.497% (29951/30720)\n",
            "train: [279/391] Loss: 0.070 | Acc: 97.511% (34948/35840)\n",
            "train: [319/391] Loss: 0.071 | Acc: 97.483% (39929/40960)\n",
            "train: [359/391] Loss: 0.071 | Acc: 97.461% (44910/46080)\n",
            "val: [39/79] Loss: 0.415 | Acc: 89.922% (4604/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 119\n",
            "train: [39/391] Loss: 0.071 | Acc: 97.598% (4997/5120)\n",
            "train: [79/391] Loss: 0.065 | Acc: 97.812% (10016/10240)\n",
            "train: [119/391] Loss: 0.066 | Acc: 97.708% (15008/15360)\n",
            "train: [159/391] Loss: 0.067 | Acc: 97.729% (20015/20480)\n",
            "train: [199/391] Loss: 0.066 | Acc: 97.762% (25027/25600)\n",
            "train: [239/391] Loss: 0.065 | Acc: 97.757% (30031/30720)\n",
            "train: [279/391] Loss: 0.066 | Acc: 97.740% (35030/35840)\n",
            "train: [319/391] Loss: 0.067 | Acc: 97.703% (40019/40960)\n",
            "train: [359/391] Loss: 0.067 | Acc: 97.719% (45029/46080)\n",
            "val: [39/79] Loss: 0.413 | Acc: 89.785% (4597/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 120\n",
            "train: [39/391] Loss: 0.105 | Acc: 96.328% (4932/5120)\n",
            "train: [79/391] Loss: 0.119 | Acc: 95.684% (9798/10240)\n",
            "train: [119/391] Loss: 0.141 | Acc: 94.948% (14584/15360)\n",
            "train: [159/391] Loss: 0.152 | Acc: 94.546% (19363/20480)\n",
            "train: [199/391] Loss: 0.156 | Acc: 94.414% (24170/25600)\n",
            "train: [239/391] Loss: 0.155 | Acc: 94.466% (29020/30720)\n",
            "train: [279/391] Loss: 0.158 | Acc: 94.311% (33801/35840)\n",
            "train: [319/391] Loss: 0.161 | Acc: 94.265% (38611/40960)\n",
            "train: [359/391] Loss: 0.165 | Acc: 94.180% (43398/46080)\n",
            "val: [39/79] Loss: 0.428 | Acc: 87.910% (4501/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 121\n",
            "train: [39/391] Loss: 0.153 | Acc: 94.551% (4841/5120)\n",
            "train: [79/391] Loss: 0.156 | Acc: 94.463% (9673/10240)\n",
            "train: [119/391] Loss: 0.159 | Acc: 94.238% (14475/15360)\n",
            "train: [159/391] Loss: 0.160 | Acc: 94.277% (19308/20480)\n",
            "train: [199/391] Loss: 0.160 | Acc: 94.289% (24138/25600)\n",
            "train: [239/391] Loss: 0.162 | Acc: 94.229% (28947/30720)\n",
            "train: [279/391] Loss: 0.161 | Acc: 94.252% (33780/35840)\n",
            "train: [319/391] Loss: 0.163 | Acc: 94.207% (38587/40960)\n",
            "train: [359/391] Loss: 0.164 | Acc: 94.171% (43394/46080)\n",
            "val: [39/79] Loss: 0.417 | Acc: 88.379% (4525/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 122\n",
            "train: [39/391] Loss: 0.145 | Acc: 94.668% (4847/5120)\n",
            "train: [79/391] Loss: 0.144 | Acc: 94.893% (9717/10240)\n",
            "train: [119/391] Loss: 0.144 | Acc: 94.883% (14574/15360)\n",
            "train: [159/391] Loss: 0.146 | Acc: 94.897% (19435/20480)\n",
            "train: [199/391] Loss: 0.148 | Acc: 94.781% (24264/25600)\n",
            "train: [239/391] Loss: 0.149 | Acc: 94.746% (29106/30720)\n",
            "train: [279/391] Loss: 0.151 | Acc: 94.634% (33917/35840)\n",
            "train: [319/391] Loss: 0.151 | Acc: 94.622% (38757/40960)\n",
            "train: [359/391] Loss: 0.151 | Acc: 94.618% (43600/46080)\n",
            "val: [39/79] Loss: 0.428 | Acc: 88.242% (4518/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 123\n",
            "train: [39/391] Loss: 0.147 | Acc: 94.844% (4856/5120)\n",
            "train: [79/391] Loss: 0.142 | Acc: 94.893% (9717/10240)\n",
            "train: [119/391] Loss: 0.139 | Acc: 95.059% (14601/15360)\n",
            "train: [159/391] Loss: 0.136 | Acc: 95.146% (19486/20480)\n",
            "train: [199/391] Loss: 0.135 | Acc: 95.227% (24378/25600)\n",
            "train: [239/391] Loss: 0.134 | Acc: 95.254% (29262/30720)\n",
            "train: [279/391] Loss: 0.135 | Acc: 95.218% (34126/35840)\n",
            "train: [319/391] Loss: 0.136 | Acc: 95.171% (38982/40960)\n",
            "train: [359/391] Loss: 0.137 | Acc: 95.152% (43846/46080)\n",
            "val: [39/79] Loss: 0.404 | Acc: 88.867% (4550/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 124\n",
            "train: [39/391] Loss: 0.138 | Acc: 94.766% (4852/5120)\n",
            "train: [79/391] Loss: 0.131 | Acc: 95.176% (9746/10240)\n",
            "train: [119/391] Loss: 0.130 | Acc: 95.208% (14624/15360)\n",
            "train: [159/391] Loss: 0.125 | Acc: 95.391% (19536/20480)\n",
            "train: [199/391] Loss: 0.124 | Acc: 95.516% (24452/25600)\n",
            "train: [239/391] Loss: 0.124 | Acc: 95.534% (29348/30720)\n",
            "train: [279/391] Loss: 0.123 | Acc: 95.603% (34264/35840)\n",
            "train: [319/391] Loss: 0.123 | Acc: 95.579% (39149/40960)\n",
            "train: [359/391] Loss: 0.123 | Acc: 95.590% (44048/46080)\n",
            "val: [39/79] Loss: 0.400 | Acc: 88.887% (4551/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 125\n",
            "train: [39/391] Loss: 0.106 | Acc: 96.270% (4929/5120)\n",
            "train: [79/391] Loss: 0.108 | Acc: 96.182% (9849/10240)\n",
            "train: [119/391] Loss: 0.106 | Acc: 96.230% (14781/15360)\n",
            "train: [159/391] Loss: 0.105 | Acc: 96.255% (19713/20480)\n",
            "train: [199/391] Loss: 0.105 | Acc: 96.258% (24642/25600)\n",
            "train: [239/391] Loss: 0.106 | Acc: 96.299% (29583/30720)\n",
            "train: [279/391] Loss: 0.105 | Acc: 96.320% (34521/35840)\n",
            "train: [319/391] Loss: 0.104 | Acc: 96.321% (39453/40960)\n",
            "train: [359/391] Loss: 0.105 | Acc: 96.287% (44369/46080)\n",
            "val: [39/79] Loss: 0.432 | Acc: 89.043% (4559/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 126\n",
            "train: [39/391] Loss: 0.096 | Acc: 96.582% (4945/5120)\n",
            "train: [79/391] Loss: 0.093 | Acc: 96.611% (9893/10240)\n",
            "train: [119/391] Loss: 0.092 | Acc: 96.673% (14849/15360)\n",
            "train: [159/391] Loss: 0.092 | Acc: 96.719% (19808/20480)\n",
            "train: [199/391] Loss: 0.090 | Acc: 96.812% (24784/25600)\n",
            "train: [239/391] Loss: 0.088 | Acc: 96.878% (29761/30720)\n",
            "train: [279/391] Loss: 0.088 | Acc: 96.883% (34723/35840)\n",
            "train: [319/391] Loss: 0.090 | Acc: 96.814% (39655/40960)\n",
            "train: [359/391] Loss: 0.090 | Acc: 96.806% (44608/46080)\n",
            "val: [39/79] Loss: 0.427 | Acc: 89.531% (4584/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 127\n",
            "train: [39/391] Loss: 0.077 | Acc: 97.383% (4986/5120)\n",
            "train: [79/391] Loss: 0.073 | Acc: 97.402% (9974/10240)\n",
            "train: [119/391] Loss: 0.074 | Acc: 97.298% (14945/15360)\n",
            "train: [159/391] Loss: 0.075 | Acc: 97.271% (19921/20480)\n",
            "train: [199/391] Loss: 0.075 | Acc: 97.281% (24904/25600)\n",
            "train: [239/391] Loss: 0.074 | Acc: 97.347% (29905/30720)\n",
            "train: [279/391] Loss: 0.074 | Acc: 97.355% (34892/35840)\n",
            "train: [319/391] Loss: 0.074 | Acc: 97.351% (39875/40960)\n",
            "train: [359/391] Loss: 0.074 | Acc: 97.370% (44868/46080)\n",
            "val: [39/79] Loss: 0.442 | Acc: 89.492% (4582/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 128\n",
            "train: [39/391] Loss: 0.067 | Acc: 97.949% (5015/5120)\n",
            "train: [79/391] Loss: 0.069 | Acc: 97.676% (10002/10240)\n",
            "train: [119/391] Loss: 0.068 | Acc: 97.728% (15011/15360)\n",
            "train: [159/391] Loss: 0.067 | Acc: 97.705% (20010/20480)\n",
            "train: [199/391] Loss: 0.066 | Acc: 97.688% (25008/25600)\n",
            "train: [239/391] Loss: 0.064 | Acc: 97.760% (30032/30720)\n",
            "train: [279/391] Loss: 0.065 | Acc: 97.751% (35034/35840)\n",
            "train: [319/391] Loss: 0.064 | Acc: 97.781% (40051/40960)\n",
            "train: [359/391] Loss: 0.065 | Acc: 97.767% (45051/46080)\n",
            "val: [39/79] Loss: 0.430 | Acc: 89.746% (4595/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 129\n",
            "train: [39/391] Loss: 0.058 | Acc: 97.891% (5012/5120)\n",
            "train: [79/391] Loss: 0.061 | Acc: 97.803% (10015/10240)\n",
            "train: [119/391] Loss: 0.061 | Acc: 97.793% (15021/15360)\n",
            "train: [159/391] Loss: 0.061 | Acc: 97.832% (20036/20480)\n",
            "train: [199/391] Loss: 0.059 | Acc: 97.910% (25065/25600)\n",
            "train: [239/391] Loss: 0.060 | Acc: 97.894% (30073/30720)\n",
            "train: [279/391] Loss: 0.059 | Acc: 97.905% (35089/35840)\n",
            "train: [319/391] Loss: 0.058 | Acc: 97.920% (40108/40960)\n",
            "train: [359/391] Loss: 0.059 | Acc: 97.895% (45110/46080)\n",
            "val: [39/79] Loss: 0.431 | Acc: 90.117% (4614/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 130\n",
            "train: [39/391] Loss: 0.096 | Acc: 96.699% (4951/5120)\n",
            "train: [79/391] Loss: 0.124 | Acc: 95.566% (9786/10240)\n",
            "train: [119/391] Loss: 0.140 | Acc: 95.098% (14607/15360)\n",
            "train: [159/391] Loss: 0.143 | Acc: 94.976% (19451/20480)\n",
            "train: [199/391] Loss: 0.145 | Acc: 94.840% (24279/25600)\n",
            "train: [239/391] Loss: 0.145 | Acc: 94.844% (29136/30720)\n",
            "train: [279/391] Loss: 0.148 | Acc: 94.710% (33944/35840)\n",
            "train: [319/391] Loss: 0.149 | Acc: 94.629% (38760/40960)\n",
            "train: [359/391] Loss: 0.150 | Acc: 94.625% (43603/46080)\n",
            "val: [39/79] Loss: 0.447 | Acc: 88.223% (4517/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 131\n",
            "train: [39/391] Loss: 0.152 | Acc: 94.316% (4829/5120)\n",
            "train: [79/391] Loss: 0.149 | Acc: 94.482% (9675/10240)\n",
            "train: [119/391] Loss: 0.143 | Acc: 94.766% (14556/15360)\n",
            "train: [159/391] Loss: 0.143 | Acc: 94.785% (19412/20480)\n",
            "train: [199/391] Loss: 0.146 | Acc: 94.668% (24235/25600)\n",
            "train: [239/391] Loss: 0.148 | Acc: 94.616% (29066/30720)\n",
            "train: [279/391] Loss: 0.148 | Acc: 94.626% (33914/35840)\n",
            "train: [319/391] Loss: 0.149 | Acc: 94.624% (38758/40960)\n",
            "train: [359/391] Loss: 0.150 | Acc: 94.581% (43583/46080)\n",
            "val: [39/79] Loss: 0.412 | Acc: 88.691% (4541/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 132\n",
            "train: [39/391] Loss: 0.119 | Acc: 95.625% (4896/5120)\n",
            "train: [79/391] Loss: 0.128 | Acc: 95.391% (9768/10240)\n",
            "train: [119/391] Loss: 0.135 | Acc: 95.208% (14624/15360)\n",
            "train: [159/391] Loss: 0.135 | Acc: 95.288% (19515/20480)\n",
            "train: [199/391] Loss: 0.135 | Acc: 95.242% (24382/25600)\n",
            "train: [239/391] Loss: 0.139 | Acc: 95.169% (29236/30720)\n",
            "train: [279/391] Loss: 0.141 | Acc: 95.045% (34064/35840)\n",
            "train: [319/391] Loss: 0.142 | Acc: 95.012% (38917/40960)\n",
            "train: [359/391] Loss: 0.142 | Acc: 95.002% (43777/46080)\n",
            "val: [39/79] Loss: 0.452 | Acc: 87.754% (4493/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 133\n",
            "train: [39/391] Loss: 0.121 | Acc: 95.605% (4895/5120)\n",
            "train: [79/391] Loss: 0.122 | Acc: 95.576% (9787/10240)\n",
            "train: [119/391] Loss: 0.124 | Acc: 95.645% (14691/15360)\n",
            "train: [159/391] Loss: 0.127 | Acc: 95.513% (19561/20480)\n",
            "train: [199/391] Loss: 0.127 | Acc: 95.383% (24418/25600)\n",
            "train: [239/391] Loss: 0.126 | Acc: 95.465% (29327/30720)\n",
            "train: [279/391] Loss: 0.126 | Acc: 95.460% (34213/35840)\n",
            "train: [319/391] Loss: 0.126 | Acc: 95.471% (39105/40960)\n",
            "train: [359/391] Loss: 0.127 | Acc: 95.484% (43999/46080)\n",
            "val: [39/79] Loss: 0.408 | Acc: 88.613% (4537/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 134\n",
            "train: [39/391] Loss: 0.128 | Acc: 95.410% (4885/5120)\n",
            "train: [79/391] Loss: 0.121 | Acc: 95.518% (9781/10240)\n",
            "train: [119/391] Loss: 0.120 | Acc: 95.625% (14688/15360)\n",
            "train: [159/391] Loss: 0.119 | Acc: 95.684% (19596/20480)\n",
            "train: [199/391] Loss: 0.116 | Acc: 95.789% (24522/25600)\n",
            "train: [239/391] Loss: 0.115 | Acc: 95.863% (29449/30720)\n",
            "train: [279/391] Loss: 0.113 | Acc: 95.935% (34383/35840)\n",
            "train: [319/391] Loss: 0.112 | Acc: 95.962% (39306/40960)\n",
            "train: [359/391] Loss: 0.111 | Acc: 95.979% (44227/46080)\n",
            "val: [39/79] Loss: 0.418 | Acc: 89.258% (4570/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 135\n",
            "train: [39/391] Loss: 0.094 | Acc: 96.621% (4947/5120)\n",
            "train: [79/391] Loss: 0.094 | Acc: 96.641% (9896/10240)\n",
            "train: [119/391] Loss: 0.096 | Acc: 96.556% (14831/15360)\n",
            "train: [159/391] Loss: 0.093 | Acc: 96.621% (19788/20480)\n",
            "train: [199/391] Loss: 0.092 | Acc: 96.684% (24751/25600)\n",
            "train: [239/391] Loss: 0.093 | Acc: 96.628% (29684/30720)\n",
            "train: [279/391] Loss: 0.093 | Acc: 96.649% (34639/35840)\n",
            "train: [319/391] Loss: 0.093 | Acc: 96.638% (39583/40960)\n",
            "train: [359/391] Loss: 0.093 | Acc: 96.649% (44536/46080)\n",
            "val: [39/79] Loss: 0.422 | Acc: 89.355% (4575/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 136\n",
            "train: [39/391] Loss: 0.086 | Acc: 96.973% (4965/5120)\n",
            "train: [79/391] Loss: 0.083 | Acc: 97.031% (9936/10240)\n",
            "train: [119/391] Loss: 0.083 | Acc: 97.064% (14909/15360)\n",
            "train: [159/391] Loss: 0.082 | Acc: 97.075% (19881/20480)\n",
            "train: [199/391] Loss: 0.082 | Acc: 97.078% (24852/25600)\n",
            "train: [239/391] Loss: 0.083 | Acc: 97.012% (29802/30720)\n",
            "train: [279/391] Loss: 0.084 | Acc: 97.009% (34768/35840)\n",
            "train: [319/391] Loss: 0.084 | Acc: 96.997% (39730/40960)\n",
            "train: [359/391] Loss: 0.082 | Acc: 97.042% (44717/46080)\n",
            "val: [39/79] Loss: 0.422 | Acc: 89.434% (4579/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 137\n",
            "train: [39/391] Loss: 0.070 | Acc: 97.441% (4989/5120)\n",
            "train: [79/391] Loss: 0.070 | Acc: 97.500% (9984/10240)\n",
            "train: [119/391] Loss: 0.068 | Acc: 97.637% (14997/15360)\n",
            "train: [159/391] Loss: 0.069 | Acc: 97.568% (19982/20480)\n",
            "train: [199/391] Loss: 0.068 | Acc: 97.570% (24978/25600)\n",
            "train: [239/391] Loss: 0.068 | Acc: 97.620% (29989/30720)\n",
            "train: [279/391] Loss: 0.067 | Acc: 97.667% (35004/35840)\n",
            "train: [319/391] Loss: 0.067 | Acc: 97.654% (39999/40960)\n",
            "train: [359/391] Loss: 0.066 | Acc: 97.700% (45020/46080)\n",
            "val: [39/79] Loss: 0.439 | Acc: 89.590% (4587/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 138\n",
            "train: [39/391] Loss: 0.061 | Acc: 97.852% (5010/5120)\n",
            "train: [79/391] Loss: 0.058 | Acc: 98.047% (10040/10240)\n",
            "train: [119/391] Loss: 0.058 | Acc: 97.988% (15051/15360)\n",
            "train: [159/391] Loss: 0.058 | Acc: 98.008% (20072/20480)\n",
            "train: [199/391] Loss: 0.057 | Acc: 98.027% (25095/25600)\n",
            "train: [239/391] Loss: 0.058 | Acc: 98.005% (30107/30720)\n",
            "train: [279/391] Loss: 0.058 | Acc: 97.985% (35118/35840)\n",
            "train: [319/391] Loss: 0.058 | Acc: 97.988% (40136/40960)\n",
            "train: [359/391] Loss: 0.058 | Acc: 97.960% (45140/46080)\n",
            "val: [39/79] Loss: 0.436 | Acc: 89.863% (4601/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 139\n",
            "train: [39/391] Loss: 0.061 | Acc: 97.988% (5017/5120)\n",
            "train: [79/391] Loss: 0.057 | Acc: 98.135% (10049/10240)\n",
            "train: [119/391] Loss: 0.056 | Acc: 98.125% (15072/15360)\n",
            "train: [159/391] Loss: 0.054 | Acc: 98.130% (20097/20480)\n",
            "train: [199/391] Loss: 0.053 | Acc: 98.172% (25132/25600)\n",
            "train: [239/391] Loss: 0.053 | Acc: 98.161% (30155/30720)\n",
            "train: [279/391] Loss: 0.052 | Acc: 98.189% (35191/35840)\n",
            "train: [319/391] Loss: 0.052 | Acc: 98.191% (40219/40960)\n",
            "train: [359/391] Loss: 0.051 | Acc: 98.234% (45266/46080)\n",
            "val: [39/79] Loss: 0.433 | Acc: 90.000% (4608/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 140\n",
            "train: [39/391] Loss: 0.081 | Acc: 97.188% (4976/5120)\n",
            "train: [79/391] Loss: 0.109 | Acc: 96.260% (9857/10240)\n",
            "train: [119/391] Loss: 0.122 | Acc: 95.723% (14703/15360)\n",
            "train: [159/391] Loss: 0.125 | Acc: 95.503% (19559/20480)\n",
            "train: [199/391] Loss: 0.132 | Acc: 95.238% (24381/25600)\n",
            "train: [239/391] Loss: 0.135 | Acc: 95.120% (29221/30720)\n",
            "train: [279/391] Loss: 0.138 | Acc: 95.020% (34055/35840)\n",
            "train: [319/391] Loss: 0.140 | Acc: 94.939% (38887/40960)\n",
            "train: [359/391] Loss: 0.141 | Acc: 94.891% (43726/46080)\n",
            "val: [39/79] Loss: 0.426 | Acc: 88.242% (4518/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 141\n",
            "train: [39/391] Loss: 0.138 | Acc: 95.117% (4870/5120)\n",
            "train: [79/391] Loss: 0.139 | Acc: 95.049% (9733/10240)\n",
            "train: [119/391] Loss: 0.138 | Acc: 94.974% (14588/15360)\n",
            "train: [159/391] Loss: 0.141 | Acc: 94.902% (19436/20480)\n",
            "train: [199/391] Loss: 0.142 | Acc: 94.891% (24292/25600)\n",
            "train: [239/391] Loss: 0.144 | Acc: 94.831% (29132/30720)\n",
            "train: [279/391] Loss: 0.142 | Acc: 94.863% (33999/35840)\n",
            "train: [319/391] Loss: 0.140 | Acc: 94.983% (38905/40960)\n",
            "train: [359/391] Loss: 0.140 | Acc: 94.983% (43768/46080)\n",
            "val: [39/79] Loss: 0.417 | Acc: 88.867% (4550/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 142\n",
            "train: [39/391] Loss: 0.127 | Acc: 95.508% (4890/5120)\n",
            "train: [79/391] Loss: 0.126 | Acc: 95.479% (9777/10240)\n",
            "train: [119/391] Loss: 0.127 | Acc: 95.443% (14660/15360)\n",
            "train: [159/391] Loss: 0.128 | Acc: 95.381% (19534/20480)\n",
            "train: [199/391] Loss: 0.128 | Acc: 95.441% (24433/25600)\n",
            "train: [239/391] Loss: 0.130 | Acc: 95.345% (29290/30720)\n",
            "train: [279/391] Loss: 0.129 | Acc: 95.343% (34171/35840)\n",
            "train: [319/391] Loss: 0.131 | Acc: 95.288% (39030/40960)\n",
            "train: [359/391] Loss: 0.130 | Acc: 95.284% (43907/46080)\n",
            "val: [39/79] Loss: 0.426 | Acc: 88.691% (4541/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 143\n",
            "train: [39/391] Loss: 0.123 | Acc: 95.410% (4885/5120)\n",
            "train: [79/391] Loss: 0.122 | Acc: 95.547% (9784/10240)\n",
            "train: [119/391] Loss: 0.124 | Acc: 95.573% (14680/15360)\n",
            "train: [159/391] Loss: 0.120 | Acc: 95.757% (19611/20480)\n",
            "train: [199/391] Loss: 0.121 | Acc: 95.715% (24503/25600)\n",
            "train: [239/391] Loss: 0.120 | Acc: 95.768% (29420/30720)\n",
            "train: [279/391] Loss: 0.121 | Acc: 95.664% (34286/35840)\n",
            "train: [319/391] Loss: 0.120 | Acc: 95.713% (39204/40960)\n",
            "train: [359/391] Loss: 0.121 | Acc: 95.653% (44077/46080)\n",
            "val: [39/79] Loss: 0.417 | Acc: 89.238% (4569/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 144\n",
            "train: [39/391] Loss: 0.106 | Acc: 96.270% (4929/5120)\n",
            "train: [79/391] Loss: 0.103 | Acc: 96.348% (9866/10240)\n",
            "train: [119/391] Loss: 0.102 | Acc: 96.380% (14804/15360)\n",
            "train: [159/391] Loss: 0.104 | Acc: 96.289% (19720/20480)\n",
            "train: [199/391] Loss: 0.105 | Acc: 96.262% (24643/25600)\n",
            "train: [239/391] Loss: 0.106 | Acc: 96.214% (29557/30720)\n",
            "train: [279/391] Loss: 0.105 | Acc: 96.236% (34491/35840)\n",
            "train: [319/391] Loss: 0.105 | Acc: 96.260% (39428/40960)\n",
            "train: [359/391] Loss: 0.105 | Acc: 96.243% (44349/46080)\n",
            "val: [39/79] Loss: 0.419 | Acc: 89.258% (4570/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 145\n",
            "train: [39/391] Loss: 0.092 | Acc: 96.934% (4963/5120)\n",
            "train: [79/391] Loss: 0.086 | Acc: 97.070% (9940/10240)\n",
            "train: [119/391] Loss: 0.089 | Acc: 96.940% (14890/15360)\n",
            "train: [159/391] Loss: 0.091 | Acc: 96.865% (19838/20480)\n",
            "train: [199/391] Loss: 0.090 | Acc: 96.902% (24807/25600)\n",
            "train: [239/391] Loss: 0.089 | Acc: 96.875% (29760/30720)\n",
            "train: [279/391] Loss: 0.089 | Acc: 96.883% (34723/35840)\n",
            "train: [319/391] Loss: 0.088 | Acc: 96.899% (39690/40960)\n",
            "train: [359/391] Loss: 0.090 | Acc: 96.845% (44626/46080)\n",
            "val: [39/79] Loss: 0.419 | Acc: 89.570% (4586/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 146\n",
            "train: [39/391] Loss: 0.065 | Acc: 97.676% (5001/5120)\n",
            "train: [79/391] Loss: 0.066 | Acc: 97.607% (9995/10240)\n",
            "train: [119/391] Loss: 0.068 | Acc: 97.663% (15001/15360)\n",
            "train: [159/391] Loss: 0.073 | Acc: 97.427% (19953/20480)\n",
            "train: [199/391] Loss: 0.074 | Acc: 97.328% (24916/25600)\n",
            "train: [239/391] Loss: 0.075 | Acc: 97.295% (29889/30720)\n",
            "train: [279/391] Loss: 0.075 | Acc: 97.305% (34874/35840)\n",
            "train: [319/391] Loss: 0.075 | Acc: 97.292% (39851/40960)\n",
            "train: [359/391] Loss: 0.075 | Acc: 97.294% (44833/46080)\n",
            "val: [39/79] Loss: 0.431 | Acc: 89.590% (4587/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 147\n",
            "train: [39/391] Loss: 0.065 | Acc: 97.734% (5004/5120)\n",
            "train: [79/391] Loss: 0.062 | Acc: 97.939% (10029/10240)\n",
            "train: [119/391] Loss: 0.059 | Acc: 98.027% (15057/15360)\n",
            "train: [159/391] Loss: 0.060 | Acc: 97.964% (20063/20480)\n",
            "train: [199/391] Loss: 0.060 | Acc: 97.953% (25076/25600)\n",
            "train: [239/391] Loss: 0.059 | Acc: 97.936% (30086/30720)\n",
            "train: [279/391] Loss: 0.059 | Acc: 97.930% (35098/35840)\n",
            "train: [319/391] Loss: 0.060 | Acc: 97.925% (40110/40960)\n",
            "train: [359/391] Loss: 0.059 | Acc: 97.941% (45131/46080)\n",
            "val: [39/79] Loss: 0.436 | Acc: 89.922% (4604/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 148\n",
            "train: [39/391] Loss: 0.054 | Acc: 98.223% (5029/5120)\n",
            "train: [79/391] Loss: 0.054 | Acc: 98.115% (10047/10240)\n",
            "train: [119/391] Loss: 0.051 | Acc: 98.184% (15081/15360)\n",
            "train: [159/391] Loss: 0.054 | Acc: 98.086% (20088/20480)\n",
            "train: [199/391] Loss: 0.055 | Acc: 98.070% (25106/25600)\n",
            "train: [239/391] Loss: 0.055 | Acc: 98.089% (30133/30720)\n",
            "train: [279/391] Loss: 0.055 | Acc: 98.119% (35166/35840)\n",
            "train: [319/391] Loss: 0.054 | Acc: 98.135% (40196/40960)\n",
            "train: [359/391] Loss: 0.054 | Acc: 98.149% (45227/46080)\n",
            "val: [39/79] Loss: 0.449 | Acc: 89.824% (4599/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 149\n",
            "train: [39/391] Loss: 0.045 | Acc: 98.359% (5036/5120)\n",
            "train: [79/391] Loss: 0.050 | Acc: 98.252% (10061/10240)\n",
            "train: [119/391] Loss: 0.050 | Acc: 98.216% (15086/15360)\n",
            "train: [159/391] Loss: 0.048 | Acc: 98.291% (20130/20480)\n",
            "train: [199/391] Loss: 0.048 | Acc: 98.324% (25171/25600)\n",
            "train: [239/391] Loss: 0.048 | Acc: 98.324% (30205/30720)\n",
            "train: [279/391] Loss: 0.048 | Acc: 98.320% (35238/35840)\n",
            "train: [319/391] Loss: 0.048 | Acc: 98.330% (40276/40960)\n",
            "train: [359/391] Loss: 0.048 | Acc: 98.296% (45295/46080)\n",
            "val: [39/79] Loss: 0.446 | Acc: 89.941% (4605/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 150\n",
            "train: [39/391] Loss: 0.075 | Acc: 97.383% (4986/5120)\n",
            "train: [79/391] Loss: 0.098 | Acc: 96.611% (9893/10240)\n",
            "train: [119/391] Loss: 0.107 | Acc: 96.120% (14764/15360)\n",
            "train: [159/391] Loss: 0.114 | Acc: 95.874% (19635/20480)\n",
            "train: [199/391] Loss: 0.120 | Acc: 95.625% (24480/25600)\n",
            "train: [239/391] Loss: 0.122 | Acc: 95.547% (29352/30720)\n",
            "train: [279/391] Loss: 0.126 | Acc: 95.513% (34232/35840)\n",
            "train: [319/391] Loss: 0.129 | Acc: 95.464% (39102/40960)\n",
            "train: [359/391] Loss: 0.131 | Acc: 95.399% (43960/46080)\n",
            "val: [39/79] Loss: 0.444 | Acc: 87.754% (4493/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 151\n",
            "train: [39/391] Loss: 0.124 | Acc: 95.527% (4891/5120)\n",
            "train: [79/391] Loss: 0.120 | Acc: 95.713% (9801/10240)\n",
            "train: [119/391] Loss: 0.124 | Acc: 95.469% (14664/15360)\n",
            "train: [159/391] Loss: 0.127 | Acc: 95.376% (19533/20480)\n",
            "train: [199/391] Loss: 0.127 | Acc: 95.418% (24427/25600)\n",
            "train: [239/391] Loss: 0.130 | Acc: 95.365% (29296/30720)\n",
            "train: [279/391] Loss: 0.131 | Acc: 95.338% (34169/35840)\n",
            "train: [319/391] Loss: 0.132 | Acc: 95.327% (39046/40960)\n",
            "train: [359/391] Loss: 0.133 | Acc: 95.258% (43895/46080)\n",
            "val: [39/79] Loss: 0.433 | Acc: 88.262% (4519/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 152\n",
            "train: [39/391] Loss: 0.121 | Acc: 95.762% (4903/5120)\n",
            "train: [79/391] Loss: 0.118 | Acc: 95.850% (9815/10240)\n",
            "train: [119/391] Loss: 0.120 | Acc: 95.749% (14707/15360)\n",
            "train: [159/391] Loss: 0.120 | Acc: 95.781% (19616/20480)\n",
            "train: [199/391] Loss: 0.123 | Acc: 95.613% (24477/25600)\n",
            "train: [239/391] Loss: 0.123 | Acc: 95.605% (29370/30720)\n",
            "train: [279/391] Loss: 0.122 | Acc: 95.611% (34267/35840)\n",
            "train: [319/391] Loss: 0.123 | Acc: 95.642% (39175/40960)\n",
            "train: [359/391] Loss: 0.124 | Acc: 95.612% (44058/46080)\n",
            "val: [39/79] Loss: 0.441 | Acc: 88.457% (4529/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 153\n",
            "train: [39/391] Loss: 0.098 | Acc: 96.738% (4953/5120)\n",
            "train: [79/391] Loss: 0.106 | Acc: 96.270% (9858/10240)\n",
            "train: [119/391] Loss: 0.113 | Acc: 95.964% (14740/15360)\n",
            "train: [159/391] Loss: 0.111 | Acc: 96.045% (19670/20480)\n",
            "train: [199/391] Loss: 0.113 | Acc: 96.004% (24577/25600)\n",
            "train: [239/391] Loss: 0.111 | Acc: 96.107% (29524/30720)\n",
            "train: [279/391] Loss: 0.112 | Acc: 96.071% (34432/35840)\n",
            "train: [319/391] Loss: 0.113 | Acc: 96.042% (39339/40960)\n",
            "train: [359/391] Loss: 0.113 | Acc: 96.029% (44250/46080)\n",
            "val: [39/79] Loss: 0.453 | Acc: 88.418% (4527/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 154\n",
            "train: [39/391] Loss: 0.097 | Acc: 96.582% (4945/5120)\n",
            "train: [79/391] Loss: 0.092 | Acc: 96.670% (9899/10240)\n",
            "train: [119/391] Loss: 0.095 | Acc: 96.641% (14844/15360)\n",
            "train: [159/391] Loss: 0.094 | Acc: 96.670% (19798/20480)\n",
            "train: [199/391] Loss: 0.093 | Acc: 96.684% (24751/25600)\n",
            "train: [239/391] Loss: 0.092 | Acc: 96.686% (29702/30720)\n",
            "train: [279/391] Loss: 0.092 | Acc: 96.696% (34656/35840)\n",
            "train: [319/391] Loss: 0.094 | Acc: 96.650% (39588/40960)\n",
            "train: [359/391] Loss: 0.094 | Acc: 96.636% (44530/46080)\n",
            "val: [39/79] Loss: 0.447 | Acc: 88.848% (4549/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 155\n",
            "train: [39/391] Loss: 0.089 | Acc: 97.051% (4969/5120)\n",
            "train: [79/391] Loss: 0.082 | Acc: 97.266% (9960/10240)\n",
            "train: [119/391] Loss: 0.080 | Acc: 97.272% (14941/15360)\n",
            "train: [159/391] Loss: 0.081 | Acc: 97.148% (19896/20480)\n",
            "train: [199/391] Loss: 0.080 | Acc: 97.195% (24882/25600)\n",
            "train: [239/391] Loss: 0.080 | Acc: 97.204% (29861/30720)\n",
            "train: [279/391] Loss: 0.079 | Acc: 97.196% (34835/35840)\n",
            "train: [319/391] Loss: 0.080 | Acc: 97.163% (39798/40960)\n",
            "train: [359/391] Loss: 0.079 | Acc: 97.168% (44775/46080)\n",
            "val: [39/79] Loss: 0.440 | Acc: 88.848% (4549/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 156\n",
            "train: [39/391] Loss: 0.077 | Acc: 97.402% (4987/5120)\n",
            "train: [79/391] Loss: 0.072 | Acc: 97.529% (9987/10240)\n",
            "train: [119/391] Loss: 0.072 | Acc: 97.480% (14973/15360)\n",
            "train: [159/391] Loss: 0.073 | Acc: 97.529% (19974/20480)\n",
            "train: [199/391] Loss: 0.073 | Acc: 97.531% (24968/25600)\n",
            "train: [239/391] Loss: 0.073 | Acc: 97.516% (29957/30720)\n",
            "train: [279/391] Loss: 0.072 | Acc: 97.539% (34958/35840)\n",
            "train: [319/391] Loss: 0.071 | Acc: 97.593% (39974/40960)\n",
            "train: [359/391] Loss: 0.070 | Acc: 97.628% (44987/46080)\n",
            "val: [39/79] Loss: 0.453 | Acc: 89.121% (4563/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 157\n",
            "train: [39/391] Loss: 0.070 | Acc: 97.500% (4992/5120)\n",
            "train: [79/391] Loss: 0.064 | Acc: 97.744% (10009/10240)\n",
            "train: [119/391] Loss: 0.060 | Acc: 97.897% (15037/15360)\n",
            "train: [159/391] Loss: 0.060 | Acc: 97.939% (20058/20480)\n",
            "train: [199/391] Loss: 0.057 | Acc: 98.062% (25104/25600)\n",
            "train: [239/391] Loss: 0.058 | Acc: 98.027% (30114/30720)\n",
            "train: [279/391] Loss: 0.058 | Acc: 98.033% (35135/35840)\n",
            "train: [319/391] Loss: 0.057 | Acc: 98.047% (40160/40960)\n",
            "train: [359/391] Loss: 0.057 | Acc: 98.077% (45194/46080)\n",
            "val: [39/79] Loss: 0.461 | Acc: 89.570% (4586/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 158\n",
            "train: [39/391] Loss: 0.053 | Acc: 98.242% (5030/5120)\n",
            "train: [79/391] Loss: 0.047 | Acc: 98.350% (10071/10240)\n",
            "train: [119/391] Loss: 0.050 | Acc: 98.242% (15090/15360)\n",
            "train: [159/391] Loss: 0.048 | Acc: 98.301% (20132/20480)\n",
            "train: [199/391] Loss: 0.047 | Acc: 98.309% (25167/25600)\n",
            "train: [239/391] Loss: 0.048 | Acc: 98.337% (30209/30720)\n",
            "train: [279/391] Loss: 0.049 | Acc: 98.298% (35230/35840)\n",
            "train: [319/391] Loss: 0.049 | Acc: 98.276% (40254/40960)\n",
            "train: [359/391] Loss: 0.047 | Acc: 98.325% (45308/46080)\n",
            "val: [39/79] Loss: 0.467 | Acc: 89.453% (4580/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 159\n",
            "train: [39/391] Loss: 0.047 | Acc: 98.301% (5033/5120)\n",
            "train: [79/391] Loss: 0.049 | Acc: 98.311% (10067/10240)\n",
            "train: [119/391] Loss: 0.048 | Acc: 98.314% (15101/15360)\n",
            "train: [159/391] Loss: 0.047 | Acc: 98.350% (20142/20480)\n",
            "train: [199/391] Loss: 0.046 | Acc: 98.371% (25183/25600)\n",
            "train: [239/391] Loss: 0.046 | Acc: 98.376% (30221/30720)\n",
            "train: [279/391] Loss: 0.045 | Acc: 98.415% (35272/35840)\n",
            "train: [319/391] Loss: 0.045 | Acc: 98.423% (40314/40960)\n",
            "train: [359/391] Loss: 0.045 | Acc: 98.433% (45358/46080)\n",
            "val: [39/79] Loss: 0.463 | Acc: 89.414% (4578/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 160\n",
            "train: [39/391] Loss: 0.082 | Acc: 97.051% (4969/5120)\n",
            "train: [79/391] Loss: 0.098 | Acc: 96.768% (9909/10240)\n",
            "train: [119/391] Loss: 0.106 | Acc: 96.302% (14792/15360)\n",
            "train: [159/391] Loss: 0.110 | Acc: 96.172% (19696/20480)\n",
            "train: [199/391] Loss: 0.113 | Acc: 96.043% (24587/25600)\n",
            "train: [239/391] Loss: 0.117 | Acc: 95.905% (29462/30720)\n",
            "train: [279/391] Loss: 0.117 | Acc: 95.904% (34372/35840)\n",
            "train: [319/391] Loss: 0.119 | Acc: 95.803% (39241/40960)\n",
            "train: [359/391] Loss: 0.121 | Acc: 95.723% (44109/46080)\n",
            "val: [39/79] Loss: 0.459 | Acc: 88.223% (4517/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 161\n",
            "train: [39/391] Loss: 0.133 | Acc: 95.645% (4897/5120)\n",
            "train: [79/391] Loss: 0.125 | Acc: 95.781% (9808/10240)\n",
            "train: [119/391] Loss: 0.124 | Acc: 95.833% (14720/15360)\n",
            "train: [159/391] Loss: 0.123 | Acc: 95.811% (19622/20480)\n",
            "train: [199/391] Loss: 0.125 | Acc: 95.766% (24516/25600)\n",
            "train: [239/391] Loss: 0.124 | Acc: 95.726% (29407/30720)\n",
            "train: [279/391] Loss: 0.123 | Acc: 95.753% (34318/35840)\n",
            "train: [319/391] Loss: 0.125 | Acc: 95.693% (39196/40960)\n",
            "train: [359/391] Loss: 0.126 | Acc: 95.623% (44063/46080)\n",
            "val: [39/79] Loss: 0.447 | Acc: 88.535% (4533/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 162\n",
            "train: [39/391] Loss: 0.110 | Acc: 95.918% (4911/5120)\n",
            "train: [79/391] Loss: 0.108 | Acc: 96.104% (9841/10240)\n",
            "train: [119/391] Loss: 0.105 | Acc: 96.230% (14781/15360)\n",
            "train: [159/391] Loss: 0.106 | Acc: 96.206% (19703/20480)\n",
            "train: [199/391] Loss: 0.129 | Acc: 95.621% (24479/25600)\n",
            "train: [239/391] Loss: 0.172 | Acc: 94.336% (28980/30720)\n",
            "train: [279/391] Loss: 0.184 | Acc: 93.820% (33625/35840)\n",
            "train: [319/391] Loss: 0.187 | Acc: 93.655% (38361/40960)\n",
            "train: [359/391] Loss: 0.185 | Acc: 93.694% (43174/46080)\n",
            "val: [39/79] Loss: 0.525 | Acc: 85.586% (4382/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 163\n",
            "train: [39/391] Loss: 0.197 | Acc: 93.223% (4773/5120)\n",
            "train: [79/391] Loss: 0.177 | Acc: 93.906% (9616/10240)\n",
            "train: [119/391] Loss: 0.165 | Acc: 94.219% (14472/15360)\n",
            "train: [159/391] Loss: 0.159 | Acc: 94.521% (19358/20480)\n",
            "train: [199/391] Loss: 0.164 | Acc: 94.312% (24144/25600)\n",
            "train: [239/391] Loss: 0.163 | Acc: 94.342% (28982/30720)\n",
            "train: [279/391] Loss: 0.163 | Acc: 94.355% (33817/35840)\n",
            "train: [319/391] Loss: 0.161 | Acc: 94.426% (38677/40960)\n",
            "train: [359/391] Loss: 0.159 | Acc: 94.518% (43554/46080)\n",
            "val: [39/79] Loss: 0.664 | Acc: 81.230% (4159/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 164\n",
            "train: [39/391] Loss: 0.356 | Acc: 88.574% (4535/5120)\n",
            "train: [79/391] Loss: 0.298 | Acc: 90.371% (9254/10240)\n",
            "train: [119/391] Loss: 0.262 | Acc: 91.322% (14027/15360)\n",
            "train: [159/391] Loss: 0.238 | Acc: 92.026% (18847/20480)\n",
            "train: [199/391] Loss: 0.223 | Acc: 92.520% (23685/25600)\n",
            "train: [239/391] Loss: 0.210 | Acc: 92.894% (28537/30720)\n",
            "train: [279/391] Loss: 0.200 | Acc: 93.128% (33377/35840)\n",
            "train: [319/391] Loss: 0.190 | Acc: 93.438% (38272/40960)\n",
            "train: [359/391] Loss: 0.182 | Acc: 93.683% (43169/46080)\n",
            "val: [39/79] Loss: 0.416 | Acc: 88.867% (4550/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 165\n",
            "train: [39/391] Loss: 0.108 | Acc: 96.172% (4924/5120)\n",
            "train: [79/391] Loss: 0.106 | Acc: 96.133% (9844/10240)\n",
            "train: [119/391] Loss: 0.104 | Acc: 96.224% (14780/15360)\n",
            "train: [159/391] Loss: 0.103 | Acc: 96.265% (19715/20480)\n",
            "train: [199/391] Loss: 0.104 | Acc: 96.254% (24641/25600)\n",
            "train: [239/391] Loss: 0.105 | Acc: 96.227% (29561/30720)\n",
            "train: [279/391] Loss: 0.104 | Acc: 96.278% (34506/35840)\n",
            "train: [319/391] Loss: 0.104 | Acc: 96.306% (39447/40960)\n",
            "train: [359/391] Loss: 0.103 | Acc: 96.359% (44402/46080)\n",
            "val: [39/79] Loss: 0.422 | Acc: 89.277% (4571/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 166\n",
            "train: [39/391] Loss: 0.093 | Acc: 97.031% (4968/5120)\n",
            "train: [79/391] Loss: 0.086 | Acc: 97.061% (9939/10240)\n",
            "train: [119/391] Loss: 0.085 | Acc: 96.966% (14894/15360)\n",
            "train: [159/391] Loss: 0.084 | Acc: 97.007% (19867/20480)\n",
            "train: [199/391] Loss: 0.084 | Acc: 97.016% (24836/25600)\n",
            "train: [239/391] Loss: 0.083 | Acc: 97.080% (29823/30720)\n",
            "train: [279/391] Loss: 0.083 | Acc: 97.062% (34787/35840)\n",
            "train: [319/391] Loss: 0.082 | Acc: 97.085% (39766/40960)\n",
            "train: [359/391] Loss: 0.082 | Acc: 97.099% (44743/46080)\n",
            "val: [39/79] Loss: 0.431 | Acc: 89.473% (4581/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 167\n",
            "train: [39/391] Loss: 0.073 | Acc: 97.402% (4987/5120)\n",
            "train: [79/391] Loss: 0.074 | Acc: 97.510% (9985/10240)\n",
            "train: [119/391] Loss: 0.075 | Acc: 97.402% (14961/15360)\n",
            "train: [159/391] Loss: 0.071 | Acc: 97.524% (19973/20480)\n",
            "train: [199/391] Loss: 0.070 | Acc: 97.547% (24972/25600)\n",
            "train: [239/391] Loss: 0.070 | Acc: 97.604% (29984/30720)\n",
            "train: [279/391] Loss: 0.070 | Acc: 97.614% (34985/35840)\n",
            "train: [319/391] Loss: 0.071 | Acc: 97.600% (39977/40960)\n",
            "train: [359/391] Loss: 0.070 | Acc: 97.622% (44984/46080)\n",
            "val: [39/79] Loss: 0.452 | Acc: 89.512% (4583/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 168\n",
            "train: [39/391] Loss: 0.070 | Acc: 97.559% (4995/5120)\n",
            "train: [79/391] Loss: 0.065 | Acc: 97.812% (10016/10240)\n",
            "train: [119/391] Loss: 0.063 | Acc: 97.760% (15016/15360)\n",
            "train: [159/391] Loss: 0.064 | Acc: 97.754% (20020/20480)\n",
            "train: [199/391] Loss: 0.065 | Acc: 97.719% (25016/25600)\n",
            "train: [239/391] Loss: 0.065 | Acc: 97.760% (30032/30720)\n",
            "train: [279/391] Loss: 0.065 | Acc: 97.751% (35034/35840)\n",
            "train: [319/391] Loss: 0.065 | Acc: 97.764% (40044/40960)\n",
            "train: [359/391] Loss: 0.064 | Acc: 97.782% (45058/46080)\n",
            "val: [39/79] Loss: 0.440 | Acc: 89.805% (4598/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 169\n",
            "train: [39/391] Loss: 0.052 | Acc: 98.262% (5031/5120)\n",
            "train: [79/391] Loss: 0.053 | Acc: 98.242% (10060/10240)\n",
            "train: [119/391] Loss: 0.054 | Acc: 98.132% (15073/15360)\n",
            "train: [159/391] Loss: 0.055 | Acc: 98.071% (20085/20480)\n",
            "train: [199/391] Loss: 0.057 | Acc: 98.031% (25096/25600)\n",
            "train: [239/391] Loss: 0.057 | Acc: 98.018% (30111/30720)\n",
            "train: [279/391] Loss: 0.058 | Acc: 98.002% (35124/35840)\n",
            "train: [319/391] Loss: 0.058 | Acc: 98.003% (40142/40960)\n",
            "train: [359/391] Loss: 0.057 | Acc: 98.036% (45175/46080)\n",
            "val: [39/79] Loss: 0.445 | Acc: 89.688% (4592/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 170\n",
            "train: [39/391] Loss: 0.067 | Acc: 97.754% (5005/5120)\n",
            "train: [79/391] Loss: 0.088 | Acc: 96.992% (9932/10240)\n",
            "train: [119/391] Loss: 0.097 | Acc: 96.712% (14855/15360)\n",
            "train: [159/391] Loss: 0.102 | Acc: 96.504% (19764/20480)\n",
            "train: [199/391] Loss: 0.105 | Acc: 96.367% (24670/25600)\n",
            "train: [239/391] Loss: 0.107 | Acc: 96.344% (29597/30720)\n",
            "train: [279/391] Loss: 0.110 | Acc: 96.306% (34516/35840)\n",
            "train: [319/391] Loss: 0.110 | Acc: 96.282% (39437/40960)\n",
            "train: [359/391] Loss: 0.114 | Acc: 96.100% (44283/46080)\n",
            "val: [39/79] Loss: 0.433 | Acc: 88.965% (4555/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 171\n",
            "train: [39/391] Loss: 0.113 | Acc: 95.957% (4913/5120)\n",
            "train: [79/391] Loss: 0.116 | Acc: 95.859% (9816/10240)\n",
            "train: [119/391] Loss: 0.114 | Acc: 96.029% (14750/15360)\n",
            "train: [159/391] Loss: 0.113 | Acc: 96.030% (19667/20480)\n",
            "train: [199/391] Loss: 0.114 | Acc: 96.043% (24587/25600)\n",
            "train: [239/391] Loss: 0.115 | Acc: 96.019% (29497/30720)\n",
            "train: [279/391] Loss: 0.115 | Acc: 96.027% (34416/35840)\n",
            "train: [319/391] Loss: 0.115 | Acc: 96.021% (39330/40960)\n",
            "train: [359/391] Loss: 0.117 | Acc: 95.938% (44208/46080)\n",
            "val: [39/79] Loss: 0.447 | Acc: 88.730% (4543/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 172\n",
            "train: [39/391] Loss: 0.098 | Acc: 96.602% (4946/5120)\n",
            "train: [79/391] Loss: 0.097 | Acc: 96.709% (9903/10240)\n",
            "train: [119/391] Loss: 0.098 | Acc: 96.595% (14837/15360)\n",
            "train: [159/391] Loss: 0.100 | Acc: 96.548% (19773/20480)\n",
            "train: [199/391] Loss: 0.100 | Acc: 96.531% (24712/25600)\n",
            "train: [239/391] Loss: 0.102 | Acc: 96.452% (29630/30720)\n",
            "train: [279/391] Loss: 0.103 | Acc: 96.415% (34555/35840)\n",
            "train: [319/391] Loss: 0.104 | Acc: 96.389% (39481/40960)\n",
            "train: [359/391] Loss: 0.106 | Acc: 96.304% (44377/46080)\n",
            "val: [39/79] Loss: 0.440 | Acc: 88.887% (4551/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 173\n",
            "train: [39/391] Loss: 0.095 | Acc: 96.445% (4938/5120)\n",
            "train: [79/391] Loss: 0.095 | Acc: 96.572% (9889/10240)\n",
            "train: [119/391] Loss: 0.093 | Acc: 96.667% (14848/15360)\n",
            "train: [159/391] Loss: 0.091 | Acc: 96.680% (19800/20480)\n",
            "train: [199/391] Loss: 0.092 | Acc: 96.672% (24748/25600)\n",
            "train: [239/391] Loss: 0.091 | Acc: 96.689% (29703/30720)\n",
            "train: [279/391] Loss: 0.092 | Acc: 96.627% (34631/35840)\n",
            "train: [319/391] Loss: 0.094 | Acc: 96.606% (39570/40960)\n",
            "train: [359/391] Loss: 0.094 | Acc: 96.621% (44523/46080)\n",
            "val: [39/79] Loss: 0.475 | Acc: 88.887% (4551/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 174\n",
            "train: [39/391] Loss: 0.085 | Acc: 96.875% (4960/5120)\n",
            "train: [79/391] Loss: 0.088 | Acc: 96.924% (9925/10240)\n",
            "train: [119/391] Loss: 0.085 | Acc: 96.992% (14898/15360)\n",
            "train: [159/391] Loss: 0.086 | Acc: 97.041% (19874/20480)\n",
            "train: [199/391] Loss: 0.085 | Acc: 97.031% (24840/25600)\n",
            "train: [239/391] Loss: 0.084 | Acc: 97.038% (29810/30720)\n",
            "train: [279/391] Loss: 0.085 | Acc: 96.995% (34763/35840)\n",
            "train: [319/391] Loss: 0.087 | Acc: 96.953% (39712/40960)\n",
            "train: [359/391] Loss: 0.087 | Acc: 96.999% (44697/46080)\n",
            "val: [39/79] Loss: 0.463 | Acc: 88.691% (4541/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 175\n",
            "train: [39/391] Loss: 0.074 | Acc: 97.617% (4998/5120)\n",
            "train: [79/391] Loss: 0.069 | Acc: 97.666% (10001/10240)\n",
            "train: [119/391] Loss: 0.069 | Acc: 97.617% (14994/15360)\n",
            "train: [159/391] Loss: 0.069 | Acc: 97.603% (19989/20480)\n",
            "train: [199/391] Loss: 0.072 | Acc: 97.496% (24959/25600)\n",
            "train: [239/391] Loss: 0.072 | Acc: 97.428% (29930/30720)\n",
            "train: [279/391] Loss: 0.072 | Acc: 97.427% (34918/35840)\n",
            "train: [319/391] Loss: 0.074 | Acc: 97.388% (39890/40960)\n",
            "train: [359/391] Loss: 0.075 | Acc: 97.370% (44868/46080)\n",
            "val: [39/79] Loss: 0.445 | Acc: 89.199% (4567/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 176\n",
            "train: [39/391] Loss: 0.066 | Acc: 97.793% (5007/5120)\n",
            "train: [79/391] Loss: 0.065 | Acc: 97.822% (10017/10240)\n",
            "train: [119/391] Loss: 0.063 | Acc: 97.839% (15028/15360)\n",
            "train: [159/391] Loss: 0.062 | Acc: 97.866% (20043/20480)\n",
            "train: [199/391] Loss: 0.062 | Acc: 97.816% (25041/25600)\n",
            "train: [239/391] Loss: 0.063 | Acc: 97.812% (30048/30720)\n",
            "train: [279/391] Loss: 0.062 | Acc: 97.840% (35066/35840)\n",
            "train: [319/391] Loss: 0.062 | Acc: 97.839% (40075/40960)\n",
            "train: [359/391] Loss: 0.062 | Acc: 97.849% (45089/46080)\n",
            "val: [39/79] Loss: 0.440 | Acc: 89.727% (4594/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 177\n",
            "train: [39/391] Loss: 0.052 | Acc: 98.008% (5018/5120)\n",
            "train: [79/391] Loss: 0.054 | Acc: 97.988% (10034/10240)\n",
            "train: [119/391] Loss: 0.054 | Acc: 98.099% (15068/15360)\n",
            "train: [159/391] Loss: 0.052 | Acc: 98.179% (20107/20480)\n",
            "train: [199/391] Loss: 0.052 | Acc: 98.223% (25145/25600)\n",
            "train: [239/391] Loss: 0.052 | Acc: 98.219% (30173/30720)\n",
            "train: [279/391] Loss: 0.052 | Acc: 98.211% (35199/35840)\n",
            "train: [319/391] Loss: 0.051 | Acc: 98.242% (40240/40960)\n",
            "train: [359/391] Loss: 0.051 | Acc: 98.270% (45283/46080)\n",
            "val: [39/79] Loss: 0.452 | Acc: 89.883% (4602/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 178\n",
            "train: [39/391] Loss: 0.044 | Acc: 98.496% (5043/5120)\n",
            "train: [79/391] Loss: 0.044 | Acc: 98.477% (10084/10240)\n",
            "train: [119/391] Loss: 0.043 | Acc: 98.574% (15141/15360)\n",
            "train: [159/391] Loss: 0.042 | Acc: 98.608% (20195/20480)\n",
            "train: [199/391] Loss: 0.042 | Acc: 98.566% (25233/25600)\n",
            "train: [239/391] Loss: 0.042 | Acc: 98.545% (30273/30720)\n",
            "train: [279/391] Loss: 0.042 | Acc: 98.538% (35316/35840)\n",
            "train: [319/391] Loss: 0.042 | Acc: 98.525% (40356/40960)\n",
            "train: [359/391] Loss: 0.042 | Acc: 98.531% (45403/46080)\n",
            "val: [39/79] Loss: 0.459 | Acc: 89.883% (4602/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 179\n",
            "train: [39/391] Loss: 0.046 | Acc: 98.340% (5035/5120)\n",
            "train: [79/391] Loss: 0.046 | Acc: 98.320% (10068/10240)\n",
            "train: [119/391] Loss: 0.044 | Acc: 98.444% (15121/15360)\n",
            "train: [159/391] Loss: 0.045 | Acc: 98.467% (20166/20480)\n",
            "train: [199/391] Loss: 0.044 | Acc: 98.516% (25220/25600)\n",
            "train: [239/391] Loss: 0.044 | Acc: 98.512% (30263/30720)\n",
            "train: [279/391] Loss: 0.045 | Acc: 98.468% (35291/35840)\n",
            "train: [319/391] Loss: 0.044 | Acc: 98.481% (40338/40960)\n",
            "train: [359/391] Loss: 0.044 | Acc: 98.477% (45378/46080)\n",
            "val: [39/79] Loss: 0.450 | Acc: 90.332% (4625/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 180\n",
            "train: [39/391] Loss: 0.060 | Acc: 98.066% (5021/5120)\n",
            "train: [79/391] Loss: 0.084 | Acc: 97.031% (9936/10240)\n",
            "train: [119/391] Loss: 0.103 | Acc: 96.400% (14807/15360)\n",
            "train: [159/391] Loss: 0.105 | Acc: 96.274% (19717/20480)\n",
            "train: [199/391] Loss: 0.111 | Acc: 96.086% (24598/25600)\n",
            "train: [239/391] Loss: 0.111 | Acc: 96.090% (29519/30720)\n",
            "train: [279/391] Loss: 0.112 | Acc: 96.077% (34434/35840)\n",
            "train: [319/391] Loss: 0.113 | Acc: 96.050% (39342/40960)\n",
            "train: [359/391] Loss: 0.114 | Acc: 96.044% (44257/46080)\n",
            "val: [39/79] Loss: 0.438 | Acc: 88.672% (4540/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 181\n",
            "train: [39/391] Loss: 0.102 | Acc: 96.016% (4916/5120)\n",
            "train: [79/391] Loss: 0.109 | Acc: 96.006% (9831/10240)\n",
            "train: [119/391] Loss: 0.112 | Acc: 95.918% (14733/15360)\n",
            "train: [159/391] Loss: 0.113 | Acc: 95.830% (19626/20480)\n",
            "train: [199/391] Loss: 0.114 | Acc: 95.836% (24534/25600)\n",
            "train: [239/391] Loss: 0.112 | Acc: 95.879% (29454/30720)\n",
            "train: [279/391] Loss: 0.114 | Acc: 95.778% (34327/35840)\n",
            "train: [319/391] Loss: 0.114 | Acc: 95.796% (39238/40960)\n",
            "train: [359/391] Loss: 0.114 | Acc: 95.799% (44144/46080)\n",
            "val: [39/79] Loss: 0.440 | Acc: 88.457% (4529/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 182\n",
            "train: [39/391] Loss: 0.105 | Acc: 96.309% (4931/5120)\n",
            "train: [79/391] Loss: 0.100 | Acc: 96.523% (9884/10240)\n",
            "train: [119/391] Loss: 0.101 | Acc: 96.497% (14822/15360)\n",
            "train: [159/391] Loss: 0.099 | Acc: 96.479% (19759/20480)\n",
            "train: [199/391] Loss: 0.102 | Acc: 96.406% (24680/25600)\n",
            "train: [239/391] Loss: 0.102 | Acc: 96.361% (29602/30720)\n",
            "train: [279/391] Loss: 0.102 | Acc: 96.384% (34544/35840)\n",
            "train: [319/391] Loss: 0.102 | Acc: 96.343% (39462/40960)\n",
            "train: [359/391] Loss: 0.103 | Acc: 96.367% (44406/46080)\n",
            "val: [39/79] Loss: 0.455 | Acc: 89.141% (4564/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 183\n",
            "train: [39/391] Loss: 0.095 | Acc: 96.699% (4951/5120)\n",
            "train: [79/391] Loss: 0.100 | Acc: 96.572% (9889/10240)\n",
            "train: [119/391] Loss: 0.100 | Acc: 96.504% (14823/15360)\n",
            "train: [159/391] Loss: 0.098 | Acc: 96.538% (19771/20480)\n",
            "train: [199/391] Loss: 0.097 | Acc: 96.555% (24718/25600)\n",
            "train: [239/391] Loss: 0.097 | Acc: 96.536% (29656/30720)\n",
            "train: [279/391] Loss: 0.097 | Acc: 96.537% (34599/35840)\n",
            "train: [319/391] Loss: 0.096 | Acc: 96.570% (39555/40960)\n",
            "train: [359/391] Loss: 0.096 | Acc: 96.584% (44506/46080)\n",
            "val: [39/79] Loss: 0.469 | Acc: 88.770% (4545/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 184\n",
            "train: [39/391] Loss: 0.084 | Acc: 97.070% (4970/5120)\n",
            "train: [79/391] Loss: 0.079 | Acc: 97.256% (9959/10240)\n",
            "train: [119/391] Loss: 0.080 | Acc: 97.214% (14932/15360)\n",
            "train: [159/391] Loss: 0.079 | Acc: 97.222% (19911/20480)\n",
            "train: [199/391] Loss: 0.079 | Acc: 97.254% (24897/25600)\n",
            "train: [239/391] Loss: 0.078 | Acc: 97.269% (29881/30720)\n",
            "train: [279/391] Loss: 0.080 | Acc: 97.229% (34847/35840)\n",
            "train: [319/391] Loss: 0.079 | Acc: 97.251% (39834/40960)\n",
            "train: [359/391] Loss: 0.080 | Acc: 97.229% (44803/46080)\n",
            "val: [39/79] Loss: 0.448 | Acc: 89.062% (4560/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 185\n",
            "train: [39/391] Loss: 0.073 | Acc: 97.441% (4989/5120)\n",
            "train: [79/391] Loss: 0.074 | Acc: 97.490% (9983/10240)\n",
            "train: [119/391] Loss: 0.068 | Acc: 97.715% (15009/15360)\n",
            "train: [159/391] Loss: 0.069 | Acc: 97.642% (19997/20480)\n",
            "train: [199/391] Loss: 0.068 | Acc: 97.668% (25003/25600)\n",
            "train: [239/391] Loss: 0.068 | Acc: 97.650% (29998/30720)\n",
            "train: [279/391] Loss: 0.068 | Acc: 97.676% (35007/35840)\n",
            "train: [319/391] Loss: 0.068 | Acc: 97.659% (40001/40960)\n",
            "train: [359/391] Loss: 0.067 | Acc: 97.682% (45012/46080)\n",
            "val: [39/79] Loss: 0.457 | Acc: 89.727% (4594/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 186\n",
            "train: [39/391] Loss: 0.054 | Acc: 98.184% (5027/5120)\n",
            "train: [79/391] Loss: 0.057 | Acc: 98.018% (10037/10240)\n",
            "train: [119/391] Loss: 0.058 | Acc: 98.040% (15059/15360)\n",
            "train: [159/391] Loss: 0.056 | Acc: 98.057% (20082/20480)\n",
            "train: [199/391] Loss: 0.057 | Acc: 98.008% (25090/25600)\n",
            "train: [239/391] Loss: 0.059 | Acc: 97.923% (30082/30720)\n",
            "train: [279/391] Loss: 0.059 | Acc: 97.932% (35099/35840)\n",
            "train: [319/391] Loss: 0.060 | Acc: 97.896% (40098/40960)\n",
            "train: [359/391] Loss: 0.059 | Acc: 97.923% (45123/46080)\n",
            "val: [39/79] Loss: 0.449 | Acc: 89.609% (4588/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 187\n",
            "train: [39/391] Loss: 0.055 | Acc: 98.223% (5029/5120)\n",
            "train: [79/391] Loss: 0.057 | Acc: 98.076% (10043/10240)\n",
            "train: [119/391] Loss: 0.055 | Acc: 98.145% (15075/15360)\n",
            "train: [159/391] Loss: 0.052 | Acc: 98.267% (20125/20480)\n",
            "train: [199/391] Loss: 0.050 | Acc: 98.348% (25177/25600)\n",
            "train: [239/391] Loss: 0.049 | Acc: 98.350% (30213/30720)\n",
            "train: [279/391] Loss: 0.050 | Acc: 98.306% (35233/35840)\n",
            "train: [319/391] Loss: 0.049 | Acc: 98.342% (40281/40960)\n",
            "train: [359/391] Loss: 0.048 | Acc: 98.355% (45322/46080)\n",
            "val: [39/79] Loss: 0.472 | Acc: 89.707% (4593/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 188\n",
            "train: [39/391] Loss: 0.038 | Acc: 98.574% (5047/5120)\n",
            "train: [79/391] Loss: 0.037 | Acc: 98.770% (10114/10240)\n",
            "train: [119/391] Loss: 0.041 | Acc: 98.639% (15151/15360)\n",
            "train: [159/391] Loss: 0.041 | Acc: 98.643% (20202/20480)\n",
            "train: [199/391] Loss: 0.041 | Acc: 98.625% (25248/25600)\n",
            "train: [239/391] Loss: 0.041 | Acc: 98.626% (30298/30720)\n",
            "train: [279/391] Loss: 0.040 | Acc: 98.650% (35356/35840)\n",
            "train: [319/391] Loss: 0.040 | Acc: 98.638% (40402/40960)\n",
            "train: [359/391] Loss: 0.040 | Acc: 98.622% (45445/46080)\n",
            "val: [39/79] Loss: 0.465 | Acc: 89.766% (4596/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 189\n",
            "train: [39/391] Loss: 0.037 | Acc: 98.828% (5060/5120)\n",
            "train: [79/391] Loss: 0.034 | Acc: 98.887% (10126/10240)\n",
            "train: [119/391] Loss: 0.034 | Acc: 98.848% (15183/15360)\n",
            "train: [159/391] Loss: 0.035 | Acc: 98.833% (20241/20480)\n",
            "train: [199/391] Loss: 0.035 | Acc: 98.812% (25296/25600)\n",
            "train: [239/391] Loss: 0.034 | Acc: 98.805% (30353/30720)\n",
            "train: [279/391] Loss: 0.034 | Acc: 98.795% (35408/35840)\n",
            "train: [319/391] Loss: 0.034 | Acc: 98.826% (40479/40960)\n",
            "train: [359/391] Loss: 0.035 | Acc: 98.800% (45527/46080)\n",
            "val: [39/79] Loss: 0.473 | Acc: 89.805% (4598/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 190\n",
            "train: [39/391] Loss: 0.054 | Acc: 98.242% (5030/5120)\n",
            "train: [79/391] Loss: 0.077 | Acc: 97.432% (9977/10240)\n",
            "train: [119/391] Loss: 0.090 | Acc: 96.940% (14890/15360)\n",
            "train: [159/391] Loss: 0.098 | Acc: 96.680% (19800/20480)\n",
            "train: [199/391] Loss: 0.102 | Acc: 96.543% (24715/25600)\n",
            "train: [239/391] Loss: 0.101 | Acc: 96.562% (29664/30720)\n",
            "train: [279/391] Loss: 0.104 | Acc: 96.470% (34575/35840)\n",
            "train: [319/391] Loss: 0.106 | Acc: 96.355% (39467/40960)\n",
            "train: [359/391] Loss: 0.108 | Acc: 96.287% (44369/46080)\n",
            "val: [39/79] Loss: 0.472 | Acc: 88.203% (4516/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 191\n",
            "train: [39/391] Loss: 0.112 | Acc: 95.898% (4910/5120)\n",
            "train: [79/391] Loss: 0.107 | Acc: 96.143% (9845/10240)\n",
            "train: [119/391] Loss: 0.112 | Acc: 95.938% (14736/15360)\n",
            "train: [159/391] Loss: 0.111 | Acc: 95.977% (19656/20480)\n",
            "train: [199/391] Loss: 0.110 | Acc: 96.012% (24579/25600)\n",
            "train: [239/391] Loss: 0.111 | Acc: 95.990% (29488/30720)\n",
            "train: [279/391] Loss: 0.132 | Acc: 95.427% (34201/35840)\n",
            "train: [319/391] Loss: 0.144 | Acc: 94.995% (38910/40960)\n",
            "train: [359/391] Loss: 0.206 | Acc: 93.201% (42947/46080)\n",
            "val: [39/79] Loss: 0.635 | Acc: 83.438% (4272/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 192\n",
            "train: [39/391] Loss: 0.377 | Acc: 87.754% (4493/5120)\n",
            "train: [79/391] Loss: 0.338 | Acc: 88.818% (9095/10240)\n",
            "train: [119/391] Loss: 0.332 | Acc: 88.997% (13670/15360)\n",
            "train: [159/391] Loss: 0.317 | Acc: 89.487% (18327/20480)\n",
            "train: [199/391] Loss: 0.294 | Acc: 90.242% (23102/25600)\n",
            "train: [239/391] Loss: 0.281 | Acc: 90.671% (27854/30720)\n",
            "train: [279/391] Loss: 0.269 | Acc: 91.083% (32644/35840)\n",
            "train: [319/391] Loss: 0.258 | Acc: 91.406% (37440/40960)\n",
            "train: [359/391] Loss: 0.250 | Acc: 91.730% (42269/46080)\n",
            "val: [39/79] Loss: 0.424 | Acc: 88.535% (4533/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 193\n",
            "train: [39/391] Loss: 0.182 | Acc: 93.965% (4811/5120)\n",
            "train: [79/391] Loss: 0.228 | Acc: 92.422% (9464/10240)\n",
            "train: [119/391] Loss: 0.210 | Acc: 92.852% (14262/15360)\n",
            "train: [159/391] Loss: 0.196 | Acc: 93.296% (19107/20480)\n",
            "train: [199/391] Loss: 0.186 | Acc: 93.629% (23969/25600)\n",
            "train: [239/391] Loss: 0.177 | Acc: 93.932% (28856/30720)\n",
            "train: [279/391] Loss: 0.170 | Acc: 94.146% (33742/35840)\n",
            "train: [319/391] Loss: 0.165 | Acc: 94.294% (38623/40960)\n",
            "train: [359/391] Loss: 0.160 | Acc: 94.447% (43521/46080)\n",
            "val: [39/79] Loss: 0.452 | Acc: 87.891% (4500/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 194\n",
            "train: [39/391] Loss: 0.128 | Acc: 95.430% (4886/5120)\n",
            "train: [79/391] Loss: 0.120 | Acc: 95.918% (9822/10240)\n",
            "train: [119/391] Loss: 0.116 | Acc: 95.977% (14742/15360)\n",
            "train: [159/391] Loss: 0.114 | Acc: 96.006% (19662/20480)\n",
            "train: [199/391] Loss: 0.112 | Acc: 96.086% (24598/25600)\n",
            "train: [239/391] Loss: 0.112 | Acc: 96.071% (29513/30720)\n",
            "train: [279/391] Loss: 0.110 | Acc: 96.141% (34457/35840)\n",
            "train: [319/391] Loss: 0.108 | Acc: 96.223% (39413/40960)\n",
            "train: [359/391] Loss: 0.106 | Acc: 96.274% (44363/46080)\n",
            "val: [39/79] Loss: 0.804 | Acc: 88.652% (4539/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 195\n",
            "train: [39/391] Loss: 0.089 | Acc: 96.836% (4958/5120)\n",
            "train: [79/391] Loss: 0.094 | Acc: 96.660% (9898/10240)\n",
            "train: [119/391] Loss: 0.092 | Acc: 96.751% (14861/15360)\n",
            "train: [159/391] Loss: 0.089 | Acc: 96.895% (19844/20480)\n",
            "train: [199/391] Loss: 0.087 | Acc: 97.000% (24832/25600)\n",
            "train: [239/391] Loss: 0.087 | Acc: 96.969% (29789/30720)\n",
            "train: [279/391] Loss: 0.086 | Acc: 97.026% (34774/35840)\n",
            "train: [319/391] Loss: 0.084 | Acc: 97.075% (39762/40960)\n",
            "train: [359/391] Loss: 0.083 | Acc: 97.125% (44755/46080)\n",
            "val: [39/79] Loss: 1.033 | Acc: 88.125% (4512/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 196\n",
            "train: [39/391] Loss: 0.069 | Acc: 97.480% (4991/5120)\n",
            "train: [79/391] Loss: 0.066 | Acc: 97.656% (10000/10240)\n",
            "train: [119/391] Loss: 0.065 | Acc: 97.721% (15010/15360)\n",
            "train: [159/391] Loss: 0.067 | Acc: 97.637% (19996/20480)\n",
            "train: [199/391] Loss: 0.067 | Acc: 97.594% (24984/25600)\n",
            "train: [239/391] Loss: 0.068 | Acc: 97.575% (29975/30720)\n",
            "train: [279/391] Loss: 0.070 | Acc: 97.536% (34957/35840)\n",
            "train: [319/391] Loss: 0.069 | Acc: 97.573% (39966/40960)\n",
            "train: [359/391] Loss: 0.068 | Acc: 97.650% (44997/46080)\n",
            "val: [39/79] Loss: 1.444 | Acc: 88.496% (4531/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 197\n",
            "train: [39/391] Loss: 0.061 | Acc: 97.852% (5010/5120)\n",
            "train: [79/391] Loss: 0.061 | Acc: 97.891% (10024/10240)\n",
            "train: [119/391] Loss: 0.060 | Acc: 97.910% (15039/15360)\n",
            "train: [159/391] Loss: 0.061 | Acc: 97.852% (20040/20480)\n",
            "train: [199/391] Loss: 0.062 | Acc: 97.855% (25051/25600)\n",
            "train: [239/391] Loss: 0.062 | Acc: 97.871% (30066/30720)\n",
            "train: [279/391] Loss: 0.061 | Acc: 97.913% (35092/35840)\n",
            "train: [319/391] Loss: 0.060 | Acc: 97.939% (40116/40960)\n",
            "train: [359/391] Loss: 0.060 | Acc: 97.947% (45134/46080)\n",
            "val: [39/79] Loss: 0.892 | Acc: 89.004% (4557/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 198\n",
            "train: [39/391] Loss: 0.064 | Acc: 97.871% (5011/5120)\n",
            "train: [79/391] Loss: 0.059 | Acc: 97.969% (10032/10240)\n",
            "train: [119/391] Loss: 0.056 | Acc: 97.995% (15052/15360)\n",
            "train: [159/391] Loss: 0.056 | Acc: 97.993% (20069/20480)\n",
            "train: [199/391] Loss: 0.055 | Acc: 98.035% (25097/25600)\n",
            "train: [239/391] Loss: 0.054 | Acc: 98.089% (30133/30720)\n",
            "train: [279/391] Loss: 0.055 | Acc: 98.069% (35148/35840)\n",
            "train: [319/391] Loss: 0.055 | Acc: 98.083% (40175/40960)\n",
            "train: [359/391] Loss: 0.055 | Acc: 98.092% (45201/46080)\n",
            "val: [39/79] Loss: 1.551 | Acc: 88.633% (4538/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 199\n",
            "train: [39/391] Loss: 0.054 | Acc: 98.262% (5031/5120)\n",
            "train: [79/391] Loss: 0.054 | Acc: 98.154% (10051/10240)\n",
            "train: [119/391] Loss: 0.053 | Acc: 98.171% (15079/15360)\n",
            "train: [159/391] Loss: 0.054 | Acc: 98.101% (20091/20480)\n",
            "train: [199/391] Loss: 0.055 | Acc: 98.078% (25108/25600)\n",
            "train: [239/391] Loss: 0.054 | Acc: 98.135% (30147/30720)\n",
            "train: [279/391] Loss: 0.053 | Acc: 98.184% (35189/35840)\n",
            "train: [319/391] Loss: 0.053 | Acc: 98.191% (40219/40960)\n",
            "train: [359/391] Loss: 0.052 | Acc: 98.216% (45258/46080)\n",
            "val: [39/79] Loss: 0.786 | Acc: 89.160% (4565/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 200\n",
            "train: [39/391] Loss: 0.073 | Acc: 97.500% (4992/5120)\n",
            "train: [79/391] Loss: 0.090 | Acc: 96.943% (9927/10240)\n",
            "train: [119/391] Loss: 0.096 | Acc: 96.719% (14856/15360)\n",
            "train: [159/391] Loss: 0.105 | Acc: 96.372% (19737/20480)\n",
            "train: [199/391] Loss: 0.105 | Acc: 96.316% (24657/25600)\n",
            "train: [239/391] Loss: 0.106 | Acc: 96.247% (29567/30720)\n",
            "train: [279/391] Loss: 0.107 | Acc: 96.256% (34498/35840)\n",
            "train: [319/391] Loss: 0.108 | Acc: 96.165% (39389/40960)\n",
            "train: [359/391] Loss: 0.109 | Acc: 96.155% (44308/46080)\n",
            "val: [39/79] Loss: 0.483 | Acc: 88.633% (4538/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 201\n",
            "train: [39/391] Loss: 0.105 | Acc: 96.113% (4921/5120)\n",
            "train: [79/391] Loss: 0.113 | Acc: 95.850% (9815/10240)\n",
            "train: [119/391] Loss: 0.108 | Acc: 96.061% (14755/15360)\n",
            "train: [159/391] Loss: 0.103 | Acc: 96.260% (19714/20480)\n",
            "train: [199/391] Loss: 0.102 | Acc: 96.262% (24643/25600)\n",
            "train: [239/391] Loss: 0.104 | Acc: 96.243% (29566/30720)\n",
            "train: [279/391] Loss: 0.104 | Acc: 96.244% (34494/35840)\n",
            "train: [319/391] Loss: 0.104 | Acc: 96.262% (39429/40960)\n",
            "train: [359/391] Loss: 0.105 | Acc: 96.209% (44333/46080)\n",
            "val: [39/79] Loss: 0.492 | Acc: 88.301% (4521/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 202\n",
            "train: [39/391] Loss: 0.094 | Acc: 96.387% (4935/5120)\n",
            "train: [79/391] Loss: 0.093 | Acc: 96.650% (9897/10240)\n",
            "train: [119/391] Loss: 0.094 | Acc: 96.562% (14832/15360)\n",
            "train: [159/391] Loss: 0.095 | Acc: 96.548% (19773/20480)\n",
            "train: [199/391] Loss: 0.096 | Acc: 96.465% (24695/25600)\n",
            "train: [239/391] Loss: 0.097 | Acc: 96.449% (29629/30720)\n",
            "train: [279/391] Loss: 0.099 | Acc: 96.373% (34540/35840)\n",
            "train: [319/391] Loss: 0.101 | Acc: 96.355% (39467/40960)\n",
            "train: [359/391] Loss: 0.101 | Acc: 96.378% (44411/46080)\n",
            "val: [39/79] Loss: 0.456 | Acc: 88.574% (4535/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 203\n",
            "train: [39/391] Loss: 0.078 | Acc: 97.188% (4976/5120)\n",
            "train: [79/391] Loss: 0.083 | Acc: 97.188% (9952/10240)\n",
            "train: [119/391] Loss: 0.081 | Acc: 97.201% (14930/15360)\n",
            "train: [159/391] Loss: 0.080 | Acc: 97.280% (19923/20480)\n",
            "train: [199/391] Loss: 0.080 | Acc: 97.301% (24909/25600)\n",
            "train: [239/391] Loss: 0.082 | Acc: 97.201% (29860/30720)\n",
            "train: [279/391] Loss: 0.082 | Acc: 97.185% (34831/35840)\n",
            "train: [319/391] Loss: 0.082 | Acc: 97.173% (39802/40960)\n",
            "train: [359/391] Loss: 0.083 | Acc: 97.114% (44750/46080)\n",
            "val: [39/79] Loss: 0.469 | Acc: 89.004% (4557/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 204\n",
            "train: [39/391] Loss: 0.073 | Acc: 97.383% (4986/5120)\n",
            "train: [79/391] Loss: 0.070 | Acc: 97.559% (9990/10240)\n",
            "train: [119/391] Loss: 0.074 | Acc: 97.493% (14975/15360)\n",
            "train: [159/391] Loss: 0.073 | Acc: 97.534% (19975/20480)\n",
            "train: [199/391] Loss: 0.071 | Acc: 97.605% (24987/25600)\n",
            "train: [239/391] Loss: 0.070 | Acc: 97.607% (29985/30720)\n",
            "train: [279/391] Loss: 0.072 | Acc: 97.545% (34960/35840)\n",
            "train: [319/391] Loss: 0.073 | Acc: 97.473% (39925/40960)\n",
            "train: [359/391] Loss: 0.073 | Acc: 97.493% (44925/46080)\n",
            "val: [39/79] Loss: 0.471 | Acc: 88.867% (4550/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 205\n",
            "train: [39/391] Loss: 0.067 | Acc: 97.773% (5006/5120)\n",
            "train: [79/391] Loss: 0.064 | Acc: 97.949% (10030/10240)\n",
            "train: [119/391] Loss: 0.064 | Acc: 97.819% (15025/15360)\n",
            "train: [159/391] Loss: 0.065 | Acc: 97.764% (20022/20480)\n",
            "train: [199/391] Loss: 0.064 | Acc: 97.820% (25042/25600)\n",
            "train: [239/391] Loss: 0.063 | Acc: 97.858% (30062/30720)\n",
            "train: [279/391] Loss: 0.063 | Acc: 97.818% (35058/35840)\n",
            "train: [319/391] Loss: 0.064 | Acc: 97.810% (40063/40960)\n",
            "train: [359/391] Loss: 0.064 | Acc: 97.819% (45075/46080)\n",
            "val: [39/79] Loss: 0.487 | Acc: 89.062% (4560/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 206\n",
            "train: [39/391] Loss: 0.047 | Acc: 98.320% (5034/5120)\n",
            "train: [79/391] Loss: 0.047 | Acc: 98.301% (10066/10240)\n",
            "train: [119/391] Loss: 0.049 | Acc: 98.242% (15090/15360)\n",
            "train: [159/391] Loss: 0.049 | Acc: 98.193% (20110/20480)\n",
            "train: [199/391] Loss: 0.049 | Acc: 98.195% (25138/25600)\n",
            "train: [239/391] Loss: 0.050 | Acc: 98.187% (30163/30720)\n",
            "train: [279/391] Loss: 0.050 | Acc: 98.186% (35190/35840)\n",
            "train: [319/391] Loss: 0.050 | Acc: 98.157% (40205/40960)\n",
            "train: [359/391] Loss: 0.050 | Acc: 98.179% (45241/46080)\n",
            "val: [39/79] Loss: 0.500 | Acc: 89.414% (4578/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 207\n",
            "train: [39/391] Loss: 0.050 | Acc: 98.340% (5035/5120)\n",
            "train: [79/391] Loss: 0.053 | Acc: 98.164% (10052/10240)\n",
            "train: [119/391] Loss: 0.050 | Acc: 98.262% (15093/15360)\n",
            "train: [159/391] Loss: 0.048 | Acc: 98.311% (20134/20480)\n",
            "train: [199/391] Loss: 0.047 | Acc: 98.336% (25174/25600)\n",
            "train: [239/391] Loss: 0.045 | Acc: 98.411% (30232/30720)\n",
            "train: [279/391] Loss: 0.044 | Acc: 98.426% (35276/35840)\n",
            "train: [319/391] Loss: 0.044 | Acc: 98.423% (40314/40960)\n",
            "train: [359/391] Loss: 0.044 | Acc: 98.414% (45349/46080)\n",
            "val: [39/79] Loss: 0.506 | Acc: 89.141% (4564/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 208\n",
            "train: [39/391] Loss: 0.048 | Acc: 98.281% (5032/5120)\n",
            "train: [79/391] Loss: 0.044 | Acc: 98.438% (10080/10240)\n",
            "train: [119/391] Loss: 0.042 | Acc: 98.548% (15137/15360)\n",
            "train: [159/391] Loss: 0.042 | Acc: 98.574% (20188/20480)\n",
            "train: [199/391] Loss: 0.040 | Acc: 98.637% (25251/25600)\n",
            "train: [239/391] Loss: 0.041 | Acc: 98.604% (30291/30720)\n",
            "train: [279/391] Loss: 0.041 | Acc: 98.588% (35334/35840)\n",
            "train: [319/391] Loss: 0.041 | Acc: 98.601% (40387/40960)\n",
            "train: [359/391] Loss: 0.041 | Acc: 98.594% (45432/46080)\n",
            "val: [39/79] Loss: 0.508 | Acc: 89.434% (4579/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 209\n",
            "train: [39/391] Loss: 0.041 | Acc: 98.730% (5055/5120)\n",
            "train: [79/391] Loss: 0.035 | Acc: 98.906% (10128/10240)\n",
            "train: [119/391] Loss: 0.037 | Acc: 98.822% (15179/15360)\n",
            "train: [159/391] Loss: 0.037 | Acc: 98.809% (20236/20480)\n",
            "train: [199/391] Loss: 0.037 | Acc: 98.812% (25296/25600)\n",
            "train: [239/391] Loss: 0.036 | Acc: 98.835% (30362/30720)\n",
            "train: [279/391] Loss: 0.036 | Acc: 98.800% (35410/35840)\n",
            "train: [319/391] Loss: 0.036 | Acc: 98.789% (40464/40960)\n",
            "train: [359/391] Loss: 0.036 | Acc: 98.793% (45524/46080)\n",
            "val: [39/79] Loss: 0.503 | Acc: 89.316% (4573/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 210\n",
            "train: [39/391] Loss: 0.047 | Acc: 98.418% (5039/5120)\n",
            "train: [79/391] Loss: 0.067 | Acc: 97.832% (10018/10240)\n",
            "train: [119/391] Loss: 0.084 | Acc: 97.148% (14922/15360)\n",
            "train: [159/391] Loss: 0.093 | Acc: 96.855% (19836/20480)\n",
            "train: [199/391] Loss: 0.100 | Acc: 96.555% (24718/25600)\n",
            "train: [239/391] Loss: 0.103 | Acc: 96.475% (29637/30720)\n",
            "train: [279/391] Loss: 0.103 | Acc: 96.490% (34582/35840)\n",
            "train: [319/391] Loss: 0.105 | Acc: 96.365% (39471/40960)\n",
            "train: [359/391] Loss: 0.105 | Acc: 96.296% (44373/46080)\n",
            "val: [39/79] Loss: 0.454 | Acc: 88.574% (4535/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 211\n",
            "train: [39/391] Loss: 0.092 | Acc: 96.777% (4955/5120)\n",
            "train: [79/391] Loss: 0.096 | Acc: 96.533% (9885/10240)\n",
            "train: [119/391] Loss: 0.096 | Acc: 96.549% (14830/15360)\n",
            "train: [159/391] Loss: 0.100 | Acc: 96.392% (19741/20480)\n",
            "train: [199/391] Loss: 0.102 | Acc: 96.309% (24655/25600)\n",
            "train: [239/391] Loss: 0.102 | Acc: 96.312% (29587/30720)\n",
            "train: [279/391] Loss: 0.100 | Acc: 96.339% (34528/35840)\n",
            "train: [319/391] Loss: 0.102 | Acc: 96.309% (39448/40960)\n",
            "train: [359/391] Loss: 0.102 | Acc: 96.337% (44392/46080)\n",
            "val: [39/79] Loss: 0.472 | Acc: 88.105% (4511/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 212\n",
            "train: [39/391] Loss: 0.087 | Acc: 96.855% (4959/5120)\n",
            "train: [79/391] Loss: 0.086 | Acc: 96.982% (9931/10240)\n",
            "train: [119/391] Loss: 0.085 | Acc: 97.038% (14905/15360)\n",
            "train: [159/391] Loss: 0.092 | Acc: 96.860% (19837/20480)\n",
            "train: [199/391] Loss: 0.091 | Acc: 96.816% (24785/25600)\n",
            "train: [239/391] Loss: 0.091 | Acc: 96.790% (29734/30720)\n",
            "train: [279/391] Loss: 0.090 | Acc: 96.805% (34695/35840)\n",
            "train: [319/391] Loss: 0.089 | Acc: 96.809% (39653/40960)\n",
            "train: [359/391] Loss: 0.090 | Acc: 96.780% (44596/46080)\n",
            "val: [39/79] Loss: 0.467 | Acc: 88.535% (4533/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 213\n",
            "train: [39/391] Loss: 0.077 | Acc: 97.383% (4986/5120)\n",
            "train: [79/391] Loss: 0.071 | Acc: 97.568% (9991/10240)\n",
            "train: [119/391] Loss: 0.076 | Acc: 97.428% (14965/15360)\n",
            "train: [159/391] Loss: 0.079 | Acc: 97.256% (19918/20480)\n",
            "train: [199/391] Loss: 0.078 | Acc: 97.266% (24900/25600)\n",
            "train: [239/391] Loss: 0.078 | Acc: 97.334% (29901/30720)\n",
            "train: [279/391] Loss: 0.079 | Acc: 97.282% (34866/35840)\n",
            "train: [319/391] Loss: 0.079 | Acc: 97.246% (39832/40960)\n",
            "train: [359/391] Loss: 0.079 | Acc: 97.229% (44803/46080)\n",
            "val: [39/79] Loss: 0.478 | Acc: 88.809% (4547/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 214\n",
            "train: [39/391] Loss: 0.066 | Acc: 97.656% (5000/5120)\n",
            "train: [79/391] Loss: 0.067 | Acc: 97.549% (9989/10240)\n",
            "train: [119/391] Loss: 0.070 | Acc: 97.480% (14973/15360)\n",
            "train: [159/391] Loss: 0.071 | Acc: 97.461% (19960/20480)\n",
            "train: [199/391] Loss: 0.072 | Acc: 97.426% (24941/25600)\n",
            "train: [239/391] Loss: 0.072 | Acc: 97.454% (29938/30720)\n",
            "train: [279/391] Loss: 0.070 | Acc: 97.542% (34959/35840)\n",
            "train: [319/391] Loss: 0.071 | Acc: 97.527% (39947/40960)\n",
            "train: [359/391] Loss: 0.071 | Acc: 97.507% (44931/46080)\n",
            "val: [39/79] Loss: 0.481 | Acc: 88.535% (4533/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 215\n",
            "train: [39/391] Loss: 0.068 | Acc: 97.441% (4989/5120)\n",
            "train: [79/391] Loss: 0.062 | Acc: 97.793% (10014/10240)\n",
            "train: [119/391] Loss: 0.063 | Acc: 97.734% (15012/15360)\n",
            "train: [159/391] Loss: 0.060 | Acc: 97.773% (20024/20480)\n",
            "train: [199/391] Loss: 0.059 | Acc: 97.859% (25052/25600)\n",
            "train: [239/391] Loss: 0.059 | Acc: 97.907% (30077/30720)\n",
            "train: [279/391] Loss: 0.058 | Acc: 97.924% (35096/35840)\n",
            "train: [319/391] Loss: 0.058 | Acc: 97.969% (40128/40960)\n",
            "train: [359/391] Loss: 0.059 | Acc: 97.919% (45121/46080)\n",
            "val: [39/79] Loss: 0.476 | Acc: 89.121% (4563/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 216\n",
            "train: [39/391] Loss: 0.055 | Acc: 98.164% (5026/5120)\n",
            "train: [79/391] Loss: 0.057 | Acc: 98.057% (10041/10240)\n",
            "train: [119/391] Loss: 0.055 | Acc: 98.105% (15069/15360)\n",
            "train: [159/391] Loss: 0.052 | Acc: 98.203% (20112/20480)\n",
            "train: [199/391] Loss: 0.051 | Acc: 98.246% (25151/25600)\n",
            "train: [239/391] Loss: 0.051 | Acc: 98.249% (30182/30720)\n",
            "train: [279/391] Loss: 0.050 | Acc: 98.281% (35224/35840)\n",
            "train: [319/391] Loss: 0.050 | Acc: 98.276% (40254/40960)\n",
            "train: [359/391] Loss: 0.050 | Acc: 98.294% (45294/46080)\n",
            "val: [39/79] Loss: 0.490 | Acc: 89.219% (4568/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 217\n",
            "train: [39/391] Loss: 0.042 | Acc: 98.594% (5048/5120)\n",
            "train: [79/391] Loss: 0.045 | Acc: 98.457% (10082/10240)\n",
            "train: [119/391] Loss: 0.044 | Acc: 98.522% (15133/15360)\n",
            "train: [159/391] Loss: 0.043 | Acc: 98.569% (20187/20480)\n",
            "train: [199/391] Loss: 0.043 | Acc: 98.578% (25236/25600)\n",
            "train: [239/391] Loss: 0.043 | Acc: 98.597% (30289/30720)\n",
            "train: [279/391] Loss: 0.042 | Acc: 98.616% (35344/35840)\n",
            "train: [319/391] Loss: 0.043 | Acc: 98.589% (40382/40960)\n",
            "train: [359/391] Loss: 0.042 | Acc: 98.585% (45428/46080)\n",
            "val: [39/79] Loss: 0.483 | Acc: 89.199% (4567/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 218\n",
            "train: [39/391] Loss: 0.040 | Acc: 98.672% (5052/5120)\n",
            "train: [79/391] Loss: 0.038 | Acc: 98.662% (10103/10240)\n",
            "train: [119/391] Loss: 0.037 | Acc: 98.672% (15156/15360)\n",
            "train: [159/391] Loss: 0.036 | Acc: 98.716% (20217/20480)\n",
            "train: [199/391] Loss: 0.036 | Acc: 98.742% (25278/25600)\n",
            "train: [239/391] Loss: 0.036 | Acc: 98.740% (30333/30720)\n",
            "train: [279/391] Loss: 0.036 | Acc: 98.753% (35393/35840)\n",
            "train: [319/391] Loss: 0.035 | Acc: 98.752% (40449/40960)\n",
            "train: [359/391] Loss: 0.035 | Acc: 98.739% (45499/46080)\n",
            "val: [39/79] Loss: 0.483 | Acc: 89.395% (4577/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 219\n",
            "train: [39/391] Loss: 0.032 | Acc: 98.887% (5063/5120)\n",
            "train: [79/391] Loss: 0.030 | Acc: 98.945% (10132/10240)\n",
            "train: [119/391] Loss: 0.031 | Acc: 98.926% (15195/15360)\n",
            "train: [159/391] Loss: 0.032 | Acc: 98.896% (20254/20480)\n",
            "train: [199/391] Loss: 0.032 | Acc: 98.887% (25315/25600)\n",
            "train: [239/391] Loss: 0.031 | Acc: 98.903% (30383/30720)\n",
            "train: [279/391] Loss: 0.031 | Acc: 98.937% (35459/35840)\n",
            "train: [319/391] Loss: 0.030 | Acc: 98.936% (40524/40960)\n",
            "train: [359/391] Loss: 0.030 | Acc: 98.939% (45591/46080)\n",
            "val: [39/79] Loss: 0.491 | Acc: 89.668% (4591/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 220\n",
            "train: [39/391] Loss: 0.040 | Acc: 98.496% (5043/5120)\n",
            "train: [79/391] Loss: 0.060 | Acc: 97.832% (10018/10240)\n",
            "train: [119/391] Loss: 0.075 | Acc: 97.292% (14944/15360)\n",
            "train: [159/391] Loss: 0.084 | Acc: 97.051% (19876/20480)\n",
            "train: [199/391] Loss: 0.086 | Acc: 96.906% (24808/25600)\n",
            "train: [239/391] Loss: 0.087 | Acc: 96.865% (29757/30720)\n",
            "train: [279/391] Loss: 0.089 | Acc: 96.816% (34699/35840)\n",
            "train: [319/391] Loss: 0.091 | Acc: 96.782% (39642/40960)\n",
            "train: [359/391] Loss: 0.093 | Acc: 96.688% (44554/46080)\n",
            "val: [39/79] Loss: 0.452 | Acc: 88.516% (4532/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 221\n",
            "train: [39/391] Loss: 0.091 | Acc: 96.523% (4942/5120)\n",
            "train: [79/391] Loss: 0.092 | Acc: 96.680% (9900/10240)\n",
            "train: [119/391] Loss: 0.097 | Acc: 96.797% (14868/15360)\n",
            "train: [159/391] Loss: 0.095 | Acc: 96.846% (19834/20480)\n",
            "train: [199/391] Loss: 0.095 | Acc: 96.801% (24781/25600)\n",
            "train: [239/391] Loss: 0.095 | Acc: 96.738% (29718/30720)\n",
            "train: [279/391] Loss: 0.097 | Acc: 96.660% (34643/35840)\n",
            "train: [319/391] Loss: 0.097 | Acc: 96.631% (39580/40960)\n",
            "train: [359/391] Loss: 0.097 | Acc: 96.654% (44538/46080)\n",
            "val: [39/79] Loss: 0.457 | Acc: 88.809% (4547/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 222\n",
            "train: [39/391] Loss: 0.080 | Acc: 97.148% (4974/5120)\n",
            "train: [79/391] Loss: 0.083 | Acc: 97.119% (9945/10240)\n",
            "train: [119/391] Loss: 0.085 | Acc: 97.044% (14906/15360)\n",
            "train: [159/391] Loss: 0.084 | Acc: 97.017% (19869/20480)\n",
            "train: [199/391] Loss: 0.084 | Acc: 97.031% (24840/25600)\n",
            "train: [239/391] Loss: 0.085 | Acc: 97.002% (29799/30720)\n",
            "train: [279/391] Loss: 0.085 | Acc: 97.003% (34766/35840)\n",
            "train: [319/391] Loss: 0.086 | Acc: 96.951% (39711/40960)\n",
            "train: [359/391] Loss: 0.087 | Acc: 96.908% (44655/46080)\n",
            "val: [39/79] Loss: 0.470 | Acc: 88.398% (4526/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 223\n",
            "train: [39/391] Loss: 0.082 | Acc: 97.363% (4985/5120)\n",
            "train: [79/391] Loss: 0.076 | Acc: 97.471% (9981/10240)\n",
            "train: [119/391] Loss: 0.074 | Acc: 97.500% (14976/15360)\n",
            "train: [159/391] Loss: 0.075 | Acc: 97.461% (19960/20480)\n",
            "train: [199/391] Loss: 0.075 | Acc: 97.457% (24949/25600)\n",
            "train: [239/391] Loss: 0.076 | Acc: 97.412% (29925/30720)\n",
            "train: [279/391] Loss: 0.075 | Acc: 97.464% (34931/35840)\n",
            "train: [319/391] Loss: 0.076 | Acc: 97.412% (39900/40960)\n",
            "train: [359/391] Loss: 0.076 | Acc: 97.405% (44884/46080)\n",
            "val: [39/79] Loss: 0.491 | Acc: 88.203% (4516/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 224\n",
            "train: [39/391] Loss: 0.076 | Acc: 97.266% (4980/5120)\n",
            "train: [79/391] Loss: 0.071 | Acc: 97.568% (9991/10240)\n",
            "train: [119/391] Loss: 0.068 | Acc: 97.656% (15000/15360)\n",
            "train: [159/391] Loss: 0.071 | Acc: 97.578% (19984/20480)\n",
            "train: [199/391] Loss: 0.071 | Acc: 97.562% (24976/25600)\n",
            "train: [239/391] Loss: 0.071 | Acc: 97.546% (29966/30720)\n",
            "train: [279/391] Loss: 0.071 | Acc: 97.547% (34961/35840)\n",
            "train: [319/391] Loss: 0.069 | Acc: 97.598% (39976/40960)\n",
            "train: [359/391] Loss: 0.069 | Acc: 97.622% (44984/46080)\n",
            "val: [39/79] Loss: 0.470 | Acc: 89.258% (4570/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 225\n",
            "train: [39/391] Loss: 0.059 | Acc: 97.852% (5010/5120)\n",
            "train: [79/391] Loss: 0.056 | Acc: 97.939% (10029/10240)\n",
            "train: [119/391] Loss: 0.055 | Acc: 98.014% (15055/15360)\n",
            "train: [159/391] Loss: 0.055 | Acc: 98.062% (20083/20480)\n",
            "train: [199/391] Loss: 0.056 | Acc: 98.016% (25092/25600)\n",
            "train: [239/391] Loss: 0.055 | Acc: 98.050% (30121/30720)\n",
            "train: [279/391] Loss: 0.055 | Acc: 98.083% (35153/35840)\n",
            "train: [319/391] Loss: 0.054 | Acc: 98.086% (40176/40960)\n",
            "train: [359/391] Loss: 0.054 | Acc: 98.110% (45209/46080)\n",
            "val: [39/79] Loss: 0.470 | Acc: 89.375% (4576/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 226\n",
            "train: [39/391] Loss: 0.048 | Acc: 98.359% (5036/5120)\n",
            "train: [79/391] Loss: 0.047 | Acc: 98.262% (10062/10240)\n",
            "train: [119/391] Loss: 0.045 | Acc: 98.372% (15110/15360)\n",
            "train: [159/391] Loss: 0.045 | Acc: 98.384% (20149/20480)\n",
            "train: [199/391] Loss: 0.046 | Acc: 98.379% (25185/25600)\n",
            "train: [239/391] Loss: 0.045 | Acc: 98.421% (30235/30720)\n",
            "train: [279/391] Loss: 0.044 | Acc: 98.471% (35292/35840)\n",
            "train: [319/391] Loss: 0.044 | Acc: 98.484% (40339/40960)\n",
            "train: [359/391] Loss: 0.043 | Acc: 98.505% (45391/46080)\n",
            "val: [39/79] Loss: 0.486 | Acc: 89.512% (4583/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 227\n",
            "train: [39/391] Loss: 0.036 | Acc: 98.691% (5053/5120)\n",
            "train: [79/391] Loss: 0.038 | Acc: 98.652% (10102/10240)\n",
            "train: [119/391] Loss: 0.038 | Acc: 98.639% (15151/15360)\n",
            "train: [159/391] Loss: 0.038 | Acc: 98.613% (20196/20480)\n",
            "train: [199/391] Loss: 0.038 | Acc: 98.598% (25241/25600)\n",
            "train: [239/391] Loss: 0.039 | Acc: 98.590% (30287/30720)\n",
            "train: [279/391] Loss: 0.038 | Acc: 98.627% (35348/35840)\n",
            "train: [319/391] Loss: 0.037 | Acc: 98.647% (40406/40960)\n",
            "train: [359/391] Loss: 0.037 | Acc: 98.655% (45460/46080)\n",
            "val: [39/79] Loss: 0.493 | Acc: 89.512% (4583/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 228\n",
            "train: [39/391] Loss: 0.031 | Acc: 99.004% (5069/5120)\n",
            "train: [79/391] Loss: 0.029 | Acc: 99.014% (10139/10240)\n",
            "train: [119/391] Loss: 0.029 | Acc: 99.004% (15207/15360)\n",
            "train: [159/391] Loss: 0.030 | Acc: 98.984% (20272/20480)\n",
            "train: [199/391] Loss: 0.030 | Acc: 98.969% (25336/25600)\n",
            "train: [239/391] Loss: 0.031 | Acc: 98.955% (30399/30720)\n",
            "train: [279/391] Loss: 0.031 | Acc: 98.929% (35456/35840)\n",
            "train: [319/391] Loss: 0.031 | Acc: 98.921% (40518/40960)\n",
            "train: [359/391] Loss: 0.031 | Acc: 98.919% (45582/46080)\n",
            "val: [39/79] Loss: 0.494 | Acc: 89.727% (4594/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 229\n",
            "train: [39/391] Loss: 0.030 | Acc: 98.945% (5066/5120)\n",
            "train: [79/391] Loss: 0.032 | Acc: 98.887% (10126/10240)\n",
            "train: [119/391] Loss: 0.032 | Acc: 98.887% (15189/15360)\n",
            "train: [159/391] Loss: 0.033 | Acc: 98.872% (20249/20480)\n",
            "train: [199/391] Loss: 0.032 | Acc: 98.941% (25329/25600)\n",
            "train: [239/391] Loss: 0.030 | Acc: 98.978% (30406/30720)\n",
            "train: [279/391] Loss: 0.031 | Acc: 98.959% (35467/35840)\n",
            "train: [319/391] Loss: 0.032 | Acc: 98.958% (40533/40960)\n",
            "train: [359/391] Loss: 0.031 | Acc: 98.987% (45613/46080)\n",
            "val: [39/79] Loss: 0.491 | Acc: 89.883% (4602/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 230\n",
            "train: [39/391] Loss: 0.044 | Acc: 98.594% (5048/5120)\n",
            "train: [79/391] Loss: 0.065 | Acc: 97.764% (10011/10240)\n",
            "train: [119/391] Loss: 0.074 | Acc: 97.474% (14972/15360)\n",
            "train: [159/391] Loss: 0.080 | Acc: 97.168% (19900/20480)\n",
            "train: [199/391] Loss: 0.084 | Acc: 97.074% (24851/25600)\n",
            "train: [239/391] Loss: 0.086 | Acc: 97.008% (29801/30720)\n",
            "train: [279/391] Loss: 0.087 | Acc: 96.981% (34758/35840)\n",
            "train: [319/391] Loss: 0.087 | Acc: 96.951% (39711/40960)\n",
            "train: [359/391] Loss: 0.089 | Acc: 96.879% (44642/46080)\n",
            "val: [39/79] Loss: 0.478 | Acc: 88.438% (4528/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 231\n",
            "train: [39/391] Loss: 0.093 | Acc: 96.719% (4952/5120)\n",
            "train: [79/391] Loss: 0.083 | Acc: 97.012% (9934/10240)\n",
            "train: [119/391] Loss: 0.085 | Acc: 96.921% (14887/15360)\n",
            "train: [159/391] Loss: 0.085 | Acc: 96.909% (19847/20480)\n",
            "train: [199/391] Loss: 0.086 | Acc: 96.949% (24819/25600)\n",
            "train: [239/391] Loss: 0.087 | Acc: 96.891% (29765/30720)\n",
            "train: [279/391] Loss: 0.087 | Acc: 96.911% (34733/35840)\n",
            "train: [319/391] Loss: 0.088 | Acc: 96.873% (39679/40960)\n",
            "train: [359/391] Loss: 0.090 | Acc: 96.873% (44639/46080)\n",
            "val: [39/79] Loss: 0.469 | Acc: 88.320% (4522/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 232\n",
            "train: [39/391] Loss: 0.081 | Acc: 96.992% (4966/5120)\n",
            "train: [79/391] Loss: 0.082 | Acc: 96.924% (9925/10240)\n",
            "train: [119/391] Loss: 0.081 | Acc: 96.992% (14898/15360)\n",
            "train: [159/391] Loss: 0.081 | Acc: 97.031% (19872/20480)\n",
            "train: [199/391] Loss: 0.083 | Acc: 96.988% (24829/25600)\n",
            "train: [239/391] Loss: 0.082 | Acc: 97.015% (29803/30720)\n",
            "train: [279/391] Loss: 0.082 | Acc: 97.040% (34779/35840)\n",
            "train: [319/391] Loss: 0.081 | Acc: 97.043% (39749/40960)\n",
            "train: [359/391] Loss: 0.082 | Acc: 97.062% (44726/46080)\n",
            "val: [39/79] Loss: 0.465 | Acc: 88.750% (4544/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 233\n",
            "train: [39/391] Loss: 0.075 | Acc: 97.383% (4986/5120)\n",
            "train: [79/391] Loss: 0.079 | Acc: 97.188% (9952/10240)\n",
            "train: [119/391] Loss: 0.073 | Acc: 97.422% (14964/15360)\n",
            "train: [159/391] Loss: 0.069 | Acc: 97.549% (19978/20480)\n",
            "train: [199/391] Loss: 0.072 | Acc: 97.465% (24951/25600)\n",
            "train: [239/391] Loss: 0.072 | Acc: 97.454% (29938/30720)\n",
            "train: [279/391] Loss: 0.073 | Acc: 97.400% (34908/35840)\n",
            "train: [319/391] Loss: 0.073 | Acc: 97.415% (39901/40960)\n",
            "train: [359/391] Loss: 0.072 | Acc: 97.463% (44911/46080)\n",
            "val: [39/79] Loss: 0.470 | Acc: 89.531% (4584/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 234\n",
            "train: [39/391] Loss: 0.069 | Acc: 97.480% (4991/5120)\n",
            "train: [79/391] Loss: 0.067 | Acc: 97.598% (9994/10240)\n",
            "train: [119/391] Loss: 0.066 | Acc: 97.630% (14996/15360)\n",
            "train: [159/391] Loss: 0.067 | Acc: 97.598% (19988/20480)\n",
            "train: [199/391] Loss: 0.065 | Acc: 97.664% (25002/25600)\n",
            "train: [239/391] Loss: 0.064 | Acc: 97.682% (30008/30720)\n",
            "train: [279/391] Loss: 0.064 | Acc: 97.706% (35018/35840)\n",
            "train: [319/391] Loss: 0.063 | Acc: 97.744% (40036/40960)\n",
            "train: [359/391] Loss: 0.064 | Acc: 97.747% (45042/46080)\n",
            "val: [39/79] Loss: 0.476 | Acc: 89.277% (4571/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 235\n",
            "train: [39/391] Loss: 0.055 | Acc: 97.930% (5014/5120)\n",
            "train: [79/391] Loss: 0.050 | Acc: 98.135% (10049/10240)\n",
            "train: [119/391] Loss: 0.050 | Acc: 98.171% (15079/15360)\n",
            "train: [159/391] Loss: 0.051 | Acc: 98.149% (20101/20480)\n",
            "train: [199/391] Loss: 0.052 | Acc: 98.152% (25127/25600)\n",
            "train: [239/391] Loss: 0.053 | Acc: 98.122% (30143/30720)\n",
            "train: [279/391] Loss: 0.053 | Acc: 98.092% (35156/35840)\n",
            "train: [319/391] Loss: 0.052 | Acc: 98.120% (40190/40960)\n",
            "train: [359/391] Loss: 0.052 | Acc: 98.108% (45208/46080)\n",
            "val: [39/79] Loss: 0.478 | Acc: 89.492% (4582/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 236\n",
            "train: [39/391] Loss: 0.049 | Acc: 98.555% (5046/5120)\n",
            "train: [79/391] Loss: 0.045 | Acc: 98.623% (10099/10240)\n",
            "train: [119/391] Loss: 0.044 | Acc: 98.600% (15145/15360)\n",
            "train: [159/391] Loss: 0.044 | Acc: 98.633% (20200/20480)\n",
            "train: [199/391] Loss: 0.043 | Acc: 98.645% (25253/25600)\n",
            "train: [239/391] Loss: 0.043 | Acc: 98.604% (30291/30720)\n",
            "train: [279/391] Loss: 0.042 | Acc: 98.622% (35346/35840)\n",
            "train: [319/391] Loss: 0.042 | Acc: 98.618% (40394/40960)\n",
            "train: [359/391] Loss: 0.041 | Acc: 98.637% (45452/46080)\n",
            "val: [39/79] Loss: 0.494 | Acc: 89.199% (4567/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 237\n",
            "train: [39/391] Loss: 0.038 | Acc: 98.770% (5057/5120)\n",
            "train: [79/391] Loss: 0.041 | Acc: 98.564% (10093/10240)\n",
            "train: [119/391] Loss: 0.037 | Acc: 98.717% (15163/15360)\n",
            "train: [159/391] Loss: 0.035 | Acc: 98.779% (20230/20480)\n",
            "train: [199/391] Loss: 0.035 | Acc: 98.766% (25284/25600)\n",
            "train: [239/391] Loss: 0.035 | Acc: 98.783% (30346/30720)\n",
            "train: [279/391] Loss: 0.035 | Acc: 98.756% (35394/35840)\n",
            "train: [319/391] Loss: 0.035 | Acc: 98.782% (40461/40960)\n",
            "train: [359/391] Loss: 0.035 | Acc: 98.763% (45510/46080)\n",
            "val: [39/79] Loss: 0.495 | Acc: 89.746% (4595/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 238\n",
            "train: [39/391] Loss: 0.031 | Acc: 98.809% (5059/5120)\n",
            "train: [79/391] Loss: 0.029 | Acc: 98.945% (10132/10240)\n",
            "train: [119/391] Loss: 0.031 | Acc: 98.913% (15193/15360)\n",
            "train: [159/391] Loss: 0.031 | Acc: 98.945% (20264/20480)\n",
            "train: [199/391] Loss: 0.029 | Acc: 99.043% (25355/25600)\n",
            "train: [239/391] Loss: 0.029 | Acc: 99.053% (30429/30720)\n",
            "train: [279/391] Loss: 0.029 | Acc: 99.062% (35504/35840)\n",
            "train: [319/391] Loss: 0.030 | Acc: 99.038% (40566/40960)\n",
            "train: [359/391] Loss: 0.030 | Acc: 99.039% (45637/46080)\n",
            "val: [39/79] Loss: 0.492 | Acc: 89.961% (4606/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 239\n",
            "train: [39/391] Loss: 0.034 | Acc: 98.887% (5063/5120)\n",
            "train: [79/391] Loss: 0.031 | Acc: 98.916% (10129/10240)\n",
            "train: [119/391] Loss: 0.031 | Acc: 98.906% (15192/15360)\n",
            "train: [159/391] Loss: 0.030 | Acc: 98.960% (20267/20480)\n",
            "train: [199/391] Loss: 0.030 | Acc: 98.988% (25341/25600)\n",
            "train: [239/391] Loss: 0.029 | Acc: 99.020% (30419/30720)\n",
            "train: [279/391] Loss: 0.029 | Acc: 99.018% (35488/35840)\n",
            "train: [319/391] Loss: 0.029 | Acc: 99.023% (40560/40960)\n",
            "train: [359/391] Loss: 0.029 | Acc: 99.002% (45620/46080)\n",
            "val: [39/79] Loss: 0.501 | Acc: 89.941% (4605/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 240\n",
            "train: [39/391] Loss: 0.040 | Acc: 98.516% (5044/5120)\n",
            "train: [79/391] Loss: 0.062 | Acc: 97.793% (10014/10240)\n",
            "train: [119/391] Loss: 0.072 | Acc: 97.337% (14951/15360)\n",
            "train: [159/391] Loss: 0.078 | Acc: 97.124% (19891/20480)\n",
            "train: [199/391] Loss: 0.082 | Acc: 97.027% (24839/25600)\n",
            "train: [239/391] Loss: 0.083 | Acc: 96.999% (29798/30720)\n",
            "train: [279/391] Loss: 0.082 | Acc: 97.048% (34782/35840)\n",
            "train: [319/391] Loss: 0.084 | Acc: 96.985% (39725/40960)\n",
            "train: [359/391] Loss: 0.087 | Acc: 96.868% (44637/46080)\n",
            "val: [39/79] Loss: 0.479 | Acc: 87.793% (4495/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 241\n",
            "train: [39/391] Loss: 0.086 | Acc: 96.836% (4958/5120)\n",
            "train: [79/391] Loss: 0.085 | Acc: 96.924% (9925/10240)\n",
            "train: [119/391] Loss: 0.084 | Acc: 96.999% (14899/15360)\n",
            "train: [159/391] Loss: 0.084 | Acc: 97.031% (19872/20480)\n",
            "train: [199/391] Loss: 0.084 | Acc: 96.996% (24831/25600)\n",
            "train: [239/391] Loss: 0.085 | Acc: 97.012% (29802/30720)\n",
            "train: [279/391] Loss: 0.083 | Acc: 97.042% (34780/35840)\n",
            "train: [319/391] Loss: 0.083 | Acc: 97.065% (39758/40960)\n",
            "train: [359/391] Loss: 0.084 | Acc: 97.053% (44722/46080)\n",
            "val: [39/79] Loss: 0.490 | Acc: 88.086% (4510/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 242\n",
            "train: [39/391] Loss: 0.078 | Acc: 97.090% (4971/5120)\n",
            "train: [79/391] Loss: 0.075 | Acc: 97.324% (9966/10240)\n",
            "train: [119/391] Loss: 0.074 | Acc: 97.324% (14949/15360)\n",
            "train: [159/391] Loss: 0.074 | Acc: 97.368% (19941/20480)\n",
            "train: [199/391] Loss: 0.075 | Acc: 97.316% (24913/25600)\n",
            "train: [239/391] Loss: 0.078 | Acc: 97.240% (29872/30720)\n",
            "train: [279/391] Loss: 0.078 | Acc: 97.260% (34858/35840)\n",
            "train: [319/391] Loss: 0.078 | Acc: 97.275% (39844/40960)\n",
            "train: [359/391] Loss: 0.078 | Acc: 97.255% (44815/46080)\n",
            "val: [39/79] Loss: 0.503 | Acc: 88.262% (4519/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 243\n",
            "train: [39/391] Loss: 0.058 | Acc: 98.086% (5022/5120)\n",
            "train: [79/391] Loss: 0.060 | Acc: 97.988% (10034/10240)\n",
            "train: [119/391] Loss: 0.060 | Acc: 97.962% (15047/15360)\n",
            "train: [159/391] Loss: 0.061 | Acc: 97.920% (20054/20480)\n",
            "train: [199/391] Loss: 0.063 | Acc: 97.801% (25037/25600)\n",
            "train: [239/391] Loss: 0.064 | Acc: 97.754% (30030/30720)\n",
            "train: [279/391] Loss: 0.065 | Acc: 97.715% (35021/35840)\n",
            "train: [319/391] Loss: 0.066 | Acc: 97.673% (40007/40960)\n",
            "train: [359/391] Loss: 0.067 | Acc: 97.658% (45001/46080)\n",
            "val: [39/79] Loss: 0.498 | Acc: 88.945% (4554/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 244\n",
            "train: [39/391] Loss: 0.066 | Acc: 97.910% (5013/5120)\n",
            "train: [79/391] Loss: 0.065 | Acc: 97.861% (10021/10240)\n",
            "train: [119/391] Loss: 0.061 | Acc: 97.982% (15050/15360)\n",
            "train: [159/391] Loss: 0.061 | Acc: 97.988% (20068/20480)\n",
            "train: [199/391] Loss: 0.060 | Acc: 97.996% (25087/25600)\n",
            "train: [239/391] Loss: 0.059 | Acc: 97.988% (30102/30720)\n",
            "train: [279/391] Loss: 0.060 | Acc: 97.958% (35108/35840)\n",
            "train: [319/391] Loss: 0.060 | Acc: 97.966% (40127/40960)\n",
            "train: [359/391] Loss: 0.060 | Acc: 97.947% (45134/46080)\n",
            "val: [39/79] Loss: 0.507 | Acc: 88.516% (4532/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 245\n",
            "train: [39/391] Loss: 0.050 | Acc: 98.223% (5029/5120)\n",
            "train: [79/391] Loss: 0.051 | Acc: 98.174% (10053/10240)\n",
            "train: [119/391] Loss: 0.053 | Acc: 98.105% (15069/15360)\n",
            "train: [159/391] Loss: 0.052 | Acc: 98.140% (20099/20480)\n",
            "train: [199/391] Loss: 0.051 | Acc: 98.184% (25135/25600)\n",
            "train: [239/391] Loss: 0.050 | Acc: 98.216% (30172/30720)\n",
            "train: [279/391] Loss: 0.049 | Acc: 98.242% (35210/35840)\n",
            "train: [319/391] Loss: 0.050 | Acc: 98.242% (40240/40960)\n",
            "train: [359/391] Loss: 0.050 | Acc: 98.234% (45266/46080)\n",
            "val: [39/79] Loss: 0.511 | Acc: 88.711% (4542/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 246\n",
            "train: [39/391] Loss: 0.049 | Acc: 98.203% (5028/5120)\n",
            "train: [79/391] Loss: 0.047 | Acc: 98.271% (10063/10240)\n",
            "train: [119/391] Loss: 0.043 | Acc: 98.359% (15108/15360)\n",
            "train: [159/391] Loss: 0.043 | Acc: 98.394% (20151/20480)\n",
            "train: [199/391] Loss: 0.043 | Acc: 98.445% (25202/25600)\n",
            "train: [239/391] Loss: 0.042 | Acc: 98.470% (30250/30720)\n",
            "train: [279/391] Loss: 0.042 | Acc: 98.499% (35302/35840)\n",
            "train: [319/391] Loss: 0.041 | Acc: 98.521% (40354/40960)\n",
            "train: [359/391] Loss: 0.041 | Acc: 98.542% (45408/46080)\n",
            "val: [39/79] Loss: 0.515 | Acc: 89.180% (4566/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 247\n",
            "train: [39/391] Loss: 0.034 | Acc: 98.809% (5059/5120)\n",
            "train: [79/391] Loss: 0.036 | Acc: 98.740% (10111/10240)\n",
            "train: [119/391] Loss: 0.035 | Acc: 98.796% (15175/15360)\n",
            "train: [159/391] Loss: 0.034 | Acc: 98.823% (20239/20480)\n",
            "train: [199/391] Loss: 0.034 | Acc: 98.809% (25295/25600)\n",
            "train: [239/391] Loss: 0.033 | Acc: 98.838% (30363/30720)\n",
            "train: [279/391] Loss: 0.033 | Acc: 98.825% (35419/35840)\n",
            "train: [319/391] Loss: 0.034 | Acc: 98.809% (40472/40960)\n",
            "train: [359/391] Loss: 0.034 | Acc: 98.815% (45534/46080)\n",
            "val: [39/79] Loss: 0.515 | Acc: 89.473% (4581/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 248\n",
            "train: [39/391] Loss: 0.030 | Acc: 98.965% (5067/5120)\n",
            "train: [79/391] Loss: 0.031 | Acc: 98.877% (10125/10240)\n",
            "train: [119/391] Loss: 0.031 | Acc: 98.971% (15202/15360)\n",
            "train: [159/391] Loss: 0.030 | Acc: 99.004% (20276/20480)\n",
            "train: [199/391] Loss: 0.031 | Acc: 98.984% (25340/25600)\n",
            "train: [239/391] Loss: 0.031 | Acc: 98.981% (30407/30720)\n",
            "train: [279/391] Loss: 0.030 | Acc: 98.990% (35478/35840)\n",
            "train: [319/391] Loss: 0.030 | Acc: 98.992% (40547/40960)\n",
            "train: [359/391] Loss: 0.030 | Acc: 99.004% (45621/46080)\n",
            "val: [39/79] Loss: 0.514 | Acc: 89.238% (4569/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 249\n",
            "train: [39/391] Loss: 0.025 | Acc: 99.180% (5078/5120)\n",
            "train: [79/391] Loss: 0.026 | Acc: 99.160% (10154/10240)\n",
            "train: [119/391] Loss: 0.027 | Acc: 99.128% (15226/15360)\n",
            "train: [159/391] Loss: 0.028 | Acc: 99.111% (20298/20480)\n",
            "train: [199/391] Loss: 0.027 | Acc: 99.125% (25376/25600)\n",
            "train: [239/391] Loss: 0.027 | Acc: 99.111% (30447/30720)\n",
            "train: [279/391] Loss: 0.027 | Acc: 99.099% (35517/35840)\n",
            "train: [319/391] Loss: 0.027 | Acc: 99.106% (40594/40960)\n",
            "train: [359/391] Loss: 0.027 | Acc: 99.108% (45669/46080)\n",
            "val: [39/79] Loss: 0.511 | Acc: 89.414% (4578/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 250\n",
            "train: [39/391] Loss: 0.033 | Acc: 98.750% (5056/5120)\n",
            "train: [79/391] Loss: 0.050 | Acc: 98.184% (10054/10240)\n",
            "train: [119/391] Loss: 0.062 | Acc: 97.845% (15029/15360)\n",
            "train: [159/391] Loss: 0.065 | Acc: 97.705% (20010/20480)\n",
            "train: [199/391] Loss: 0.070 | Acc: 97.520% (24965/25600)\n",
            "train: [239/391] Loss: 0.073 | Acc: 97.435% (29932/30720)\n",
            "train: [279/391] Loss: 0.076 | Acc: 97.330% (34883/35840)\n",
            "train: [319/391] Loss: 0.079 | Acc: 97.244% (39831/40960)\n",
            "train: [359/391] Loss: 0.081 | Acc: 97.194% (44787/46080)\n",
            "val: [39/79] Loss: 0.504 | Acc: 88.574% (4535/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 251\n",
            "train: [39/391] Loss: 0.082 | Acc: 97.383% (4986/5120)\n",
            "train: [79/391] Loss: 0.087 | Acc: 97.051% (9938/10240)\n",
            "train: [119/391] Loss: 0.081 | Acc: 97.253% (14938/15360)\n",
            "train: [159/391] Loss: 0.082 | Acc: 97.217% (19910/20480)\n",
            "train: [199/391] Loss: 0.085 | Acc: 97.074% (24851/25600)\n",
            "train: [239/391] Loss: 0.085 | Acc: 97.048% (29813/30720)\n",
            "train: [279/391] Loss: 0.085 | Acc: 97.031% (34776/35840)\n",
            "train: [319/391] Loss: 0.085 | Acc: 97.012% (39736/40960)\n",
            "train: [359/391] Loss: 0.085 | Acc: 97.051% (44721/46080)\n",
            "val: [39/79] Loss: 0.474 | Acc: 88.438% (4528/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 252\n",
            "train: [39/391] Loss: 0.071 | Acc: 97.500% (4992/5120)\n",
            "train: [79/391] Loss: 0.071 | Acc: 97.432% (9977/10240)\n",
            "train: [119/391] Loss: 0.073 | Acc: 97.422% (14964/15360)\n",
            "train: [159/391] Loss: 0.073 | Acc: 97.422% (19952/20480)\n",
            "train: [199/391] Loss: 0.073 | Acc: 97.473% (24953/25600)\n",
            "train: [239/391] Loss: 0.073 | Acc: 97.461% (29940/30720)\n",
            "train: [279/391] Loss: 0.073 | Acc: 97.439% (34922/35840)\n",
            "train: [319/391] Loss: 0.074 | Acc: 97.397% (39894/40960)\n",
            "train: [359/391] Loss: 0.076 | Acc: 97.313% (44842/46080)\n",
            "val: [39/79] Loss: 0.465 | Acc: 88.535% (4533/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 253\n",
            "train: [39/391] Loss: 0.067 | Acc: 97.520% (4993/5120)\n",
            "train: [79/391] Loss: 0.064 | Acc: 97.676% (10002/10240)\n",
            "train: [119/391] Loss: 0.063 | Acc: 97.689% (15005/15360)\n",
            "train: [159/391] Loss: 0.064 | Acc: 97.627% (19994/20480)\n",
            "train: [199/391] Loss: 0.065 | Acc: 97.688% (25008/25600)\n",
            "train: [239/391] Loss: 0.065 | Acc: 97.676% (30006/30720)\n",
            "train: [279/391] Loss: 0.065 | Acc: 97.673% (35006/35840)\n",
            "train: [319/391] Loss: 0.066 | Acc: 97.678% (40009/40960)\n",
            "train: [359/391] Loss: 0.066 | Acc: 97.658% (45001/46080)\n",
            "val: [39/79] Loss: 0.505 | Acc: 88.223% (4517/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 254\n",
            "train: [39/391] Loss: 0.059 | Acc: 97.949% (5015/5120)\n",
            "train: [79/391] Loss: 0.059 | Acc: 97.969% (10032/10240)\n",
            "train: [119/391] Loss: 0.056 | Acc: 98.034% (15058/15360)\n",
            "train: [159/391] Loss: 0.056 | Acc: 98.013% (20073/20480)\n",
            "train: [199/391] Loss: 0.055 | Acc: 98.043% (25099/25600)\n",
            "train: [239/391] Loss: 0.055 | Acc: 98.079% (30130/30720)\n",
            "train: [279/391] Loss: 0.054 | Acc: 98.147% (35176/35840)\n",
            "train: [319/391] Loss: 0.055 | Acc: 98.093% (40179/40960)\n",
            "train: [359/391] Loss: 0.054 | Acc: 98.116% (45212/46080)\n",
            "val: [39/79] Loss: 0.494 | Acc: 89.297% (4572/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 255\n",
            "train: [39/391] Loss: 0.051 | Acc: 98.223% (5029/5120)\n",
            "train: [79/391] Loss: 0.049 | Acc: 98.320% (10068/10240)\n",
            "train: [119/391] Loss: 0.048 | Acc: 98.346% (15106/15360)\n",
            "train: [159/391] Loss: 0.047 | Acc: 98.350% (20142/20480)\n",
            "train: [199/391] Loss: 0.047 | Acc: 98.340% (25175/25600)\n",
            "train: [239/391] Loss: 0.047 | Acc: 98.343% (30211/30720)\n",
            "train: [279/391] Loss: 0.047 | Acc: 98.334% (35243/35840)\n",
            "train: [319/391] Loss: 0.047 | Acc: 98.364% (40290/40960)\n",
            "train: [359/391] Loss: 0.046 | Acc: 98.383% (45335/46080)\n",
            "val: [39/79] Loss: 0.506 | Acc: 89.199% (4567/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 256\n",
            "train: [39/391] Loss: 0.045 | Acc: 98.633% (5050/5120)\n",
            "train: [79/391] Loss: 0.041 | Acc: 98.574% (10094/10240)\n",
            "train: [119/391] Loss: 0.038 | Acc: 98.717% (15163/15360)\n",
            "train: [159/391] Loss: 0.037 | Acc: 98.740% (20222/20480)\n",
            "train: [199/391] Loss: 0.038 | Acc: 98.695% (25266/25600)\n",
            "train: [239/391] Loss: 0.038 | Acc: 98.717% (30326/30720)\n",
            "train: [279/391] Loss: 0.038 | Acc: 98.703% (35375/35840)\n",
            "train: [319/391] Loss: 0.037 | Acc: 98.716% (40434/40960)\n",
            "train: [359/391] Loss: 0.037 | Acc: 98.715% (45488/46080)\n",
            "val: [39/79] Loss: 0.497 | Acc: 89.277% (4571/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 257\n",
            "train: [39/391] Loss: 0.029 | Acc: 99.023% (5070/5120)\n",
            "train: [79/391] Loss: 0.028 | Acc: 99.092% (10147/10240)\n",
            "train: [119/391] Loss: 0.029 | Acc: 99.043% (15213/15360)\n",
            "train: [159/391] Loss: 0.028 | Acc: 99.058% (20287/20480)\n",
            "train: [199/391] Loss: 0.029 | Acc: 99.016% (25348/25600)\n",
            "train: [239/391] Loss: 0.030 | Acc: 98.994% (30411/30720)\n",
            "train: [279/391] Loss: 0.030 | Acc: 99.001% (35482/35840)\n",
            "train: [319/391] Loss: 0.029 | Acc: 99.036% (40565/40960)\n",
            "train: [359/391] Loss: 0.030 | Acc: 99.023% (45630/46080)\n",
            "val: [39/79] Loss: 0.500 | Acc: 89.492% (4582/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 258\n",
            "train: [39/391] Loss: 0.028 | Acc: 98.906% (5064/5120)\n",
            "train: [79/391] Loss: 0.027 | Acc: 98.965% (10134/10240)\n",
            "train: [119/391] Loss: 0.028 | Acc: 98.965% (15201/15360)\n",
            "train: [159/391] Loss: 0.029 | Acc: 98.970% (20269/20480)\n",
            "train: [199/391] Loss: 0.029 | Acc: 98.949% (25331/25600)\n",
            "train: [239/391] Loss: 0.029 | Acc: 98.991% (30410/30720)\n",
            "train: [279/391] Loss: 0.029 | Acc: 98.998% (35481/35840)\n",
            "train: [319/391] Loss: 0.029 | Acc: 98.997% (40549/40960)\n",
            "train: [359/391] Loss: 0.029 | Acc: 99.008% (45623/46080)\n",
            "val: [39/79] Loss: 0.507 | Acc: 89.316% (4573/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 259\n",
            "train: [39/391] Loss: 0.027 | Acc: 98.945% (5066/5120)\n",
            "train: [79/391] Loss: 0.025 | Acc: 99.033% (10141/10240)\n",
            "train: [119/391] Loss: 0.025 | Acc: 99.076% (15218/15360)\n",
            "train: [159/391] Loss: 0.025 | Acc: 99.048% (20285/20480)\n",
            "train: [199/391] Loss: 0.025 | Acc: 99.051% (25357/25600)\n",
            "train: [239/391] Loss: 0.025 | Acc: 99.079% (30437/30720)\n",
            "train: [279/391] Loss: 0.025 | Acc: 99.099% (35517/35840)\n",
            "train: [319/391] Loss: 0.025 | Acc: 99.094% (40589/40960)\n",
            "train: [359/391] Loss: 0.025 | Acc: 99.110% (45670/46080)\n",
            "val: [39/79] Loss: 0.509 | Acc: 89.531% (4584/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 260\n",
            "train: [39/391] Loss: 0.037 | Acc: 98.652% (5051/5120)\n",
            "train: [79/391] Loss: 0.052 | Acc: 98.164% (10052/10240)\n",
            "train: [119/391] Loss: 0.066 | Acc: 97.819% (15025/15360)\n",
            "train: [159/391] Loss: 0.071 | Acc: 97.549% (19978/20480)\n",
            "train: [199/391] Loss: 0.072 | Acc: 97.527% (24967/25600)\n",
            "train: [239/391] Loss: 0.076 | Acc: 97.383% (29916/30720)\n",
            "train: [279/391] Loss: 0.078 | Acc: 97.319% (34879/35840)\n",
            "train: [319/391] Loss: 0.081 | Acc: 97.231% (39826/40960)\n",
            "train: [359/391] Loss: 0.081 | Acc: 97.211% (44795/46080)\n",
            "val: [39/79] Loss: 0.478 | Acc: 88.613% (4537/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 261\n",
            "train: [39/391] Loss: 0.083 | Acc: 97.129% (4973/5120)\n",
            "train: [79/391] Loss: 0.080 | Acc: 97.256% (9959/10240)\n",
            "train: [119/391] Loss: 0.083 | Acc: 97.135% (14920/15360)\n",
            "train: [159/391] Loss: 0.086 | Acc: 96.953% (19856/20480)\n",
            "train: [199/391] Loss: 0.086 | Acc: 96.977% (24826/25600)\n",
            "train: [239/391] Loss: 0.085 | Acc: 97.005% (29800/30720)\n",
            "train: [279/391] Loss: 0.085 | Acc: 97.003% (34766/35840)\n",
            "train: [319/391] Loss: 0.084 | Acc: 97.058% (39755/40960)\n",
            "train: [359/391] Loss: 0.083 | Acc: 97.090% (44739/46080)\n",
            "val: [39/79] Loss: 0.519 | Acc: 88.496% (4531/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 262\n",
            "train: [39/391] Loss: 0.077 | Acc: 97.148% (4974/5120)\n",
            "train: [79/391] Loss: 0.073 | Acc: 97.432% (9977/10240)\n",
            "train: [119/391] Loss: 0.072 | Acc: 97.461% (14970/15360)\n",
            "train: [159/391] Loss: 0.072 | Acc: 97.412% (19950/20480)\n",
            "train: [199/391] Loss: 0.071 | Acc: 97.426% (24941/25600)\n",
            "train: [239/391] Loss: 0.071 | Acc: 97.386% (29917/30720)\n",
            "train: [279/391] Loss: 0.071 | Acc: 97.430% (34919/35840)\n",
            "train: [319/391] Loss: 0.070 | Acc: 97.461% (39920/40960)\n",
            "train: [359/391] Loss: 0.070 | Acc: 97.474% (44916/46080)\n",
            "val: [39/79] Loss: 0.492 | Acc: 88.379% (4525/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 263\n",
            "train: [39/391] Loss: 0.055 | Acc: 97.930% (5014/5120)\n",
            "train: [79/391] Loss: 0.055 | Acc: 97.959% (10031/10240)\n",
            "train: [119/391] Loss: 0.058 | Acc: 97.878% (15034/15360)\n",
            "train: [159/391] Loss: 0.058 | Acc: 97.915% (20053/20480)\n",
            "train: [199/391] Loss: 0.058 | Acc: 97.965% (25079/25600)\n",
            "train: [239/391] Loss: 0.059 | Acc: 97.881% (30069/30720)\n",
            "train: [279/391] Loss: 0.059 | Acc: 97.888% (35083/35840)\n",
            "train: [319/391] Loss: 0.060 | Acc: 97.896% (40098/40960)\n",
            "train: [359/391] Loss: 0.060 | Acc: 97.895% (45110/46080)\n",
            "val: [39/79] Loss: 0.486 | Acc: 88.770% (4545/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 264\n",
            "train: [39/391] Loss: 0.056 | Acc: 98.164% (5026/5120)\n",
            "train: [79/391] Loss: 0.052 | Acc: 98.223% (10058/10240)\n",
            "train: [119/391] Loss: 0.052 | Acc: 98.151% (15076/15360)\n",
            "train: [159/391] Loss: 0.053 | Acc: 98.125% (20096/20480)\n",
            "train: [199/391] Loss: 0.053 | Acc: 98.148% (25126/25600)\n",
            "train: [239/391] Loss: 0.054 | Acc: 98.154% (30153/30720)\n",
            "train: [279/391] Loss: 0.054 | Acc: 98.145% (35175/35840)\n",
            "train: [319/391] Loss: 0.054 | Acc: 98.152% (40203/40960)\n",
            "train: [359/391] Loss: 0.054 | Acc: 98.149% (45227/46080)\n",
            "val: [39/79] Loss: 0.499 | Acc: 89.277% (4571/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 265\n",
            "train: [39/391] Loss: 0.046 | Acc: 98.691% (5053/5120)\n",
            "train: [79/391] Loss: 0.044 | Acc: 98.662% (10103/10240)\n",
            "train: [119/391] Loss: 0.044 | Acc: 98.607% (15146/15360)\n",
            "train: [159/391] Loss: 0.044 | Acc: 98.599% (20193/20480)\n",
            "train: [199/391] Loss: 0.045 | Acc: 98.520% (25221/25600)\n",
            "train: [239/391] Loss: 0.045 | Acc: 98.509% (30262/30720)\n",
            "train: [279/391] Loss: 0.044 | Acc: 98.552% (35321/35840)\n",
            "train: [319/391] Loss: 0.044 | Acc: 98.513% (40351/40960)\n",
            "train: [359/391] Loss: 0.044 | Acc: 98.524% (45400/46080)\n",
            "val: [39/79] Loss: 0.503 | Acc: 89.629% (4589/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 266\n",
            "train: [39/391] Loss: 0.041 | Acc: 98.555% (5046/5120)\n",
            "train: [79/391] Loss: 0.038 | Acc: 98.613% (10098/10240)\n",
            "train: [119/391] Loss: 0.039 | Acc: 98.607% (15146/15360)\n",
            "train: [159/391] Loss: 0.040 | Acc: 98.604% (20194/20480)\n",
            "train: [199/391] Loss: 0.040 | Acc: 98.570% (25234/25600)\n",
            "train: [239/391] Loss: 0.039 | Acc: 98.571% (30281/30720)\n",
            "train: [279/391] Loss: 0.039 | Acc: 98.588% (35334/35840)\n",
            "train: [319/391] Loss: 0.039 | Acc: 98.616% (40393/40960)\n",
            "train: [359/391] Loss: 0.039 | Acc: 98.624% (45446/46080)\n",
            "val: [39/79] Loss: 0.509 | Acc: 89.316% (4573/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 267\n",
            "train: [39/391] Loss: 0.030 | Acc: 99.004% (5069/5120)\n",
            "train: [79/391] Loss: 0.029 | Acc: 99.043% (10142/10240)\n",
            "train: [119/391] Loss: 0.029 | Acc: 99.043% (15213/15360)\n",
            "train: [159/391] Loss: 0.029 | Acc: 99.038% (20283/20480)\n",
            "train: [199/391] Loss: 0.029 | Acc: 98.992% (25342/25600)\n",
            "train: [239/391] Loss: 0.029 | Acc: 98.978% (30406/30720)\n",
            "train: [279/391] Loss: 0.029 | Acc: 98.979% (35474/35840)\n",
            "train: [319/391] Loss: 0.030 | Acc: 98.970% (40538/40960)\n",
            "train: [359/391] Loss: 0.030 | Acc: 98.961% (45601/46080)\n",
            "val: [39/79] Loss: 0.507 | Acc: 89.414% (4578/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 268\n",
            "train: [39/391] Loss: 0.031 | Acc: 98.945% (5066/5120)\n",
            "train: [79/391] Loss: 0.033 | Acc: 98.877% (10125/10240)\n",
            "train: [119/391] Loss: 0.032 | Acc: 98.906% (15192/15360)\n",
            "train: [159/391] Loss: 0.031 | Acc: 98.960% (20267/20480)\n",
            "train: [199/391] Loss: 0.030 | Acc: 99.004% (25345/25600)\n",
            "train: [239/391] Loss: 0.029 | Acc: 99.020% (30419/30720)\n",
            "train: [279/391] Loss: 0.028 | Acc: 99.060% (35503/35840)\n",
            "train: [319/391] Loss: 0.028 | Acc: 99.077% (40582/40960)\n",
            "train: [359/391] Loss: 0.027 | Acc: 99.076% (45654/46080)\n",
            "val: [39/79] Loss: 0.515 | Acc: 89.297% (4572/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 269\n",
            "train: [39/391] Loss: 0.022 | Acc: 99.297% (5084/5120)\n",
            "train: [79/391] Loss: 0.024 | Acc: 99.238% (10162/10240)\n",
            "train: [119/391] Loss: 0.024 | Acc: 99.186% (15235/15360)\n",
            "train: [159/391] Loss: 0.024 | Acc: 99.233% (20323/20480)\n",
            "train: [199/391] Loss: 0.023 | Acc: 99.270% (25413/25600)\n",
            "train: [239/391] Loss: 0.023 | Acc: 99.255% (30491/30720)\n",
            "train: [279/391] Loss: 0.023 | Acc: 99.258% (35574/35840)\n",
            "train: [319/391] Loss: 0.023 | Acc: 99.243% (40650/40960)\n",
            "train: [359/391] Loss: 0.023 | Acc: 99.223% (45722/46080)\n",
            "val: [39/79] Loss: 0.513 | Acc: 89.473% (4581/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 270\n",
            "train: [39/391] Loss: 0.033 | Acc: 98.926% (5065/5120)\n",
            "train: [79/391] Loss: 0.048 | Acc: 98.320% (10068/10240)\n",
            "train: [119/391] Loss: 0.055 | Acc: 98.034% (15058/15360)\n",
            "train: [159/391] Loss: 0.064 | Acc: 97.739% (20017/20480)\n",
            "train: [199/391] Loss: 0.068 | Acc: 97.613% (24989/25600)\n",
            "train: [239/391] Loss: 0.072 | Acc: 97.493% (29950/30720)\n",
            "train: [279/391] Loss: 0.075 | Acc: 97.372% (34898/35840)\n",
            "train: [319/391] Loss: 0.074 | Acc: 97.383% (39888/40960)\n",
            "train: [359/391] Loss: 0.075 | Acc: 97.346% (44857/46080)\n",
            "val: [39/79] Loss: 0.530 | Acc: 87.773% (4494/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 271\n",
            "train: [39/391] Loss: 0.085 | Acc: 97.188% (4976/5120)\n",
            "train: [79/391] Loss: 0.085 | Acc: 97.100% (9943/10240)\n",
            "train: [119/391] Loss: 0.081 | Acc: 97.181% (14927/15360)\n",
            "train: [159/391] Loss: 0.079 | Acc: 97.256% (19918/20480)\n",
            "train: [199/391] Loss: 0.076 | Acc: 97.363% (24925/25600)\n",
            "train: [239/391] Loss: 0.076 | Acc: 97.363% (29910/30720)\n",
            "train: [279/391] Loss: 0.077 | Acc: 97.316% (34878/35840)\n",
            "train: [319/391] Loss: 0.078 | Acc: 97.283% (39847/40960)\n",
            "train: [359/391] Loss: 0.080 | Acc: 97.198% (44789/46080)\n",
            "val: [39/79] Loss: 0.468 | Acc: 88.730% (4543/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 272\n",
            "train: [39/391] Loss: 0.065 | Acc: 97.695% (5002/5120)\n",
            "train: [79/391] Loss: 0.067 | Acc: 97.656% (10000/10240)\n",
            "train: [119/391] Loss: 0.069 | Acc: 97.643% (14998/15360)\n",
            "train: [159/391] Loss: 0.067 | Acc: 97.729% (20015/20480)\n",
            "train: [199/391] Loss: 0.067 | Acc: 97.703% (25012/25600)\n",
            "train: [239/391] Loss: 0.068 | Acc: 97.718% (30019/30720)\n",
            "train: [279/391] Loss: 0.068 | Acc: 97.665% (35003/35840)\n",
            "train: [319/391] Loss: 0.069 | Acc: 97.607% (39980/40960)\n",
            "train: [359/391] Loss: 0.068 | Acc: 97.667% (45005/46080)\n",
            "val: [39/79] Loss: 0.514 | Acc: 88.516% (4532/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 273\n",
            "train: [39/391] Loss: 0.064 | Acc: 97.559% (4995/5120)\n",
            "train: [79/391] Loss: 0.057 | Acc: 97.852% (10020/10240)\n",
            "train: [119/391] Loss: 0.056 | Acc: 97.930% (15042/15360)\n",
            "train: [159/391] Loss: 0.058 | Acc: 97.856% (20041/20480)\n",
            "train: [199/391] Loss: 0.057 | Acc: 97.879% (25057/25600)\n",
            "train: [239/391] Loss: 0.056 | Acc: 97.965% (30095/30720)\n",
            "train: [279/391] Loss: 0.056 | Acc: 97.946% (35104/35840)\n",
            "train: [319/391] Loss: 0.056 | Acc: 97.959% (40124/40960)\n",
            "train: [359/391] Loss: 0.057 | Acc: 97.949% (45135/46080)\n",
            "val: [39/79] Loss: 0.507 | Acc: 88.555% (4534/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 274\n",
            "train: [39/391] Loss: 0.055 | Acc: 98.066% (5021/5120)\n",
            "train: [79/391] Loss: 0.052 | Acc: 98.223% (10058/10240)\n",
            "train: [119/391] Loss: 0.051 | Acc: 98.262% (15093/15360)\n",
            "train: [159/391] Loss: 0.049 | Acc: 98.335% (20139/20480)\n",
            "train: [199/391] Loss: 0.048 | Acc: 98.359% (25180/25600)\n",
            "train: [239/391] Loss: 0.047 | Acc: 98.408% (30231/30720)\n",
            "train: [279/391] Loss: 0.047 | Acc: 98.382% (35260/35840)\n",
            "train: [319/391] Loss: 0.047 | Acc: 98.406% (40307/40960)\n",
            "train: [359/391] Loss: 0.049 | Acc: 98.336% (45313/46080)\n",
            "val: [39/79] Loss: 0.504 | Acc: 89.121% (4563/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 275\n",
            "train: [39/391] Loss: 0.040 | Acc: 98.438% (5040/5120)\n",
            "train: [79/391] Loss: 0.041 | Acc: 98.545% (10091/10240)\n",
            "train: [119/391] Loss: 0.038 | Acc: 98.574% (15141/15360)\n",
            "train: [159/391] Loss: 0.038 | Acc: 98.599% (20193/20480)\n",
            "train: [199/391] Loss: 0.039 | Acc: 98.555% (25230/25600)\n",
            "train: [239/391] Loss: 0.039 | Acc: 98.555% (30276/30720)\n",
            "train: [279/391] Loss: 0.041 | Acc: 98.541% (35317/35840)\n",
            "train: [319/391] Loss: 0.040 | Acc: 98.550% (40366/40960)\n",
            "train: [359/391] Loss: 0.041 | Acc: 98.559% (45416/46080)\n",
            "val: [39/79] Loss: 0.519 | Acc: 89.102% (4562/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 276\n",
            "train: [39/391] Loss: 0.043 | Acc: 98.672% (5052/5120)\n",
            "train: [79/391] Loss: 0.041 | Acc: 98.613% (10098/10240)\n",
            "train: [119/391] Loss: 0.039 | Acc: 98.672% (15156/15360)\n",
            "train: [159/391] Loss: 0.037 | Acc: 98.730% (20220/20480)\n",
            "train: [199/391] Loss: 0.037 | Acc: 98.711% (25270/25600)\n",
            "train: [239/391] Loss: 0.037 | Acc: 98.717% (30326/30720)\n",
            "train: [279/391] Loss: 0.036 | Acc: 98.750% (35392/35840)\n",
            "train: [319/391] Loss: 0.037 | Acc: 98.750% (40448/40960)\n",
            "train: [359/391] Loss: 0.036 | Acc: 98.767% (45512/46080)\n",
            "val: [39/79] Loss: 0.514 | Acc: 89.277% (4571/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 277\n",
            "train: [39/391] Loss: 0.032 | Acc: 98.828% (5060/5120)\n",
            "train: [79/391] Loss: 0.030 | Acc: 98.945% (10132/10240)\n",
            "train: [119/391] Loss: 0.029 | Acc: 99.062% (15216/15360)\n",
            "train: [159/391] Loss: 0.028 | Acc: 99.043% (20284/20480)\n",
            "train: [199/391] Loss: 0.029 | Acc: 99.008% (25346/25600)\n",
            "train: [239/391] Loss: 0.028 | Acc: 99.033% (30423/30720)\n",
            "train: [279/391] Loss: 0.027 | Acc: 99.043% (35497/35840)\n",
            "train: [319/391] Loss: 0.027 | Acc: 99.033% (40564/40960)\n",
            "train: [359/391] Loss: 0.027 | Acc: 99.039% (45637/46080)\n",
            "val: [39/79] Loss: 0.530 | Acc: 89.375% (4576/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 278\n",
            "train: [39/391] Loss: 0.023 | Acc: 99.316% (5085/5120)\n",
            "train: [79/391] Loss: 0.027 | Acc: 99.160% (10154/10240)\n",
            "train: [119/391] Loss: 0.026 | Acc: 99.154% (15230/15360)\n",
            "train: [159/391] Loss: 0.025 | Acc: 99.155% (20307/20480)\n",
            "train: [199/391] Loss: 0.024 | Acc: 99.223% (25401/25600)\n",
            "train: [239/391] Loss: 0.024 | Acc: 99.189% (30471/30720)\n",
            "train: [279/391] Loss: 0.024 | Acc: 99.185% (35548/35840)\n",
            "train: [319/391] Loss: 0.025 | Acc: 99.158% (40615/40960)\n",
            "train: [359/391] Loss: 0.025 | Acc: 99.151% (45689/46080)\n",
            "val: [39/79] Loss: 0.526 | Acc: 89.316% (4573/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 279\n",
            "train: [39/391] Loss: 0.020 | Acc: 99.297% (5084/5120)\n",
            "train: [79/391] Loss: 0.019 | Acc: 99.307% (10169/10240)\n",
            "train: [119/391] Loss: 0.019 | Acc: 99.342% (15259/15360)\n",
            "train: [159/391] Loss: 0.020 | Acc: 99.302% (20337/20480)\n",
            "train: [199/391] Loss: 0.021 | Acc: 99.273% (25414/25600)\n",
            "train: [239/391] Loss: 0.020 | Acc: 99.268% (30495/30720)\n",
            "train: [279/391] Loss: 0.020 | Acc: 99.286% (35584/35840)\n",
            "train: [319/391] Loss: 0.021 | Acc: 99.221% (40641/40960)\n",
            "train: [359/391] Loss: 0.022 | Acc: 99.204% (45713/46080)\n",
            "val: [39/79] Loss: 0.528 | Acc: 89.570% (4586/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 280\n",
            "train: [39/391] Loss: 0.030 | Acc: 98.965% (5067/5120)\n",
            "train: [79/391] Loss: 0.047 | Acc: 98.447% (10081/10240)\n",
            "train: [119/391] Loss: 0.056 | Acc: 98.171% (15079/15360)\n",
            "train: [159/391] Loss: 0.062 | Acc: 97.974% (20065/20480)\n",
            "train: [199/391] Loss: 0.069 | Acc: 97.680% (25006/25600)\n",
            "train: [239/391] Loss: 0.074 | Acc: 97.493% (29950/30720)\n",
            "train: [279/391] Loss: 0.074 | Acc: 97.472% (34934/35840)\n",
            "train: [319/391] Loss: 0.076 | Acc: 97.393% (39892/40960)\n",
            "train: [359/391] Loss: 0.075 | Acc: 97.387% (44876/46080)\n",
            "val: [39/79] Loss: 0.509 | Acc: 88.125% (4512/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 281\n",
            "train: [39/391] Loss: 0.072 | Acc: 97.598% (4997/5120)\n",
            "train: [79/391] Loss: 0.070 | Acc: 97.686% (10003/10240)\n",
            "train: [119/391] Loss: 0.066 | Acc: 97.806% (15023/15360)\n",
            "train: [159/391] Loss: 0.066 | Acc: 97.783% (20026/20480)\n",
            "train: [199/391] Loss: 0.067 | Acc: 97.723% (25017/25600)\n",
            "train: [239/391] Loss: 0.069 | Acc: 97.607% (29985/30720)\n",
            "train: [279/391] Loss: 0.072 | Acc: 97.503% (34945/35840)\n",
            "train: [319/391] Loss: 0.074 | Acc: 97.434% (39909/40960)\n",
            "train: [359/391] Loss: 0.075 | Acc: 97.396% (44880/46080)\n",
            "val: [39/79] Loss: 0.494 | Acc: 88.438% (4528/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 282\n",
            "train: [39/391] Loss: 0.062 | Acc: 97.832% (5009/5120)\n",
            "train: [79/391] Loss: 0.063 | Acc: 97.861% (10021/10240)\n",
            "train: [119/391] Loss: 0.063 | Acc: 97.767% (15017/15360)\n",
            "train: [159/391] Loss: 0.063 | Acc: 97.764% (20022/20480)\n",
            "train: [199/391] Loss: 0.063 | Acc: 97.770% (25029/25600)\n",
            "train: [239/391] Loss: 0.063 | Acc: 97.702% (30014/30720)\n",
            "train: [279/391] Loss: 0.063 | Acc: 97.673% (35006/35840)\n",
            "train: [319/391] Loss: 0.065 | Acc: 97.639% (39993/40960)\n",
            "train: [359/391] Loss: 0.066 | Acc: 97.611% (44979/46080)\n",
            "val: [39/79] Loss: 0.480 | Acc: 89.004% (4557/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 283\n",
            "train: [39/391] Loss: 0.060 | Acc: 97.949% (5015/5120)\n",
            "train: [79/391] Loss: 0.055 | Acc: 98.154% (10051/10240)\n",
            "train: [119/391] Loss: 0.055 | Acc: 98.079% (15065/15360)\n",
            "train: [159/391] Loss: 0.054 | Acc: 98.135% (20098/20480)\n",
            "train: [199/391] Loss: 0.054 | Acc: 98.109% (25116/25600)\n",
            "train: [239/391] Loss: 0.055 | Acc: 98.073% (30128/30720)\n",
            "train: [279/391] Loss: 0.056 | Acc: 98.025% (35132/35840)\n",
            "train: [319/391] Loss: 0.056 | Acc: 98.044% (40159/40960)\n",
            "train: [359/391] Loss: 0.056 | Acc: 98.036% (45175/46080)\n",
            "val: [39/79] Loss: 0.477 | Acc: 89.102% (4562/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 284\n",
            "train: [39/391] Loss: 0.059 | Acc: 98.047% (5020/5120)\n",
            "train: [79/391] Loss: 0.056 | Acc: 98.057% (10041/10240)\n",
            "train: [119/391] Loss: 0.053 | Acc: 98.184% (15081/15360)\n",
            "train: [159/391] Loss: 0.050 | Acc: 98.296% (20131/20480)\n",
            "train: [199/391] Loss: 0.049 | Acc: 98.297% (25164/25600)\n",
            "train: [239/391] Loss: 0.049 | Acc: 98.285% (30193/30720)\n",
            "train: [279/391] Loss: 0.049 | Acc: 98.278% (35223/35840)\n",
            "train: [319/391] Loss: 0.049 | Acc: 98.289% (40259/40960)\n",
            "train: [359/391] Loss: 0.049 | Acc: 98.288% (45291/46080)\n",
            "val: [39/79] Loss: 0.471 | Acc: 89.961% (4606/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 285\n",
            "train: [39/391] Loss: 0.038 | Acc: 98.555% (5046/5120)\n",
            "train: [79/391] Loss: 0.037 | Acc: 98.652% (10102/10240)\n",
            "train: [119/391] Loss: 0.037 | Acc: 98.646% (15152/15360)\n",
            "train: [159/391] Loss: 0.039 | Acc: 98.545% (20182/20480)\n",
            "train: [199/391] Loss: 0.039 | Acc: 98.547% (25228/25600)\n",
            "train: [239/391] Loss: 0.039 | Acc: 98.574% (30282/30720)\n",
            "train: [279/391] Loss: 0.040 | Acc: 98.544% (35318/35840)\n",
            "train: [319/391] Loss: 0.039 | Acc: 98.562% (40371/40960)\n",
            "train: [359/391] Loss: 0.039 | Acc: 98.585% (45428/46080)\n",
            "val: [39/79] Loss: 0.511 | Acc: 89.238% (4569/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 286\n",
            "train: [39/391] Loss: 0.032 | Acc: 98.965% (5067/5120)\n",
            "train: [79/391] Loss: 0.032 | Acc: 98.936% (10131/10240)\n",
            "train: [119/391] Loss: 0.034 | Acc: 98.841% (15182/15360)\n",
            "train: [159/391] Loss: 0.032 | Acc: 98.896% (20254/20480)\n",
            "train: [199/391] Loss: 0.033 | Acc: 98.852% (25306/25600)\n",
            "train: [239/391] Loss: 0.032 | Acc: 98.877% (30375/30720)\n",
            "train: [279/391] Loss: 0.032 | Acc: 98.876% (35437/35840)\n",
            "train: [319/391] Loss: 0.033 | Acc: 98.872% (40498/40960)\n",
            "train: [359/391] Loss: 0.033 | Acc: 98.865% (45557/46080)\n",
            "val: [39/79] Loss: 0.509 | Acc: 89.414% (4578/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 287\n",
            "train: [39/391] Loss: 0.030 | Acc: 99.062% (5072/5120)\n",
            "train: [79/391] Loss: 0.027 | Acc: 99.131% (10151/10240)\n",
            "train: [119/391] Loss: 0.026 | Acc: 99.121% (15225/15360)\n",
            "train: [159/391] Loss: 0.026 | Acc: 99.111% (20298/20480)\n",
            "train: [199/391] Loss: 0.026 | Acc: 99.137% (25379/25600)\n",
            "train: [239/391] Loss: 0.027 | Acc: 99.108% (30446/30720)\n",
            "train: [279/391] Loss: 0.026 | Acc: 99.124% (35526/35840)\n",
            "train: [319/391] Loss: 0.026 | Acc: 99.111% (40596/40960)\n",
            "train: [359/391] Loss: 0.026 | Acc: 99.110% (45670/46080)\n",
            "val: [39/79] Loss: 0.514 | Acc: 89.629% (4589/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 288\n",
            "train: [39/391] Loss: 0.026 | Acc: 99.043% (5071/5120)\n",
            "train: [79/391] Loss: 0.025 | Acc: 99.102% (10148/10240)\n",
            "train: [119/391] Loss: 0.024 | Acc: 99.134% (15227/15360)\n",
            "train: [159/391] Loss: 0.023 | Acc: 99.194% (20315/20480)\n",
            "train: [199/391] Loss: 0.023 | Acc: 99.195% (25394/25600)\n",
            "train: [239/391] Loss: 0.024 | Acc: 99.183% (30469/30720)\n",
            "train: [279/391] Loss: 0.024 | Acc: 99.177% (35545/35840)\n",
            "train: [319/391] Loss: 0.023 | Acc: 99.192% (40629/40960)\n",
            "train: [359/391] Loss: 0.023 | Acc: 99.201% (45712/46080)\n",
            "val: [39/79] Loss: 0.520 | Acc: 89.707% (4593/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 289\n",
            "train: [39/391] Loss: 0.021 | Acc: 99.434% (5091/5120)\n",
            "train: [79/391] Loss: 0.022 | Acc: 99.326% (10171/10240)\n",
            "train: [119/391] Loss: 0.021 | Acc: 99.336% (15258/15360)\n",
            "train: [159/391] Loss: 0.024 | Acc: 99.233% (20323/20480)\n",
            "train: [199/391] Loss: 0.023 | Acc: 99.246% (25407/25600)\n",
            "train: [239/391] Loss: 0.023 | Acc: 99.209% (30477/30720)\n",
            "train: [279/391] Loss: 0.023 | Acc: 99.230% (35564/35840)\n",
            "train: [319/391] Loss: 0.023 | Acc: 99.209% (40636/40960)\n",
            "train: [359/391] Loss: 0.023 | Acc: 99.214% (45718/46080)\n",
            "val: [39/79] Loss: 0.524 | Acc: 89.805% (4598/5120)\n",
            "Learing rate:  0.001\n",
            "best accurary 90.370000%\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [128, 32, 32, 32]             864\n",
            "       BatchNorm2d-2          [128, 32, 32, 32]              64\n",
            "           Sigmoid-3          [128, 32, 32, 32]               0\n",
            "             Swish-4          [128, 32, 32, 32]               0\n",
            "            Conv2d-5          [128, 32, 32, 32]           1,024\n",
            "       BatchNorm2d-6          [128, 32, 32, 32]              64\n",
            "           Sigmoid-7          [128, 32, 32, 32]               0\n",
            "             Swish-8          [128, 32, 32, 32]               0\n",
            "            Conv2d-9          [128, 32, 16, 16]             288\n",
            "      BatchNorm2d-10          [128, 32, 16, 16]              64\n",
            "          Sigmoid-11          [128, 32, 16, 16]               0\n",
            "            Swish-12          [128, 32, 16, 16]               0\n",
            "           Conv2d-13          [128, 16, 16, 16]             512\n",
            "      BatchNorm2d-14          [128, 16, 16, 16]              32\n",
            "           Conv2d-15             [128, 1, 1, 1]              17\n",
            "          Sigmoid-16             [128, 1, 1, 1]               0\n",
            "            Swish-17             [128, 1, 1, 1]               0\n",
            "           Conv2d-18            [128, 16, 1, 1]              32\n",
            "            Block-19          [128, 16, 16, 16]               0\n",
            "           Conv2d-20          [128, 96, 16, 16]           1,536\n",
            "      BatchNorm2d-21          [128, 96, 16, 16]             192\n",
            "          Sigmoid-22          [128, 96, 16, 16]               0\n",
            "            Swish-23          [128, 96, 16, 16]               0\n",
            "           Conv2d-24          [128, 96, 16, 16]             864\n",
            "      BatchNorm2d-25          [128, 96, 16, 16]             192\n",
            "          Sigmoid-26          [128, 96, 16, 16]               0\n",
            "            Swish-27          [128, 96, 16, 16]               0\n",
            "           Conv2d-28          [128, 24, 16, 16]           2,304\n",
            "      BatchNorm2d-29          [128, 24, 16, 16]              48\n",
            "           Conv2d-30          [128, 24, 16, 16]             384\n",
            "      BatchNorm2d-31          [128, 24, 16, 16]              48\n",
            "           Conv2d-32             [128, 1, 1, 1]              25\n",
            "          Sigmoid-33             [128, 1, 1, 1]               0\n",
            "            Swish-34             [128, 1, 1, 1]               0\n",
            "           Conv2d-35            [128, 24, 1, 1]              48\n",
            "            Block-36          [128, 24, 16, 16]               0\n",
            "           Conv2d-37         [128, 144, 16, 16]           3,456\n",
            "      BatchNorm2d-38         [128, 144, 16, 16]             288\n",
            "          Sigmoid-39         [128, 144, 16, 16]               0\n",
            "            Swish-40         [128, 144, 16, 16]               0\n",
            "           Conv2d-41         [128, 144, 16, 16]           1,296\n",
            "      BatchNorm2d-42         [128, 144, 16, 16]             288\n",
            "          Sigmoid-43         [128, 144, 16, 16]               0\n",
            "            Swish-44         [128, 144, 16, 16]               0\n",
            "           Conv2d-45          [128, 24, 16, 16]           3,456\n",
            "      BatchNorm2d-46          [128, 24, 16, 16]              48\n",
            "           Conv2d-47             [128, 1, 1, 1]              25\n",
            "          Sigmoid-48             [128, 1, 1, 1]               0\n",
            "            Swish-49             [128, 1, 1, 1]               0\n",
            "           Conv2d-50            [128, 24, 1, 1]              48\n",
            "            Block-51          [128, 24, 16, 16]               0\n",
            "           Conv2d-52         [128, 144, 16, 16]           3,456\n",
            "      BatchNorm2d-53         [128, 144, 16, 16]             288\n",
            "          Sigmoid-54         [128, 144, 16, 16]               0\n",
            "            Swish-55         [128, 144, 16, 16]               0\n",
            "           Conv2d-56           [128, 144, 8, 8]           1,296\n",
            "      BatchNorm2d-57           [128, 144, 8, 8]             288\n",
            "          Sigmoid-58           [128, 144, 8, 8]               0\n",
            "            Swish-59           [128, 144, 8, 8]               0\n",
            "           Conv2d-60            [128, 40, 8, 8]           5,760\n",
            "      BatchNorm2d-61            [128, 40, 8, 8]              80\n",
            "           Conv2d-62             [128, 2, 1, 1]              82\n",
            "          Sigmoid-63             [128, 2, 1, 1]               0\n",
            "            Swish-64             [128, 2, 1, 1]               0\n",
            "           Conv2d-65            [128, 40, 1, 1]             120\n",
            "            Block-66            [128, 40, 8, 8]               0\n",
            "           Conv2d-67           [128, 240, 8, 8]           9,600\n",
            "      BatchNorm2d-68           [128, 240, 8, 8]             480\n",
            "          Sigmoid-69           [128, 240, 8, 8]               0\n",
            "            Swish-70           [128, 240, 8, 8]               0\n",
            "           Conv2d-71           [128, 240, 8, 8]           2,160\n",
            "      BatchNorm2d-72           [128, 240, 8, 8]             480\n",
            "          Sigmoid-73           [128, 240, 8, 8]               0\n",
            "            Swish-74           [128, 240, 8, 8]               0\n",
            "           Conv2d-75            [128, 40, 8, 8]           9,600\n",
            "      BatchNorm2d-76            [128, 40, 8, 8]              80\n",
            "           Conv2d-77             [128, 2, 1, 1]              82\n",
            "          Sigmoid-78             [128, 2, 1, 1]               0\n",
            "            Swish-79             [128, 2, 1, 1]               0\n",
            "           Conv2d-80            [128, 40, 1, 1]             120\n",
            "            Block-81            [128, 40, 8, 8]               0\n",
            "           Conv2d-82           [128, 240, 8, 8]           9,600\n",
            "      BatchNorm2d-83           [128, 240, 8, 8]             480\n",
            "          Sigmoid-84           [128, 240, 8, 8]               0\n",
            "            Swish-85           [128, 240, 8, 8]               0\n",
            "           Conv2d-86           [128, 240, 4, 4]           2,160\n",
            "      BatchNorm2d-87           [128, 240, 4, 4]             480\n",
            "          Sigmoid-88           [128, 240, 4, 4]               0\n",
            "            Swish-89           [128, 240, 4, 4]               0\n",
            "           Conv2d-90            [128, 80, 4, 4]          19,200\n",
            "      BatchNorm2d-91            [128, 80, 4, 4]             160\n",
            "           Conv2d-92             [128, 5, 1, 1]             405\n",
            "          Sigmoid-93             [128, 5, 1, 1]               0\n",
            "            Swish-94             [128, 5, 1, 1]               0\n",
            "           Conv2d-95            [128, 80, 1, 1]             480\n",
            "            Block-96            [128, 80, 4, 4]               0\n",
            "           Conv2d-97           [128, 480, 4, 4]          38,400\n",
            "      BatchNorm2d-98           [128, 480, 4, 4]             960\n",
            "          Sigmoid-99           [128, 480, 4, 4]               0\n",
            "           Swish-100           [128, 480, 4, 4]               0\n",
            "          Conv2d-101           [128, 480, 4, 4]           4,320\n",
            "     BatchNorm2d-102           [128, 480, 4, 4]             960\n",
            "         Sigmoid-103           [128, 480, 4, 4]               0\n",
            "           Swish-104           [128, 480, 4, 4]               0\n",
            "          Conv2d-105            [128, 80, 4, 4]          38,400\n",
            "     BatchNorm2d-106            [128, 80, 4, 4]             160\n",
            "          Conv2d-107             [128, 5, 1, 1]             405\n",
            "         Sigmoid-108             [128, 5, 1, 1]               0\n",
            "           Swish-109             [128, 5, 1, 1]               0\n",
            "          Conv2d-110            [128, 80, 1, 1]             480\n",
            "           Block-111            [128, 80, 4, 4]               0\n",
            "          Conv2d-112           [128, 480, 4, 4]          38,400\n",
            "     BatchNorm2d-113           [128, 480, 4, 4]             960\n",
            "         Sigmoid-114           [128, 480, 4, 4]               0\n",
            "           Swish-115           [128, 480, 4, 4]               0\n",
            "          Conv2d-116           [128, 480, 4, 4]           4,320\n",
            "     BatchNorm2d-117           [128, 480, 4, 4]             960\n",
            "         Sigmoid-118           [128, 480, 4, 4]               0\n",
            "           Swish-119           [128, 480, 4, 4]               0\n",
            "          Conv2d-120            [128, 80, 4, 4]          38,400\n",
            "     BatchNorm2d-121            [128, 80, 4, 4]             160\n",
            "          Conv2d-122             [128, 5, 1, 1]             405\n",
            "         Sigmoid-123             [128, 5, 1, 1]               0\n",
            "           Swish-124             [128, 5, 1, 1]               0\n",
            "          Conv2d-125            [128, 80, 1, 1]             480\n",
            "           Block-126            [128, 80, 4, 4]               0\n",
            "          Conv2d-127           [128, 480, 4, 4]          38,400\n",
            "     BatchNorm2d-128           [128, 480, 4, 4]             960\n",
            "         Sigmoid-129           [128, 480, 4, 4]               0\n",
            "           Swish-130           [128, 480, 4, 4]               0\n",
            "          Conv2d-131           [128, 480, 4, 4]           4,320\n",
            "     BatchNorm2d-132           [128, 480, 4, 4]             960\n",
            "         Sigmoid-133           [128, 480, 4, 4]               0\n",
            "           Swish-134           [128, 480, 4, 4]               0\n",
            "          Conv2d-135           [128, 112, 4, 4]          53,760\n",
            "     BatchNorm2d-136           [128, 112, 4, 4]             224\n",
            "          Conv2d-137           [128, 112, 4, 4]           8,960\n",
            "     BatchNorm2d-138           [128, 112, 4, 4]             224\n",
            "          Conv2d-139             [128, 7, 1, 1]             791\n",
            "         Sigmoid-140             [128, 7, 1, 1]               0\n",
            "           Swish-141             [128, 7, 1, 1]               0\n",
            "          Conv2d-142           [128, 112, 1, 1]             896\n",
            "           Block-143           [128, 112, 4, 4]               0\n",
            "          Conv2d-144           [128, 672, 4, 4]          75,264\n",
            "     BatchNorm2d-145           [128, 672, 4, 4]           1,344\n",
            "         Sigmoid-146           [128, 672, 4, 4]               0\n",
            "           Swish-147           [128, 672, 4, 4]               0\n",
            "          Conv2d-148           [128, 672, 4, 4]           6,048\n",
            "     BatchNorm2d-149           [128, 672, 4, 4]           1,344\n",
            "         Sigmoid-150           [128, 672, 4, 4]               0\n",
            "           Swish-151           [128, 672, 4, 4]               0\n",
            "          Conv2d-152           [128, 112, 4, 4]          75,264\n",
            "     BatchNorm2d-153           [128, 112, 4, 4]             224\n",
            "          Conv2d-154             [128, 7, 1, 1]             791\n",
            "         Sigmoid-155             [128, 7, 1, 1]               0\n",
            "           Swish-156             [128, 7, 1, 1]               0\n",
            "          Conv2d-157           [128, 112, 1, 1]             896\n",
            "           Block-158           [128, 112, 4, 4]               0\n",
            "          Conv2d-159           [128, 672, 4, 4]          75,264\n",
            "     BatchNorm2d-160           [128, 672, 4, 4]           1,344\n",
            "         Sigmoid-161           [128, 672, 4, 4]               0\n",
            "           Swish-162           [128, 672, 4, 4]               0\n",
            "          Conv2d-163           [128, 672, 4, 4]           6,048\n",
            "     BatchNorm2d-164           [128, 672, 4, 4]           1,344\n",
            "         Sigmoid-165           [128, 672, 4, 4]               0\n",
            "           Swish-166           [128, 672, 4, 4]               0\n",
            "          Conv2d-167           [128, 112, 4, 4]          75,264\n",
            "     BatchNorm2d-168           [128, 112, 4, 4]             224\n",
            "          Conv2d-169             [128, 7, 1, 1]             791\n",
            "         Sigmoid-170             [128, 7, 1, 1]               0\n",
            "           Swish-171             [128, 7, 1, 1]               0\n",
            "          Conv2d-172           [128, 112, 1, 1]             896\n",
            "           Block-173           [128, 112, 4, 4]               0\n",
            "          Conv2d-174           [128, 672, 4, 4]          75,264\n",
            "     BatchNorm2d-175           [128, 672, 4, 4]           1,344\n",
            "         Sigmoid-176           [128, 672, 4, 4]               0\n",
            "           Swish-177           [128, 672, 4, 4]               0\n",
            "          Conv2d-178           [128, 672, 2, 2]           6,048\n",
            "     BatchNorm2d-179           [128, 672, 2, 2]           1,344\n",
            "         Sigmoid-180           [128, 672, 2, 2]               0\n",
            "           Swish-181           [128, 672, 2, 2]               0\n",
            "          Conv2d-182           [128, 192, 2, 2]         129,024\n",
            "     BatchNorm2d-183           [128, 192, 2, 2]             384\n",
            "          Conv2d-184            [128, 12, 1, 1]           2,316\n",
            "         Sigmoid-185            [128, 12, 1, 1]               0\n",
            "           Swish-186            [128, 12, 1, 1]               0\n",
            "          Conv2d-187           [128, 192, 1, 1]           2,496\n",
            "           Block-188           [128, 192, 2, 2]               0\n",
            "          Conv2d-189          [128, 1152, 2, 2]         221,184\n",
            "     BatchNorm2d-190          [128, 1152, 2, 2]           2,304\n",
            "         Sigmoid-191          [128, 1152, 2, 2]               0\n",
            "           Swish-192          [128, 1152, 2, 2]               0\n",
            "          Conv2d-193          [128, 1152, 2, 2]          10,368\n",
            "     BatchNorm2d-194          [128, 1152, 2, 2]           2,304\n",
            "         Sigmoid-195          [128, 1152, 2, 2]               0\n",
            "           Swish-196          [128, 1152, 2, 2]               0\n",
            "          Conv2d-197           [128, 192, 2, 2]         221,184\n",
            "     BatchNorm2d-198           [128, 192, 2, 2]             384\n",
            "          Conv2d-199            [128, 12, 1, 1]           2,316\n",
            "         Sigmoid-200            [128, 12, 1, 1]               0\n",
            "           Swish-201            [128, 12, 1, 1]               0\n",
            "          Conv2d-202           [128, 192, 1, 1]           2,496\n",
            "           Block-203           [128, 192, 2, 2]               0\n",
            "          Conv2d-204          [128, 1152, 2, 2]         221,184\n",
            "     BatchNorm2d-205          [128, 1152, 2, 2]           2,304\n",
            "         Sigmoid-206          [128, 1152, 2, 2]               0\n",
            "           Swish-207          [128, 1152, 2, 2]               0\n",
            "          Conv2d-208          [128, 1152, 2, 2]          10,368\n",
            "     BatchNorm2d-209          [128, 1152, 2, 2]           2,304\n",
            "         Sigmoid-210          [128, 1152, 2, 2]               0\n",
            "           Swish-211          [128, 1152, 2, 2]               0\n",
            "          Conv2d-212           [128, 192, 2, 2]         221,184\n",
            "     BatchNorm2d-213           [128, 192, 2, 2]             384\n",
            "          Conv2d-214            [128, 12, 1, 1]           2,316\n",
            "         Sigmoid-215            [128, 12, 1, 1]               0\n",
            "           Swish-216            [128, 12, 1, 1]               0\n",
            "          Conv2d-217           [128, 192, 1, 1]           2,496\n",
            "           Block-218           [128, 192, 2, 2]               0\n",
            "          Conv2d-219          [128, 1152, 2, 2]         221,184\n",
            "     BatchNorm2d-220          [128, 1152, 2, 2]           2,304\n",
            "         Sigmoid-221          [128, 1152, 2, 2]               0\n",
            "           Swish-222          [128, 1152, 2, 2]               0\n",
            "          Conv2d-223          [128, 1152, 2, 2]          10,368\n",
            "     BatchNorm2d-224          [128, 1152, 2, 2]           2,304\n",
            "         Sigmoid-225          [128, 1152, 2, 2]               0\n",
            "           Swish-226          [128, 1152, 2, 2]               0\n",
            "          Conv2d-227           [128, 192, 2, 2]         221,184\n",
            "     BatchNorm2d-228           [128, 192, 2, 2]             384\n",
            "          Conv2d-229            [128, 12, 1, 1]           2,316\n",
            "         Sigmoid-230            [128, 12, 1, 1]               0\n",
            "           Swish-231            [128, 12, 1, 1]               0\n",
            "          Conv2d-232           [128, 192, 1, 1]           2,496\n",
            "           Block-233           [128, 192, 2, 2]               0\n",
            "          Conv2d-234          [128, 1152, 2, 2]         221,184\n",
            "     BatchNorm2d-235          [128, 1152, 2, 2]           2,304\n",
            "         Sigmoid-236          [128, 1152, 2, 2]               0\n",
            "           Swish-237          [128, 1152, 2, 2]               0\n",
            "          Conv2d-238          [128, 1152, 1, 1]          10,368\n",
            "     BatchNorm2d-239          [128, 1152, 1, 1]           2,304\n",
            "         Sigmoid-240          [128, 1152, 1, 1]               0\n",
            "           Swish-241          [128, 1152, 1, 1]               0\n",
            "          Conv2d-242           [128, 320, 1, 1]         368,640\n",
            "     BatchNorm2d-243           [128, 320, 1, 1]             640\n",
            "          Conv2d-244            [128, 20, 1, 1]           6,420\n",
            "         Sigmoid-245            [128, 20, 1, 1]               0\n",
            "           Swish-246            [128, 20, 1, 1]               0\n",
            "          Conv2d-247           [128, 320, 1, 1]           6,720\n",
            "           Block-248           [128, 320, 1, 1]               0\n",
            "          Linear-249                  [128, 10]           3,210\n",
            "================================================================\n",
            "Total params: 2,912,089\n",
            "Trainable params: 2,912,089\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 1.50\n",
            "Forward/backward pass size (MB): 1791.54\n",
            "Params size (MB): 11.11\n",
            "Estimated Total Size (MB): 1804.15\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAEWCAYAAABfdFHAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9e5huR1kn+nu/S3/dvZPsXIhyCZIIQSYg6nMiXg4zBw94AG85KniIOMOMIjNH0Bkvo+BhHOXIGdQRnFFgDjcFFBMkwxgligiIcstNICFAZJMgSUjIde/s7Ev37v5q/qhVa9Wq9d5Wd3+d3WG9z9NPd39d/a6qWlX13n7vWxRCwEADDTTQQAMtikYPdgcGGmiggQZ6aNMgaAYaaKCBBlooDYJmoIEGGmighdIgaAYaaKCBBlooDYJmoIEGGmighdIgaAYaaKCBBlooDYJmoIG2SUT0q0T0hw92P7xERIGIHrdDvP6GiF64020HemjRIGgGOqmIiP4lEV1PREeJ6A4iej0Rnb6D/B9PRH9CRHcT0SEiuo6Ifo6Ixjv1DOP57GFLROdWAuCB6uuLRPRSg9dFRPRJIrq/Gs8HiOi8xfV+oIG2RoOgGeikISL6eQC/AeDfA9gP4NsBPAbA+4hoaQf4PxbAlQBuAfCNIYT9AJ4L4EIAp26X/w7R6SGEUwA8B8B/IKLv5hpVFsnbAPw84lydB+C1ADZ3q6O7RUQ0ebD7MND2aBA0A50URESnAfg1AD8dQvjLEMKJEMIXAfwIgHMB/FjV7leJ6J1E9DYiOkxENxDRhRmfRxLRZUR0FxHdTEQ/kz3m1wB8NITwcyGE2wEghHBjCOFHQwgHieg9RPTTRb+uI6IfrH5+IhG9j4juJaKvENEvC2P5diL6KBEdJKJPEdHT+s5HCOEaADcA+GahyTcDuDmE8P4Q6XAI4bIQwpeqPoyJ6JeJ6AvVPF1LRI/O/v8ZRPT5qo+vJSLK+v/jRPRZIrqPiN5LRI/J/vbdRPS5yhr8PQD5/7VciJmVxgoK4zmBiF5MRJ8H8PlekzfQSUeDoBnoZKHvBLAM4L/nH4YQHgBwBYBcs/8BAJcAOB3A5QB+DwCIaATgzwB8CsCjADwdwL8jomdW//cMAO9S+vBWVAKt4vdNFZ/3ENGpAP4awF8CeCSAxwF4f8mAiB4F4D0Afh3AmQB+AcBlRHS2NQEFn28H8CQAB4Qmfw/gCUT0GiL6LiI6pfj7zwG4GMD3ADgNwI8DOJr9/fsAfCuAJyMK82dWz70IwC8D+CEAZwP4OwB/XP3tYYjv5+UAHgbgCwD+1z7jysYnPiej/xPAtwG4YCvPGOjkoUHQDHSy0MMA3B1C2GD+dnv190QfDiFcEULYBPB2AN9Uff6tAM4OIbwihLAeQrgJwBsBPK/6+1kVL4kuB/B4Ijq/+v2fA7g0hLCOeDDfEUL47RDC8cqCuJLh8WMArqj6Nw8hvA/ANYgHvofuJqJjAD4G4HUA/gfXqBrb0xAF4Tur//uDTOC8EMDLK4sthBA+FUK4J2PxqhDCwcoC+iAay+nfAPhPIYTPVu/i/wPwzZW18T0AbgghvCuEcALA7wC4wzmukrTnJPpPIYR7QwjHtviMgU4SGgTNQCcL3Q3gYYKb5RHV3xPlh9tRAMvV/z0GwCMrd9BBIjqIqDV/bdX2nooXSyGE4wAuBfBjlXV0MaIgA4BHI2rwFj0GwHOLPjxVe25BDwNwCmLs5WkApkp/Px5C+JEQwtkA/imAfwbg/3H2t5zDJKAeA+C/ZH2/F9E99ihES+6W7Pkh/70nac9JtFXeA51kNAiagU4W+hiANURXSk2Vhv5sMG4qhm5BjFucnn2dGkJI1sRfA/hhg8dbATwf0e12NITwsYz31zv78PaiD/tCCK9y/C8AIISwGUJ4NYDjAH7K+T9XI7q1npT147HeZ2Z0C4B/XfR/JYTwUURrsI7zVHGdPO5zBMBq9vvDt/icelhb6P9AJyENgmagk4JCCIcQg/W/S0TPIqIpEZ2L6Ba6FY1lodFVAA4T0S8R0UoVEH8SEX1r9ff/COA7iei3iOjhQERvEdEfUgWhrgTLHMBvF8/8cwCPIKJ/R0QzIjqViL6N6cMfAvh+Inpm9fxlInoaEZ2TtZlUn6cvyWp5FYBfJKLl8g9E9FQi+kki+prq9ycgxq4+XjV5E4D/l4jOp0hPJqKzzBkE/huAlxHREyu++4noudXf3gPgiUT0Q5UF+TNoC5NPAvhnRPR1RLQfwMu2+JyBHmI0CJqBThoKIfwmoqvrPwO4Hw0U+ekhhDXH/28ixlK+GcDNiO62NyHCfxFC+AKA70BEsd1ARIcAXIYYQzmcsXobgG9EFBqJ92FEQML3I7qdPg/gu5g+3AIgBbrvqvr/79Hea68HcCz7+n1hSO8BcB+An2T+dhBRsFxPRA8gghTeDeA3q7+/GlFI/xXiXL4ZwIrwnLz/70aEmF9CRPcD+DSiRYkQwt2IcPBXIbohzwfwkex/34foerwOwLWIwrn3cwZ66BENF58NNFCbiOhfAHhRCOGpD3ZfBhrooUCDRTPQQBkR0SpiXOQND3ZfBhrooUKDoBlooIqqfJu7AHwFwDse5O4MNNBDhgbX2UADDTTQQAulwaIZaKCBBhpoofRVXazuYQ97WDj33HMf7G4MNNBAA+0puvbaa++uEoVd9FUtaM4991xcc801D3Y3BhpooIH2FBHRP/ZpP7jOBhpooIEGWigNgmaggQYaaKCF0iBoBhpooIEGWigNgmaggQYaaKCF0iBoBhpooIEGWigtVNBUVXhvJKIDRPRS5u8zIrq0+vuVVbXe9LeXVZ/fmN2QCCJ6CxHdSUSfLnidSfGa3c9X389Y5NgGGmiggQby0cIEDRGNAbwWsSLrBQAuJqLyStafAHBfCOFxAF6DWM0VVbvnAXgigGcBeF3FDwD+oPqspJcCeH8I4XzEu0s6gm2ggQYaaKDdp0VaNE8BcCCEcFN1Fe4liOXTc7oI8aIpIN7l/vTqMqWLAFwSQlgLIdyMeG/6UwAghPC3iLfxlZTzeivifeMPGm3OAy69+ktY35ibbW/48iFc/UVuSG0KIeCd19yCY+ubZtsDdx7GRw7cbbYDgHd/4lbcf/yE2e5L9xzFB2+808Xzz6/7Mu5+wKzsj9sPHcN7b/DdBvxXN9yBLx+0b/W954E1/Pl1X3bx/NA/3IUv3n3EbHf/8RN49yduhadk08e+cA/+4SuHzXbHT2zindfcgvnc5nntP96HT992yGy3sTnHpVd/CRub9rq7/tZDuPYf7zPbzau1fPyEve5uvOMwPvaFe8x2APCua2/FA2vczd1tuvnuI/jQP9zl4nn5p76M+46sm+1uO3gMf/2Zr7h4/uWnb8dX7j9utrvz8HFccb12U3hDH/zcnbjl3qNmu4NH1/Gnn7zNte4+cuBuHLjzAbPd5+64H7/9VzfiHsf+3ClapKB5FNpXsd6K9jWtrTbVveGHEO919/xvSV8bQkhv+Q401/e2iIheRETXENE1d93lW7xbofd/9iv4pcuux2//1Y1m2+/9rx/Gc//bx8x21916CL/4ruvwi5ddZ7Z99n/5Ozz/TVeah9jth47hZy/9FF74B3bi6o++6eP4V79/NdY29APn2PomXvKOT+AHX/cRtR0AvOQdn8C/fvu1OHjUPhxe9PZr8bTf+huz3a/86Q14yTs+gc87DvsXvOUqPO0/2zx/532fx89e+in87edt4X3xGz+O/+M1f2u2e9vHvohffNd1+JNr7RuLf/j1H8X3/e6HzXbvuf52/NJl1+P3PnjAbPv9v/dh/PDrP2q2u/Lme/FLl12PX738BrPtM3/nb3HxGz9uHow33fUAfuFPPoWXvOPvTZ7Pef1H8YK3XGUKz0NHT+Bn/vgTuPiNH1fbAcBPvvUavPBt15iCLoSAf/OHf+96ny+97Hr81B/9vUuA/Ks/uBr/9Dc/aLb7jb/8HP7tJZ90KQTPf9OVeMarP2S2u/GOw/jdDxzAwWO2crlT9JAEA1R3mbMrPYTwhhDChSGEC88+211BoTedshyLLng1MQCmBTCbxtf1wc/ZVsWJzTj8L96ja+vpPLjKYVGtVdbZZ2/XD/DNiukt99rWx7xq+6lbbW0dANYdmvpoRACAT3zpoIunh1aW4txf45inRNZhe8osXqzptQAAmEL+lFlcdx92CMREh47qB87KUvRa/82N/rV86336ux8RuXkerSz4Gw3FIa27z91hKxiblQJ2vbHukp52yHEoU/X9U7fu3LpbGlfrziFoElnrLv2Z1FY7S4sUNLehfZ/4OdVnbJvqatj9iDf3ef63pK8Q0SMqXo8A4PPxLIjG1UbyLPp0OHzqFn2BpgXicTc84eGnAgA+afE0OTX0Tefsjzy/pC/6fKFvGhbVNz4q8rTGnpPlOvyGrz0FAPDJHhveEvLnnLEaefbo5xfv0TXb/StR0FznFLKALeSXJqPePK2DMR1IdzjcR486fcXFs8+6e/I5aY3oY8rXnXXYfmPiafUz42O5wf/JI04D0G/uLSF/7sP2VTz96+42w70cqtkn2j1Rs0hBczWA84noPCJaQgzuX160uRzAC6qfnwPgA5U1cjmA51WotPMQr4y9ynhezusFAP50B8awZcqX+QlDC7+gWqCmUMiYHjZiKo89Ox62tvBqmN5xSD9Ivua0eHW9ZX3kY7d8xvsqIdvnAP/0l/Xnj0fpsPXz9Ar5PofIJ28xBHI1Uzc5YkRn7lsC4O/n+ubcFPJPetRpPp7Zz6aQTwqOYU3m686KFTz6zCTkrflsyBLyp1dC3uxn9vNnb79fbZuEfB+l6brbfELeErI52QI5fh/tokmzMEFTxVxeAuC9AD4L4J0hhBuI6BVE9ANVszcDOIuIDgD4OVRIsRDCDYj3nX8G8S70F1f3wYOI/hjAxwB8AxHdSkQ/UfF6FYDvJqLPA3hG9ftC6NKrv4Q3/d1Napt5tpFuNKya5BKzDtuc5/VWYLhaRJ+0hEK2k7yCzjyYMrlqta1dZ7ccVLXQ/G9enjd8+X5TyE+q3ebleejYCdOifMT+SiAbGz6XA5aQ/9ok5HuskZvuMoT8UmVJGwI553mDKeTJybP52bYqqnbmfPZZI75nt3g62376tkMugAfg7+dtB4+ZYIzTV6fOfsbvtIvOs4XGaEIIV4QQHh9CeGwI4ZXVZ78SQri8+vl4COG5IYTHhRCeEkK4KfvfV1b/9w0hhL/IPr84hPCIEMI0hHBOCOHN1ef3hBCeHkI4P4TwjBCC35nek/76s3fiXdfeqjfK1tkXjA2f6Ka7bM224Wm0rZ5/k2FRhF79bDRwTVsO2eBNnlXTe46sq37wfv1s/uf2g/oBvjwdVzyNWFb2s4VSS/EHq5+hh1BIbU2e2c/etubYe8x9amuuz3yN3GnNfVp3D+gusT79rBrffug4jq7LikNr7M69dGR9E3ce1q20ZP30WXdfMkAGk8qSt/uZXGdqsx2lhyQYYNF0+soUBw3far5AzLZV4/sM5FW+6A8aEM60kQ6vbahafS4ULORX/nyvUNipeeo3n9mYjlljim2tuc8HZb8nJ8+M7jPG5G6XTZS3rd3PfI1Y/WwsP91CbX623lF6/InNgCOK6y5/Wp+9tFNz3+f53rnP59CGbfvWXeI4CJqTnM7Yt2S+zHmPgym1Pbq+qaKK2jz1RT/PZIt2OMx7bDjvmHqNfb4Anr3G1K9dL55HdmY+87aLWHeHjp1QLdStjH1zHnD/cdlS2PK6Uw7bPvvDe4D3mc+wpffpU0I9bdOcehWxhwoY4CFLp69OsbYxV32mvbR6Z9u2Vu+zaKy2Le2/j0Wl8UTebocsmi3Pp2+eDvU4RMy2Fc+dms+87eHjG2o+yVasyRCA+90Wqn+e9DXSY91lP3vXiIXmynnumHW+hbnfqX0cn++0aOoYze7RIGi2QGesRgSQ9kK3LhS0RZ+7hHosem0jZT8vRCiYritfW+8c9W7rtGjabhEfzyPrmyokdqsuRvVgzH7uJRTUw7bP3OfPX8ABrq6Rhvq5zpz72FrLvdZdcLbLn++b+0PHTqhghPSX0WDRnNyUoJGaa6SXm8u56L3tOm0V18BWzP3Y1ncw9XOL+A6mfq4z34Fz7MSmaqFude61w6nfGvG1PanWndsVukNjn/sP+q3M50HTFdr87J2n+4/3cVv63NDzEC1fmWdynansdpQGQbMFOr2yaFSNsVog+1emdnmV0CTv6W1DxtMOyjY87QPcxdPZz7Q30tit5LlTZxMQWS6+hufaxlzP56gan7o8cVk/CRbqsSZ3dO6r7zu5RkKPdRfgfZ/+sYewlXW3M/1MdPrq1BUQr3mqiljTTxNYEyKabHk6crmr969MEYJloWZzbwi6fEyqlZZcZ4OgObnpjH3VAlUWSNIazty3ZJq88xDqhDxdu0LN06MxNTxtbTX1UxMK8xBw2soE4xG5tNWz9i2ZSKF5CBiPCftXpi7N0jumEUUXp2uekuKguWXyeeoz9w5r0vc+fWukzdMWCmfV/bSFgqefwbnu+vEMOGPV70U4c3XJBNaEELA0GWHf0tjlNkxjspQRQlp3vvkEfPN0lnPuz3LxrCyah0oezUOVTl/xL5AzVqcmLDFU7QCfFnjG6tT2w4aAU2YTTEZkxGhCzXNzHnBYSUYMiH5dC95d93OfbfmFgHpzemJJrg2PACLCGas+DfwMz2GbzZPnAK/fpyPQfMbq1OV/b9aIbU36xp69I0c/PcpIqKy5aKHa83nmviUcP2EDa6bjEU6dTVzKQBqTBghI6+50QxnJ3xEAHDJiRES25Ze/I8BnSZ/hUFjztezZS4NFc5KTy9VSfT9z3xLuN5FCAatLEyxNfCb3mfuWIlJIKUMThULcSB5XS32AG5rtiAinG4dYs5F8QqHhaY/9DIfbMgTf2FNfvTwBn0UTkAtEWyhEnrZQ8GnV7X5aQuG05QlGltsSzdyvb8xxzBQKhNOWrfeJmidgj2lEhNP39Vt3llXRZy17rckRUVSanPsY8K07i2fqa5+1PIABTnJano6xMh2rL3NeHIxWTgFR0pZtN5dnI80r9SpaVLa7wYOkm1euAUsLTJbWmftsf3Ea++krvrE3PPWxE6jy1XsOcI8rNH63LC8gjt/3jpq5P3h0XbVQ5yHg1OWp2215xuoSNuZBLZczn8eSMft3dO6Da+4bV6hnjYRqjRjrruBp7U8i+wDv7A8156ayzvft8D5GHJMHNFHPp2PPD/DmPUDWRurlh0VMnoobyeduMHnWQkFf9H37CfK5j4DGhWHPU9QC1QTHwi2i9zNU/bTcIo1bwsUTcZ6sBMeAWCx0OtaFQj73FlIotATyTrkYgXrune4bwIo7ASCHS6r67pv7SN4951p3sZvYb1k0pcVvueOIsH9Ft1DLfeyyJj0WakDmthzAAA8Jst0y7c2pI9RC5eqZ6olmPXzQQOM+8tylUfPU2tbuhiU1abHenKupn/o8efpZb85Vbz/jfGoJjqmfq9MJZpORO+4Ugp7g2J4nh6un9qvrrpHk6lHjBKFcd5arx153tfByzH1y2aY4ovzsgqfDzXWGse5QrBEzNpgsfudBD9i5XskzocWy0senrUwrt6Vvnk5shvpuHv75sXL5/hU95tfEaAbX2UlPVrB13kMTqk1uyyXVw/qIrgF7I3XQXEY5jthPn1vEo1nO543b8IG1DTHBMY19NhlhdWns6CfVrgnpwMvzCXr71Q3EoccVWvK0XVIwLb++iCbqYfn5LWnaAk8DzeWwpDt7zkCTpbFrwJo0n6cuR2CNZVGld6QBa/IYiQ1G8Lu28zF5LNTBotkD5HVJuQKTFUrKdA2gXHReVI19MNVJqKZrIPLUEhzTlp2OCafMJqbGmHz6gKwx5vWZTPdR3U99TLVmhx5uGWdVCAJsV2gfntmYXBq4q59x7r3uI/e6Izvvoxu4d6C5VpfUBMfaQl2aYGk88vFcmapuy9rNBB9ghSjOJyADa5qLx+wCvc3YfahUgp2b1Ixp92gQNFukU2YTw4yNb/PU6kpnqxT5iBJPu2R5upHzqAZFrtwNp8wmWN+Yi4lmjVAYYWU61p9fIcTq5wvjT0IhtT26pqOURtRcfS21Tf1MbfV+ouinoVmOyJ776vupRj8T39GI7H6GYo1oPLO5V9ddpoED8jtq+hkTZo841l3TT3vuT12OPC330fJ0jMmIzHkaEeHU2QQhQES9pT1XrxH1HYW6nwDE8bd4ziY4or6j2O7UmcGztmhiP625T2MHgCPeuVfbNftzt2gQNFuk1SX9Zc4LoaAt0HkVEF+tDhHJjE+unqXJCEvjkZkICQJWq/vepQNnnqk3+2Zjg2f8nnhK48+7vzobqxtpXvnKV5f0zdn0M7a1xk5Aw1OY+7y8yKpxiKS2pxiHSPv5Y/Ww66wRc55QrZE+687qZ7XuHGNfXYoVHMy5p9h2HoA10RXaHOCrS2NzfxDiWgJkQZcqllPiac0nmjUizem8WXZYXbLmPlo0q4aC06C+yLFGqrVkKHfN86t159jHg+tsD9C+WXyZssYWP1+ejjEiy6KJQdl91QEua2yRRhQ3na3VN1clm1p9dYB7rKR95qJvNKZ9Sz7Lb9+SwTMz9/fNxmY/k+CMPPUbMamae481ac1n6ipVY1e11eq710KNPPVDOa07+x01bpl9S2Osb87F+FgTPAZWp465RzP3kqCrt021Rk1rsppPQBZ0TVHLat2Z1gdl/dStc6ra6nOP1j4WeaZeElxrZFS9d0BXRpo14rNQh8oAe4BWlybYmAesG4imUVpMxgJN2iqgmdzZojd5hlpjAhwbCUkLNHhmVpJsfaDqZ9JWlYO+ct+sWgdT9X3ksGjydrGf0tgLnq4DXJ/P1JaSMtBDKJhzj7jujp3YNOMU0zFhaTJyHUxpnqT6ca15mulznw7GVUNxyN03nnU3ItuSzvecbUkni9/n5kpzb1uTZFpJoba8Gi+G2c+Z4VrO97G17rIY0W7RIGi2SEnDkGMKjVCwrY8oFEyerUVva+BJYwKURV99J6dmSbmVZPaTKp6Wa4BMiyZHiFnWR3Ih7LNcLdnm3OewED3zCTRB2aRZ2nGKEcZWnCI9f2ZZvbkyYgu6NHbAo+B4LL9Qt9N5xu/x+bYlndoBWmyw4umwpJG9I8CxlpNFoyo47XUnWzTZunMoYqkd4JhPj5U0uM72DtnWR/xeWzSOoKxXu+qlWZqugYynwy2T2qn9zAKolq88ZO1iPz3aqm0huiwaJJ62lZR4rkwdFg2audfiFB2t3jX3uputNU8ut4wdp+jMk2fujdhkDsQwLRpscd0ZAfHW/nDx9LiWHfNZvCPLvemzEBPP2Pb4ibli9Q5ggD1DtgYev8dgq+7XrrX6mS9wX2uWFk+HBt7S6h2ugZYWKPHMgrKWrzyBASxtNQ+guiwaZEAIy6JJcQoFnZd4poPRZVHVYATbxeie+/pgtNfdPsOFkoLXlgbemifD8kvzVFvnO7Hu5j5LujWfliU9Ly1UB09DIJb7WH5HRdxHsXrn87iWliYjTMekrKW2dR7HZKw7cSQ7T4Og2SKZMYVsc3o0y5a/2IhTAHBp4IB9MOVB2VUr2AlE1JfTNYBKyOpmfFsoWNYcMi1QTLKL3cR0PKriFMbYYaN6Ek/AN/ctQWe5eup5suI+5FgjPdZd3c9+cQpz7K21bAfELdRbci3ba6RRRkzXcuLp3cc1KtR2Vy9PxiBSrM7qe5qnEIDjJ2QgRhIImvWTu8NM6ydzB+4WDYJmi2RpQm1kiQ43TCVLLIsmd3N5YgouV0sHIeZBx/ldA8lXLsYpEOdoNqniFMqBU/fTilOEZhOp1k8+n4YG3uKpWKhtlJIvR6Ox/PrEiOwDJyEjRZ7JmrTiFEg8SR17008yteo2sMVSRoqxOw7wfQ73akLRxX769vGJzaCi80ZE0eqdyopDGfcBdCE/ytayrWDkFqrtDtwtGgTNFsmOKeRgACvnJtQuBB9POHzlhfvG1Krh0KpjOytOkW+k1dkYm/MgxykqTZ1q9JHtGrA18NDSAj0uITumUPB0aZbeefIg/tDWwF1avZG4l6HjXDw9MYUMHRd5eoAtdlJvu582FHl1aayi85LLdjIeYaag89o8fW5oQM93Csxalt2BoTZpNIRaH4tmnu3P3aJB0GyRvAiYhKu3A34OVwsSTztOUQevjThFHpTdt2RVEQiNxqY8P9eYPBp4Wu5aPKfU1OOYNIum4qlYfpk3zrZo0NYsPYFzO4cptNr2siYdFRS8685tJQF+68NA/JUauIrOq9xcS+ORWkWg3c8Ksi3enRNqjX6f4rpr7WND0CWoPpCsD98+jjzlOW1ZNA6g0D5DCYYwz4ukQdBskawF0gIDmFUEUlDWlx1f+7WNKgIEu4pA21IwwAhzX5yizKMBdDRZ2khaPCffnKZFUwlEq5/1QT8iM6aQgrKxnw4rCbZFkwdlPVUEWvMpAjHalrQduHfEKXJ0nCOeMiKq4xR2DpNdRSChMmur13GAm1UEQr6WNUs6fk/7WOcZ6iRILeeGs6S1tvmeUy0fJM+E7dbfTbcZMAiaLZOZQJX531N9Ki2fgmDHKXKN7RRXnCL+7NLqCXZtMDQbyRWngA9NVvdTccu0EWK2a6C2kpR+5ge9GVNAw/MUbT4zIeuttUYUa5jpgJFQtwMccQo4anM54xRtFKFeRSAd4ClOYc+9HVPID3DN8muErMdl3PDU6vGV+1jliWbPaXPP7WOtbb6WLUUsvaPYT0147a6kGQTNFqmOUzgOnNXZ2Kwi4IlT5EFZW6tvfLBanKKMJQHGondYNPmit3MfGtJyH9oIMdtKao3dKlnSmk957CPPfOaJus6SKU2Oxk7EKVDxtOMUaY2YcYrqe67Vy1UEMq1eyfUq4z6AF/GnBNnrfjosaTRavbrnWpaX7YbOrXNvXhKgK3f5uvMUsrWrCOwutBkYBM2WaTyKyXseWOQ+wy0TSq3eFAqeOEVoa/Uuv7bnAE/9dIw9dwc6Nuc+VavP4hkmdDa4+onsEPFo1ch5ejK0He6b2NYRp6jAAClOYeVTwIUOzOZJAUhwG7sAACAASURBVKxw6+4BVcGpeKooKTQ8q3UnXTvdRvxp/Uw87XU3zzqq8WwC/LlrW7HOq5+1JO35vPuOHlDcq7lnQpqjXJcw5xO7m6wJDIJmW6SVpGC1euXF14gmzTxu+fTt3Aef9cFp4LbrTNdW837atdZybdWTjOfJucnjKXbimp2fksZT83SU37HiFLmZZsYpKuGZrF4VLo+29eMBYmhzzyKanNakjbyyqx23EX9aP3MryRp7wbMHilC1VFprWV5Lqemq5bLNmKqVCTIh60LHDTGavUPai2+hQBwItQZZopjHjFavtW0sBS2ekvrpQ3PlqBorTtDupwfNJVs07HwqGng798DjurIPptGo4SnFKXKeVpwid994qh3nKCm7VE9u+W1z7vN154AY52OyS9D4qh3XPNV+IuunffWCh2fTNreSFNRZyzp35MMZ1jlydNzSGEdP8ACgfN3VVQQ0dNwgaPYOaf5iFn3kQpbI7obWfRumq6cnQgy2RdNF1eiuAU9MoTV2pWQKP582GECrdpxbSU0VAWU+M54AH6co7/rQLL88KGvnPuRzL2vgvVCE5dy7UIQW8qo9T32sSR151VjSHsBIE09xzH3Pdaf2s/pZg//n/VyejkCkl3PK11IIwPENZd3Vz5eV4Hwt7RYtVNAQ0bOI6EYiOkBEL2X+PiOiS6u/X0lE52Z/e1n1+Y1E9EyLJxE9nYj+nog+SUQfJqLHLXJsgI6Aafvq7dyH3FduZinDYdHkSB0t74OL+ziCsi4EDBzZ3DmqRql2nM9njc5zACG0KgK5+yY+X3HLIHR4cuNneWouvupnszov2mvEhT5yxMdac2/FPvJ4o2JJ5/Nku2wd1Y6LebL7maPO/OtO5ZkJWQ9YZt9sIlYRyN+RdeVHC+mpWH55XDS11SyqXQ7RLE7QENEYwGsBPBvABQAuJqILimY/AeC+EMLjALwGwG9U/3sBgOcBeCKAZwF4HRGNDZ6vB/D8EMI3A3gHgJcvamyJPJj+5FMHrJhCo636MrQdcZ9cq/bUnDIrExT+d0dlgAadpwmvRrOU4hR5LMnMpyhyDwBe0OWHSGqrlwIpeHKCBgxP1SXUaOqAdYhlFo2zGnfspzZPtpXUcjE6Ygr52L1Vpm2etiXd2nOO+FSO5pKqHefuuPGIsDwdORFi8jyV1ZPV5Oee626U7XmPu3q3aJEWzVMAHAgh3BRCWAdwCYCLijYXAXhr9fO7ADyd4oq6CMAlIYS1EMLNAA5U/DSeAcBp1c/7AXx5QeOqSasN1r7Dwx/s9GfHOzLuM+3GFZTtEUBV4xTZ2OsqAko8JUfcAbygy7XV1NZlKSiuu1wgprY6GKGwaDjNMrvUqubpCMpqlQnykiWxrSwQ8zIoZq21QgO3khZzlJRufdhadRvN5Xevaui83CXlQucV8TFu7ufFwtPnvvlZs/xKeabFc+ahvZYAfu7zq87j82VFLHfx7RYtUtA8CsAt2e+3Vp+xbUIIGwAOAThL+V+N5wsBXEFEtwL45wBexXWKiF5ERNcQ0TV33XXXFobVkFbtuKVZJkioS7O1C+fl2qoMCQ0tTUiqIpAHZes4hSf3QEEK5Vpger7HhaEh1EqhoM49urEPVnihfYDr1Y5DR7PUeOZtNXi1i2e2loBkodrBa18uSWZJi0HuTBlxVDtuW9IG8oqaascuxF9V7ZhNVM5kQmP1evJoZOHZnXt9z+ftIk/u+V2LRtvzpXXOrtHMQkxtVRfjQ8V19iDQzwL4nhDCOQB+H8CruUYhhDeEEC4MIVx49tlnb+uBKSGOI64I5HFPUFblGb+nOMWIgONK2/IA14OIzaLXxpS7WgB+w+eaZc3TgY6rg+xcPKVzgMv9bLsbtBhN/J6QdFo/2+V3tLHH7/ncq/OJLcznVOGZAzGM5MoyeC2/o0gjQl1FQL7hM5v7aUxU5mrn5Qf4qMpJ066SLl1SXNvQOcAnyhrxzX1pTa5OJ3qyarlGPIARbe4Ds+40no59DDy0XGe3AXh09vs51WdsGyKaILq87lH+l/2ciM4G8E0hhCurzy8F8J07MwyZlidj8aDPffXLkzjNYttMKCxP7M1JiHGK5an2/EYoLFeHA3fnRXl/uDqmvJ/TNCatn01b6b6N/ABveNqbU+tnfoA3Y1cO8KwtJ4yBdlBWnc8iKBvfkVYRAjbP6rtnPvO29XwqNcTy5x/f4Msk5fXTJpVLaufmvv18kWe5Rpgx5ajM1NY39/K6K2Nuy9ORskba61PkWQgFdd0557Ozj5WzIZ/P3aJFCpqrAZxPROcR0RJicP/yos3lAF5Q/fwcAB8IcaVfDuB5FSrtPADnA7hK4XkfgP1E9PiK13cD+OwCxwYAWKm0Bg0lNao253RMunZVvflUbVlbTEljXFE023wjrShaaB6Ubcakl8pp8WQ1y0h5W+2O+7SR9H62hcKKYdF0xq72s2mra9UFT+VgGmUbXtf+HfNZW17VgWOMHRXfdNh5rMnl6RhBBGJESmeTd90tO6zJ9twrl38VAll9n9nB7Jn7Zdf+yHiqOUTNOxJ5FvtY5Vm0s/vZtFWFLPuXxdFkUYxDCBtE9BIA7wUwBvCWEMINRPQKANeEEC4H8GYAbyeiAwDuRRQcqNq9E8BnAGwAeHEIYRMAOJ7V5z8J4DIimiMKnh9f1NgSpc25vjnHrNrUieaF8IkauK1dzTLN9tTldrt5sePtxUQVT81SaDOdTUYuTajuJ+uOa2tXM83yYsduC4XZZIy7H1jneSK3KGQNuIz7zDTLK7TbSf0sg7JRq7ZdVzVPZT4TLU/itdPzeZNEWrZN0NmliayBzzNffTrE1k7M658TlYftTFt3yOa+suTXFCttlM+9ptV3rHMN4tuMybOW+1peB4+dEHi2PRORp8fyGosVIfJ1l9YyN59d61xed7mCsVu0MEEDACGEKwBcUXz2K9nPxwE8V/jfVwJ4pYdn9fm7Abx7m13uRfUCXe8KmkS5dqdpV6VWzy4SZjHpVhJaPCWtKe/nypK8OeOz2zzZuFPotj14VBAKoYmRaGMvXVJaP3N03LLSz9L/vmK4IpG1k/rJjV0TsqgFZ0zck+J4QPsdAVEopThM8fiWNSnyDN11d+zEJvZj2mpWIq9WlpRDLF93jrhTIq2fuQauzX3pXjXn3sGz0MOwMh3jjkPHeZ7Zwz2eCdRzPzKKlLb7qVk0eT+1WO9DyXX2kKekYWiunpFDs80RYirP6nvLPO5hcmsbqe6nEaMp3Q2cFloGZfWYQmgJTsBwtVS/L0+0zcm4Wlz+dyWAmglEj/smn6cTmwEbQuXu1I4qV5fLfTOR42Mo/O/W3JfrTlUy+sbxNNddGXMz3FylRcO37aGIgXGdMa47bi1ryp1rH3NuLk/cR1NwkHjmsUHNrb+7kmYQNNsgl3aVtVURYoX1wfLMgrI1Ty2AmsUzIk/N5EbdVkedtfvJbc7SNWD59GuLRulneYCvLOmbs4x9eITsirI58/mcpoC4w21YP58LXhdCIVppvncEyKi3/Ajxara+OJ4jPla0A6x4Y3vuWZ7ZAd68TwXJlq0n2bXcXct64D7nuT3Lq3TZ6rHBrttQRSZm/ZxXbn2O51DrbA+RZ3Pmi0QNiHe0Ky3YiYanR6ufePqZeOqIJtfmrPtpu7laAlEZew7bTW1V2K7Doum1ObNnJ76sBlxaXkmzFYL8+X5fnvDaMmd5AfIBnl9qpSITc0STIhRYhFiP2IcKA29ZNEqcovp5ZUm2FEqIr7k/6vE4LOkUSxKsTkDYxxpCrPp9VsVopDy31G48qmJuGtKzs0b4toPrbA+RB27YRoH4XT1a8LptHttB2bQ5NaHgRdWUgUkVIVbHHyy3COpnA5I7Dm2e1uasGo5H8Q4XfXOmQ0SHbI/KA1zpZ8fFKKyRFk9BIHfdsDpCLddWbeRV1U5TRpgxacjEUbFGVOsja7vmCF7PNNgws+fWFLh6iRBzIT2XxmwwPo1pVK95x9iL2CSL+CvXnQDW6b4jbc/vPhhgEDTbIJ/G5tucnTgFm5TVVpetuE9+0Mv9LBBN2ubMtUBVA6666dicLTRX2pzK2Evrh9ucZYkNaZ46bi5Dq+/wVCyvEs0lzVPLShJiH6UsrRFiQgJuXpk3HuDyuiuD1yqiqbYmZaGQryfdtdwFYshJqE1D/R2h6KcvAVfLeeHyt9Y3+bpouXUe89yMdQdbKPDu1R4uW4d7dTdoEDTbIA1uWAZltc2ZB2VXlEMkUd5WD9xX7epDRA7KJpis6i9GGzQASIHJQgNXNieyA7zenBzUk3Hxyc/vurk8SDo1ya7gqcXHAC5GJLkwGqZiTIGJJQFy8Lrspw5Wib973EeJLNCEz8VXkcPij80cFmIBvUpriQ2Io5n76TgWzPQk4KregcCsEYcr1MrLKl2hXle9yLNYd7tBg6DZBvUJymqbc85sTs2i8cR9uAxtT5xC25y5dqVtzq5Fo7vuRuXB6EBzWfGxUXGAe4PcMs9u7EN9R1k7lWf2u4RoKrVV1S1SWknCYVfGM1TIdkWuwD187wi1oGvm3oXmmiSLn7fSIk/Uz9+cB5zY5NyrhYIjxcc6MTfrAG9+l9YIVz0CkIVnvkZE4cVYiJGnrYzsBg2CZhuka8Dtg8lC1Xg2PI+AcWRTa/7i6ju5Nid6bk6foPMEr7uHbQ83l+iSkg5wwfrIfl8R4mNlUHZHNeCsHaAJr/a688TR1Hc077b11E/TAtIlKnO5CnLzCk7z7FRlQ0f8FQc4F0sr516Kj6GHUEB37tVE4er3PsJrJrjguzFMA4QyCJq9Q7oGzARle5WZsCG+6aCXsPKpnVYChwMYiGMqgoiipcAEZQEf8ko+GOP3LWmWQlmdcnPaQfaG6UywPsp3pMXHyvmcWW6ukeMdFetOqsrQ9BNtnk60o6vKxcQDGKnGlNy7wl1E7biTZCl0+wkIybrl3Js5TAVPae6zE3XmtM6tWmutPSfEBjvWuWklDa6zPUO6m6sMyuqbE/XBpME3C81S2ZxckL+Pq4eL5+RB2WZMvqBs7KftL5Z4NkKh7ebyWZPS5iz875aV5LA++gEMGCtJrcwbSX1HZdxHWHel9m8hn4D2ISbF3PIDfDSSA+KloLNqveUHuOw+6vYTUFzbDotGcjGy+WMda1IAAzBQfUDOc/PEBqVcK4+7ejdoEDTboBoMIN0IWSwQbnOWvtW0OblDpDRcrFIo5QJlD/r0gyMwGZs5hEInKGtszpbw4qHIHSGruAO5+JjnnnVX8Loi0dXS4x11wACSq6VAQqiWNAqf/pLu3kw0HRNGZMUG4+9mle0txIg0N1d+6ZzOU3Kv8kH+jptLBQM07aR+5u5qQHPHRSqFgivXyrLm6v1hCdnBotkztDQeiZsTHdcZvzlLLSy29d0lYZVX6fBk+9kOylqbswzcS1Za5Nm0A/jNCRRavRQUZuJTgDz3HeQV67bktXoPwEAqF9NB3Kkuqa6Q1cq1lAFxTzLe8iTF3OwSODZKyrYmUVjyIrij8HPp9ficSLridy25sxQK0tx33atW7MNeIxJCTBZepTJix5LUKhtFDHM3aBA02yDtTpiOGS1szlILAxS3TF+TO/tddqFUY6n7qWdeezZnNyirb87yAPeULLFqrbmSK0X/u7A5O9qqA3FX99MWCukQKWNupfavx9wKt6Fg/XDrTlRwSqvTjDu1eUr34QBdeLWcLJvzFKpXpPeZYlnGnTAdly27RpLHoVTEbISYlYDbgTcLArkDBvBUpDAScAcwwB4jLXjt2ZylGQ3IyZ3doKwfWSJBZztBWQtJV7owlAPc45LquM4Ei4arnwYo8bHyAGXddqmfDosGKDa8D3FXx9ykQySbz9mUvxOmvNQK0ARyF6UEyJZ0x8XocUWqtfN8EF8JISYiE0swgAeqb8HV8zF5XVI91oh0NUi5j5srInxoR82t3oArNLRjWxnZDRoEzTZJ2pwd15WwOUOhhTU87WBnb63egz6ykHTZirE255ZcfBO+ikCJ1NE3Zzl2KeaVrCS0eQpt2+if5k6Ydj/bPEcjinf8CG6R0hUJdBOAS8sL0LTlEqWUkFel8Eo82/PkKpWj3BjLzn0Pa5J9TygtGuGwLcaUlCYPzxjD9BVzjTztcjHSdQrlnteu3OjuYwnt2F53S+Pq2gkHMnE3aBA02yStzERLYxI2Z4kOs3gCTLBTQiqVLgxPUFbZnGVQVtqcEvJKLG/ScknpSYv5swFl7EVbrUxQen59J4wDISYJ+dLySm2lfpbuG0Bxc6EUCr7yO2w/OZetgrwq2wF+JJ3njnsVITbv5qR5UJnWfThUHuAeIISBCm3veaflpVn882Lup/xV76XLVou5lRbibtAgaLZJmvXh3ZxAVwt1BWW1IDuj2aqomqJUvxToLn3lrgCqsjlRHuCSS0hwYcjuo/YBzm3OpoouVbzJcEkxlkJpfRRB2dTWF/fRASMegIPUz7Jt6WoB9OB16boCZGQiirnXg9dtnpqrqeFpFEnN2ok8hfiYxLMs5+RCES7x106U+zhdOyGiCBllpLtGuutOK1czxGj2GGmZ1720VUdQtjyYTNgw8rbeEhsGbNi1OSUEjO06S6izbhJqGwygbU4w6Dj2+YxLSi5X00WIAUzMjY19yHNfClmVpzN4za0R8WDKPtOy48v3HnmWLr62+ya19cQ+9HI1zpJCSDwLK0mEDReHslbItrD4ZSup+X15yl87Ie357cTHSlAPoCg4GODNe460Rc9tTlGzRLutN4se8AUmrSx+3+YsgrJLzgztnpuTC4iXroHUV1edORF5lXgWbhkB1cO5zlyWggjHLZQRoZ/l/SU1T2feBcuTmU8RhMIE+DmepTsstVUt6XSAq2jH7hrRE3D1fqa2JU/p2omc56i6E0bOM2vzBLrxsRKqn9qKFr9HaWIsaQ0AtMsGzSBotkszCVkSuoFWoBu85oKyM8k1wAQGAV9Qdma4pDoQX0fwennCb85yTGlzemqINUFhPiBeorTkooG5S4ivss2huWZTKXDfBVcAjKVQBGVTW09QVkL8NcHjgqfn/hLBQuXBABKiqUR9SS4+TlPXA+KlUJAC9+Ua8aAdU8zNCzAAGMRfvT/a68kDBpDiYyVUP7Xl++nb8yxgREGonXSuMyJ6PBG9n4g+Xf3+ZCJ6+eK7tjdIux+C124cQVklcF9CgQFnUNZZYkO9E6YMygolcPqMqVM/TdLAmQNcQ/WU7jCWJ+c6kwL3TFAW4MAd8bvvffbMtSrLmwiuHl8/uXcku/g8rshGeLXbemrC2SVo2mvkxGbARhlzC+0DPMXcPJDpFcEVWgbugco70GPuy7YcYES1zjmeDmtSm/uT8eKzNwJ4GYATABBCuA7A8xbZqb1E2o13rK+81G4YS10FGOSapXonTLsDyxNhcxZavXYnTCcoKyDppDiFhKppo+MkdF5nSMZFYfY8lf73+Hw5eM0FxKU4hQch1nVJ6dZHx30kZZKj3S7y9LwjTcGxrbkyNybn2U1CbR/g0/EoXjsh1PEq1xLQ9Q6wa8Q998Y8eefes+er795yTr3WnSvuc3JeE7AaQriq+GxjEZ3Zi6TddeJB/zQB6a4m0kWrtBdIuhOmq6nz2j/AuO6k2IdYVqergYtJqA5IKtDVwliejFDwBu6lcjXcAa7GxxyaJctTjePZa4SL++j99FuInndU1k+TQCic0rSyxAfE5XUnKDgOS4E7wDWt3lNBgXMxamV1fO+Tt/h9pXL8684bx9sN8giau4nosUjvm+g5AG5faK/2EOmaSLsdwKFFeL/2PKBzJ0x52KU7YWQEiiOmwGrgiqvHpQVK2pUUS7J5JipjGq52UnIn53/vbX1IY29bVL3WSMfq5cEActY3w1OKebXiU1ED7ig4xdjHI8LSmI9lAUW8UajLxikjy0J8rFRw5GoH1TyVsSzHPM1Mqzdrq8THqNWOHztneUlx2a41qefjdSxpR/203aCJo82LAbwBwBOI6DYANwN4/kJ7tYcov7Apf3llYHA8IvbCJikoC8QDZ2nS7JoywJ/aShuuV6C54xqQqh00vzeb0w6gSoH70gctb87uAT6bjnD4eNe4Lv3v1iHSPhxk19koO8EsV0v3ThgheJwfitJ8IvFsj11+R9xBL/Wze4itbczr8cW23ZIlM8YVKsF2gSogvzIteLb7LgFWpHhnWXGAcweqc+9wW9bKSF65Q3GXS/uYY1q2vevwWodnCdUXFTERDGDnb+0GeSyaEEJ4BoCzATwhhPBU5/99VdCsXvTdg7F8mZxmy21OSWMrA/wA74eVNKbIU2rbPpxki4bRrqSs87yf4uYU0FzCIeJGcxVaLeA8bIUSOKJAlOIEhWbJV1ooa50Z2mqhOHAlcMp1VyP+XO9IQPyhW7KEW3fSO4pj6rbtKk0Cmgtd1BfLU0JziXGf7ror3xM7TwKaq2PxC1U2+FiSVGXDh8rk9rGG+DsZwQCXAUAI4UgI4XD12bsW16W9RSIcF12hwGlXbFBWguOGrsnLwXE5/7sFx/W5ZXoGmh1WUgdJZ23O4nCQSuBwcNzu5uTdln2CsmU/RbehI3gswXGlwD3Aw3E7yggDx5V8+gCvOJTrjhOekvtG5Fn2c5su2z6Akd6B+84a8UD1BYHIrWVFEeOg+vKez57vtBB3g0TXGRE9AcATAewnoh/K/nQagOVFd2yvUL6R9iN3DXSFArc5+woFzkoSDybOhSG5G4qNJGmWXD/lQ6zQLO/nhRe4g8mDqlFcGOhxiOSkFR/lBKIE8S3bntgM2JwHjDN1v1wjRKkApxRP4X31KZCdnt/Hverx//PCqys8pbGzPIt2dT+lW1iLdpGn4wCfjnD3A133aqnVW0LBde3EbuRa9XDZarlWu+0602I03wDg+wCcDuD7s88PA/jJRXZqL5Ec+2CEArNANaHALfquFtjVriSNiefJC7p7j6yjpG4AtU/inhS49/mgJQ2c5dk5RHq4zoTYRxmUTTdSirEkIcFx32zSauvR6rXYB+sSY91cjniGEiPqKk2Kdc7OvW2d82u5h0DktHrFSmo9u0ccT7QUOlD9HpaXAgbImUrgCg6qP5uM6kvvpuMs1ouuhbpoEgVNCOFPAfwpEX1HCOFju9inPUVS7IPT2GYMQowTCjPJPOaCssyi5zPe/ZqlFKMRNUtPUFaoDNCJfZjCq4wl2XXeROQTFx/LbqTMNycKntH6YA4cLj424QVN2a4ZkwQDt8cERhlR153LouEC991+cug4Ke7ElUGZTcY4ePSE0M92O0BZd8XcS0g2TwyzFkdFW+lmWX4f8+uuLRSaS+/aoKL23E+qGn9eRSyOabO1lrlY76LJgzr7BBG9GNGNVrvMQgg/vrBe7SHSLBqXC0Mwo1me4IKyIxxe4zenB1XDB2Wdd+yIm5NH1YhFIDkYtuQ+KvvJbU60N1ydDOjanM3B2NYC+bnvhyLc2txL7pvUz/bzhXUnlkFpt2N5CiCUo+sbnXbAdlxnjPCqvvMWoidZ1rmWpeTnIKzlnYbqT0cIIeYbJUHKtRPHJFjn8flznLrcbptDwHeDPI97O4CHA3gmgA8BOAfRfTYQdPOYd3P5zOjIk0Oy2QcT675RXAOs/70HqsY3Jjn20cd1Vm6ktDlbbbkDXNHq+QN8m3PvcqH45l4CLbA8574DXF13LoHIzGf13VXjj1PEmLqBLFS/b5yiB2S664qM39uusxHWNrr5RqV1Xqc0OKD66roDM/cOFKHsxejO/aLJI2geF0L4DwCOhBDeCuB7AXybhzkRPYuIbiSiA0T0UubvMyK6tPr7lUR0bva3l1Wf30hEz7R4UqRXEtE/ENFniehnPH3cLmnFMjsHk7KRfMmVvGbZy4xmNHDPwcQ9v1dQNss3yqmsn5Y2p6d+moS8kmIfJRBCg/huee5Zy0sJsrNWkgMhJsZTuIPJt+60fKOO64yxksw8mlZbxsXHoAglJBnfz9Q2G5MEbBFdtnaMSEppYOeec21zrmURxci5VxWe7P5g5n6XfWceQZP8MgeJ6EkA9gP4GuufiGgM4LUAng3gAgAXE9EFRbOfAHBfCOFxAF4D4Deq/70AsZ7aEwE8C8DriGhs8PyXAB6NmOvzTwBc4hjbtkmDuXJgABfqayItZAHJ1gOl5IFMc7BhKUYS++mPEXWsD3Sp7+Zk2zrcDdwBnnz1LGy4Ex9j5r7m6TyYSp5MjoZ+2PkOJrmfzWciZBo86kyusJ2PR7aStorKFN8RB9WfjLG+yV+5nWv1Kd+ou5arv7PCk7H8ihOVE56sJT2RxiTkGzmqwGsAoJPxhs03ENEZAF4O4HIAn0ElEAx6CoADIYSbQgjriAf/RUWbiwC8tfr5XQCeTnFFXQTgkhDCWgjhZgAHKn4az/8bwCtCCHMACCHc6ejjtklzN3Q1tkUEZRXoaqufcnkTSbPMrQ9OIErl//sg6UqEWHq+q2SJkG8EcDyZeIrqwrC1ejdCTNHAWStJTALN28kJuK4DvC+8mT3s/FB9ToBsFZVp5hu5rF50lREm30hKwAUkN5s9933dq5zSJPEs2wGcIngSWjQhhDeFEO4LIfxtCOHrQwhfA+AvHLwfBeCW7Pdbq8/YNiGEDQCHAJyl/K/G87EA/i8iuoaI/oKIzuc6RUQvqtpcc9dddzmGoZPkbpCCstLmLEuWsDwDHxiUNMtRsTkjT5/GBLQ3J6cxAcklxh9ibHkT1wGuHYx5u36xj36JkF3XIQcG6Ag5JSjLteXms5t1zgek+X4GIXjsi2fEfjqh+h4Fo48rMss36vDMWsv5Rn7hyc69lm/Elgpyzr0rCdSviPUHjNjKyKJJFTRE9B1E9Bwi+prq9ycT0TsAfGRXetePZgCOhxAuRLza4C1coxDCG0IIF4YQLjz77LO3/VDRB80FZZkYjRaU5WMPXaGwvjlvbU4u2Jg2Z7effBIo03KowwAAIABJREFU0N5IXDwjPp+LfaDTVqvjxWv1fqHA52iUPLngNXMwKXkKbOzDE/fZtpWEqp8enr64j1541YbqswgxhqeUb8RB9TmhwLlhU1+9SaDA1isT9Fl34tz3yQ1y9VOBljPnCIek22WDRhY0RPRbiIf1DwN4DxH9OoC/AnAlANZaKOg2xJhJonOqz9g2RDRBjP/co/yvxvNWAP+9+vndAJ7s6OO2SYP4smgR0c3VtF0aR9eAL3jc1Ww5jSm1ZTdnZ0zdjcRpqw3PHol7jAAZjbh5ktBH9uYUIb49tHrX3PcMyvqBGNtLwHUFpNmSJbLLlkOIbczb9xtx1jkRieuO0/7L53PvSBwTq/3789zcyoiaG+SYe1V4OdaItyqDojiU87lo0vJovhfAt4QQjlcxmlsAPCmE8EUn76sBnE9E5yEKg+cB+NGizeUAXgDgYwCeA+ADIYRARJcDeAcRvRrAIxEF21WI70bi+T8AfBdiden/DcA/OPu5Lep1ME2bzTmpcjS4BUJEwuHAu1ri8+dYXWqenfi0n88fYvLm5DTLIvbBuc7YHA25YCTnQtl5rX6M+4+f6LQD/Fr9tiG+jFBw5bz0gfiCP8CPn2jnG3EH+ES4fEyKJQExNnlKtZa5dySNSaqflnjm7QAISpsDbdg3z81ZzJXlCX7uO/lGSDxzxUG2ztl4o6diuqA4cHO/aNIEzfEQwnEACCHcR0Sf7yFkEELYIKKXAHgvgDGAt4QQbiCiVwC4JoRwOYA3A3g7ER0AcC+qmzurdu9EBB5sAHhxCGETADie1SNfBeCPiOhnATwA4IXevm6HJqPKNcBCCLuHMtDenJwZDfCLSbobAyi1QN7NNWM0IS7YyGVe6xaNA2bao7zJbDpSLinjNpIPIearDKBUUGbmSQIDlO14np2mrOCu27H95ILc3bGn+42WJlT0s6u4eIRsnvF/SlXtQFzLAmCkPJS5eZKsc74iht7PnFh4tTL3noz/Eqqf2t57xIbqa5W7fYqY4rLlvAgnkUXz9ZVlkei8/PcQwg9YzEMIVwC4ovjsV7KfjwN4rvC/rwTwSg/P6vODiFbYrlLjGiiRT7wPGCg3Z8WnbCuUN+HM/cQza1b1rduWy4/hfMCRJ+M6c2iWYA7bPki65ckY9x0pqx34wQAckk71vzvcXFxpFzVOwVpzHrh6c/lYbX3UsN2mbV2KxIEizMeU7jfqpzh0B89ZCuJaFpURhyU9T/1k1h2XayX2kykZw8x9eb+RVD+t7Gds2yV3vFG7L6oUsk6ovjR2bu4XTZqgKaHIv73Ijuxlkg6xElOvCQXWLePB1DOLiTOjm34y7ptOjITTLLvum9T2gbWua8Djf0995Q4mz6VWer5RyZPLPdD87x7XGVcdl4klKeVNpHlqXT4mxSmErHcOHZeef9rytGhrrxFOyHK1wSSXLZdvBAbezOUbcbkxqS2riDHtgNI659fybDLGXSfal4/pioONZOMSRjnrXLwFFtK9PRIYILO8eih3iyatqOaHdrMje5n465R5lBLQ3pycGZ3aelFKQHsjiQgxIfYhWzSbrXZcP2eTMe5+oF3p2et/B2R3h/dSq7KfAO9/17KpPf53KTs+XT6WhDU3TyPh6mPJfQNEKHTqC+eKjH0V1l1nPrsJhtIake434gQS4FdGXFB9xUrieB462l53UhwNaKNCOXdYaismKjMVFLj7jVwAHNZ1JlVQEKxzZyFboi5PYPddZ8NNmTtAfH4MfygDBUJMcDdIGcWSZpkvetGvLWj1XQ2U0SyVoCybnV72U7zQzFdBgb81tF8ipCcZr6l2YM99OsTyageSi5HXbPn6afH5NsQ3Zvz7LMQOT8l1xmT88/XTumtEBgNwiZA8KhMQBCIDgunA/zmoPmclKcrdtu52Aq/ciZU7Ws/WKlJ0eXbyjRjrvE5pcKRJLJoGQbMDJN2cKecJ2O4GNhESvKsD4F0DvP/foVmy2mrqZ7ctf/Wv1E/HAc7WhKuez2Roc1ooN/dlvhF3gEv5RqxWzwavZa2eEwry3HOu0LItI7wY9w0HhdZctlxScRe227UUZGWE18BF69y57vpB9Tl3ddlWrjPnvSSNh+pLKMKmrZjSwFqT3dJPmuXnQcQumgZBswPEbU7J5AVKM96vXWnBzjXG3cDHPuzAIKdZqkFZV9kM5WBy8OSEgrw5NQ18i0i6OZgxca5QiSfn5kKHKZcbJCLEvIF75rCdC4vEezCxCk7qJbNGt640JZ6229ArZGUrqSeay+nmKvONuDUipTRIUP3umCo+bB6PrQgumsz7aIjoz9AFVBwCcA2A/z9BoL+aSSqrL2qrbHkTeyPx5ShkK8lTLgZC2Yyyn6KmLqDj+iTjcRupk2/UY3PGs1Y+cJp8I9kdyKGUXPlGPQ6x8v6S1K7Ds35+0VYob9LPSuoe9iXyiq+fxvHUFAcbqs+vZSHeKFVl2Oa6k/KN8rZTId8oKjiS4pClNCDx9AlPVwUFUWEVrhQ4CS2amxDzUt5Yfd2PeB/N46vfv+rJG5TlNpJW2sVbNiPy9FlJnKbucWFwZWWAFDy2fcDN5WNMLEnU6n0xIr68iT0m1S3TZ+4dMbfelsIWD3A2O107mIp+8jd88mVQujzB8mTRcULsocOz+u5Zy9KzI08fVD/lGzVt5RjRltddj/0p3UTa5QmWp3QL7C7LGdcNm98ZQvjW7Pc/I6KrQwjfSkQ3iP/1VUTiIVK267FARE19Wwcoj+bStP/m2bKmXl59LPmAS4tKgplq+UYuH7TBs3l+4umxPvj5jDwZOK5HKAhliqR+cuuJu/p4W24uIZ7SheozB3j13XezrOI6c1vnRb4RA9WvLx9zgCvYfCOmnTimvnue4+mE6keeW113u+8681g0pxDR16Vfqp9PqX5d5//lq4vEzenZSNV3X3kTOTDIaf9SiY2y/H95KHP3kmvumzimdltuIZfzJMYz2KKeqS03pp22Jp1z3xOO2+8AdwhE7wHeK9fKCdXXCq+WWrVk9Xa0bw5WL1heAuqNOz5LxUF6R3yVDWUt96qg0BUKXQg+DxzgrtEAeEuaG5MHqr9o8lg0Pw/gw0T0BcT3eB6AnyKifWjukvmqJq7EBl+yRN5IfPYvD/Ft85QPZa6fIbTvJZdKhpdjavopH+DpXnLOzcTxFOunqZuT42lbH1pZHU/ODReU5RLipKDsbDLC3Q/YBzhX3kQ8wKX8LVc/JdeZz0JsLjRzHHaTbr4Rd4Bz+UaSIsblG0lXFJd5WVL9tJxn3lZcy2z9trKdpjSV/XTmuWk8GcDIoWNdq/ekAwOEEK6o7nZ5QvXRjRkA4HcW1rM9RL2hlh6ffnYveR6YLA9l/l5yyYxuNNu0AQJ4qdTRwMUYjaCBO9wNml8ZaCPEJItGzjfihReXo9EV8iPcX5YiCd2GXP02yf/O3pzJPFtzhZY0UyootHgqlhf3fDbfSDjoWWVEcAeub86xPBrXbdkDvIBsz4XgYL6X9mNa95M7P2fFXUSa2zCOqd2W5Snk3EixLD53rusO7MLVZet8jVXaiudPRrjTgY5bNHnhzf8L4rXK3wTgR4joXyyuS3uP2HvJGa2BL4QomNGMa4BDiAH9XQOtDcK4bwBGKCDx5IVXeeDxmuUY3qAsUMwTd4KCR9JJtzcCPOLPAy3n5p5PrhR4iug8SSB6XGf9CmBypV04np18I+Zg4vKNtHgjwAhPQRnhLp0TlbbO8zk314hX7hxQaE65q3m6ABucu1wBA7iukuiPpGvxFBTBRZIH3vx2xNsrPwkg9TgAeNsC+7WnKN1LvjkPGNelSAImnGug2JxaaXUgLqa8FImoXXHavyPIL2UJdzRLCSkj8WQPkRGb8S4e4J24D8dzjINlKRIm7qTlG3liH9KlVoAT3iyi87rjiTx9meyeOlapFIk3IA1E4bm61BwP0tzzsH77sOWEbGxbHuAST15pk/vpg0wD3eTnPnGf7QkFwQ3MuMO4fsYx2coIcBK6zgBcCOCCEASVcqD6xa9vzLGylFxSMvKqXQ4jfhfLcRRBfk4olBniFqqm1MD5fo4Fc9/BUwnKtoUX1H6WSYsezbJu67CS5JpwfM0racPz77M7Js51Jmn/XiuJyzcSrQ+un4JWv5blG2lzv8bFkph2QDdG5DnAZQWHca8yUP34fGEtiwpOG6wjKU15MdnGaejbx3xbzg3MX3XO9VMakyeZfNHkcZ19GsDDF92RvUySZiujVRwQX8HdIGpsznIUkWf7wHFplkg8fa4BkadTC+P6yR5M23E3VN9dlgL6lepxVeNm5p67+tjS6kuXmGvdGdZkuZ7Eue+j1Zc8PXE8GDyLdScpYl6kZ+S54H0s7U8nEEPnyVnnXBJoZ0gLJY9F8zAAnyGiqwDUNbQ999F8tRCvgcttWYSYYyNxQdnYVkCIOcubeA4RKSjLI+m2eYiIQpYZu5jzIgnErcU+uEut9PIm3X6W+Ubc3HNXH2tJvWlM+6p8I1Gr71gK6XntdlL5f3nu+xzgTmWEcXP5hIKgiBX5RlpCM9Ddxx4Fx3aBc5a8Y90x1mSD+LOtyYSOK+83OhldZ7+66E7sdeI3ktdS6LORtgcG6IOqWZ52rz7m+8lbSbyrpTiYqh89AVSpbIYYlGXaAfzmZPvZ2ZzdeWLzjRy++nZiqzBPrOtMUhxKV4//AOcScFM/E6lzz9VkcygOmiKWu6S0dhxPed15wADbsfjl2FzZz77XKbgUMcWi4VMaOkNaKHngzR/ajY7sZeJfvBQQ591cmraat5W0q/xe8n4byRf76ON/FwOoRbBTutSKvWdmu+4b6fIxri2zObmgbPN8X6mc+Pwm30hcI2W+kaWBu+ZemCemXYcn0w7o5n1IPCUIPJ+/Vd5vZBzgDJKu28+eyohDyJb5Rn3chhZUv5PSUHSgzjfyQPUzqztfy7sNOxNjNET04er7YSK6P/s6TET3714XT37qJRTEBerbSC5ttccB3r+fJU+//13anL6injJkOm3O1A5MW25zSm4ZDoYuKw58gmHXJeU/wL1CQap5JSETuUPZkywrI8RK/79lnXsQYjxkmksCjTx9UH0PuEK630h6R2uF5RPb2mtJhOoLMTf+fQrXsgt7qQ1E4ed+kaTdsPnU6vupu9edvUnswaRkKefVceWMd6kcBvd8AQww6rbjefL9dPnfOStJy9BmM97bbZfGTHIlc6lVHFOzOZenY9HySm15GHjZz2xzrkyrMSlz73BhNP20535JqKDgLW/Czn2nzlzqZ7udhPgrofocT+0WVqCbgCspI+xlamICbhlvlPppgyu4S++4+mmpbZ7SIO1jom5Kg7aWgHZKgxh3EpB0Htf2g+E6cyVsEtGYiB5JRF+Xvhbdsb1EXO0hKShbHkza/SUABwnlD3D+ml7bSpJyXspsaq2OVbefspWUSpEAsmaZ8o1KS0HSLIFGKEmWV+yrkDDqDPKzrh5x7vl++vKNxh3YLjcmLjdIA2K4rjPm8o2YZzf97AHVLy1EESHGuFdFt6EHiuwDV3D5RhoYAGiEp7SPgeQK9UH1yzFJe763a7tAe550YAAi+mkA/xHAVwCkkQUAT15gv/YUsZd6AeCWaJl70KQnldoVg+kXTlsJzdUtrcIfIhxJsaSSuHvJtSA3EDXGlaWxaNEA6JTjmIsw02Yj7cdU5zktk2XloGzi2RpTh+MWXIyuA7w8bCueDhSheIBPRviKK+7DH3Y86qx7gAJeK8mXBJqErAfiO5dcfEW+kaTgpHyjUtBZQmF1SY7NNWNyQPUFtKUHRSiNSZqnXTZoXKizfwvgG0II9yy6M3uVpGxuz0ZqXFICz06cguHpdN/IkFB+I6V7yWO1A959U29Oh185f/7K0rgevDtOIYy9HFPsl8DTheaSNrygWXp49ipvMsY9WUDcBgM4tXrnNQGRp61kSLEkGapfPF+cTy7A325b5xt5svgzS/6U8UhE3DVjKlFnHM/2mKT5rHk64o1yvFPq59ZcZxIAaJHkcZ3dgnij5kACidnxUrCTyxIWhILX3cBnHrcpXX3cqTagbCSXa6Bwy8hjb2fSS3Gf1LYUHurmLFxnHkgqlAB/3s/YVHFz9UCIrXkORqkqg4unZs35stMjTx9U31U/TQBXSEpTyjdK44k82+1SvpEvi79tyUvgivT8bhxPtmjqtSxA9VPbjntVW3f1WpbjjVJVBrmftpW0SPJYNDcB+Bsieg/aCZuvXliv9hixG0lKSNtOzouCquFcA5z1wZncklskjSm6BniezZjKQ0RzNyThxR/Kqa23fhrLs9uUTVrU/O+uZMAi36iXZsm0S2Pql2vltKQdgBERrq5Y50lDlg5wKd/ImvvpeCTG0fgxGW6ujS0oIxI6blJaNLzbkOsnvFaS2s8xjqxtdD5357mdLPDmjL4E4H0AlgCcmn0NVJGcPMa3TZsztQO6L567l1wLDALNRkrB9u0d4O0xqWiuTuKe4YPecGiWzMHoEQqmC8Pjf2eQdDsWlO0Ehe1+9r0gzuOKlO/NkQ4mnmd+9bEpFMpyMY6AuATVB1KMyNfPyNOj4HRjRL79AaWfXTCCax8r/ezcFyXsea5yh6Q4LJJUi4aIxgAeH0J4/i71Z0+S6AcVFlPanEsTEl0tALORpHbZoj9lNhG82k3bvpplaif3s2slaWCA5hCJ5LL8lJIlQK6t8tp/anvvEY//ncvm3mZQtg/AQMyj8VhJkvBsX30sobmkq48lyHQa09JkZGj13VL9LMRX0uo9V1kI8cYS1q8pI1y+kYT0jDwLN5eATGylNAhKqNjPbtOqnxxgpE1SfGy382hUiyaEsAngMUS0tEv92ZPEb05DKGxsQQOX/O/lAjXdDb6SJZFnoVl6XD0SZLp0cylqYPdWRN/mlEqWRJ7d+lQSSijnGcckHSI8ZNp3nbIMmeYC4tztonw/Oyw79xtJLtvU121p9cJ78mn1ZWyQjyWlMfVzrzqUESbfSHVzFfvYCwDS442lkOX3PJcwKuZadVIadlfSeGM0HyGiywEcSR8OMZo2cbEX6WAC4mI6bXnaExYp30oYeZbaFdO2s5EkoeB3c3GlSLyxD4nn8rRdiiSIQdkieK0J2XJzCnVtys1pBmUZiK/I0wOaKK4+lgqvsvlGhvBMVx9LhVdjX7limTrP+GyZZxewwjbrxDvr2RQECJfxL/fTVkaWp+37jcT6aZOSpyxly3wjuZBtGwwgraXUls0NEizpDlhH5LwY8giaL1RfIwyxGZG4ekrsoVzcS665pMp7ySUrqbfJ7Vh0pbZsWUn5veSS+6asoKAFO8vbG6WgbKefivtmVhxMUlB2Vhwi+jvqXlPAtW3ee/E+FbdMyjeCoozMJl00mbSWgCbfSOon0BWeEsCgO/eRpDl1ZccXFo21RnyuszLfSHFzCVUZuv0sgBiaIsZVZdD2cbGefPHG1FaypG3X9iLJU1Tz13ajI3udvBtJCl6LdbycOS8AszlZk7t7SZpmxq8VrjPe1dK+l9xyN3TcIjsQlC21VTko6wvwA753VOYbSW4ZNt9Igfim568sjbfgluHbtcakHOAsEEPlaccpePdq13PfsWhUa7Kbb+TrZ/zcjRAT4kORp2+N+KD6vHIn3+1kQ/X5+41OMjAAABDR2QB+EcATASynz0MI//sC+7XniCuZLgVlgebF94b4ss9ubyQV4jsd477cNSAd4ELsw4eS0v3vPoQYkwSqCgXvYWcfoCnfyDufaUz7ZhNd0DHIL08cr0EU2WOyhadnTF2tXkdJOVGEPXKtXGuEBaHYPCXEXRqTaz4La07dx5yC4xCIFlQ/v99IQhty9xs9GK4zD7z5jwB8DsB5AH4NwBcBXO1hTkTPIqIbiegAEb2U+fuMiC6t/n4lEZ2b/e1l1ec3EtEze/D8r0T0gKd/O0nuMhM9Yh983Efh6T3AS4iv6xDRYkkMzNTjf1eDshyiiBuPpFnybVO+UWrLPbvMN9IRd31cjP3Lm+Rj8lhpWtyH76fEs5h7j1bfx0pS2kWezpwXRw0vGdjCj6kX0rMAV4hCdqNJaZDOhjqlwQnVj2NqXMaSO8w794skj6A5K4TwZgAnQggfCiH8OADTmqmg0a8F8GwAFwC4mIguKJr9BID7QgiPA/AaAL9R/e8FAJ6HaEU9C8DrqsKeKk8iuhDAGY4x7Ti5EWKiUHBsJKVd5OnR6p0HkwBFFjenq59+hBiXb8TxLPON9KBsN09B2m753OuIu+LAsRL3es391g7wPuvOU5VB1uoFDdxrnQvt8n5a1mQHmci1E4SsyxUpuqv7uc5CiPcbxbZyUcs8pcGC6sfnN4JOgiyXaRLS3C+SPIImRXlvJ6LvJaJvAXCm4/+eAuBACOGmEMI6gEsAXFS0uQjAW6uf3wXg6RR3yUUALgkhrIUQbgZwoOIn8qyE0G8huvl2nbgyMNpG6pSZEBZ9twQN064ow25ZSb5SOWVAXA+guvpZIsSEjPfYtsk3ii3lzdHaSMYhArQPHJFnNveq9i/FiCTUmwsh1i5vIl1qldp6wAAiT4bidQoerb6IDVafe60kXXD7QCjdKtMyz+7YeZdYfr+RpIyklIa1Ikmaf0fl8411t2ErYuXVCxJUH2jubEokzf0iySNofp2I9gP4eQC/AOBNAH7W8X+PQqyTlujW6jO2TQhhA7Gm2lnK/2o8XwLg8hDC7VqniOhFRHQNEV1z1113OYbhI760C9NO2EjeoKzL3WAc4N6722M//QHU3DXAuVrqfCNn3Cc+3ycUXP73jmZruRt87pvIMwk6RSCyc8+0EwqFekvLeIRsLZA5wEhv67wM3DPKiBOqX1pJKlS/jONZiDuHRdPJNxKenfrqCfJzSajquusoTRzPrjtQ6meOetPAFYskD+rsz6sfDwH4rsV2Z2tERI8E8FwAT7PahhDeAOANAHDhhRcqOkM/2ra7geW5RXdD9S++wH1PMIAQ5E+ugdlkLI498S3979LBlJ6f8o1Entnm1Pzv3c0pHyL55tTL7xSBeyXQWkLLxSTUjotRsybb+Ua9y5uw/XRC9QWUlAsyLbTr8Kw+l4RSnm8kxT7KfCPLSgKafCNNGcmFpx6473ox5DUy6riBve5VjyKmvaNFkmnRENHjiej9RPTp6vcnE9HLHbxvA/Do7Pdzqs/YNkQ0AbAfwD3K/0qffwuAxwE4QERfBLBKRAccfdwx6m5O3d3gefGd2xuFdmK1AeEAb119LBxM5dXHVhJoHJPjAM80cAsyDeTwatkHnW9Oy/KK/Wzair7yfHNWn7mC7GZQ1lemKPL0BJqdB3iPMiy5+ya13S5U32udl/cb1e9TsLyA3PrQYx8lVF+aT6CwpFmO7bn3uFddQiFLk1CFbCffSFPERq19BJycYIA3AngZqlhNCOE6xEC9RVcDOJ+IzqtK2DwPwOVFm8sBvKD6+TkAPhDiiXY5gOdVqLTzAJwP4CqJZwjhPSGEh4cQzg0hnAvgaAUw2DXiNqdVGSC1A+TN2fW/85sz8vTEfYry5kK72NeR+2AC2oeDfNiO3JZX7GdmKYiHSDNPWskSzlevHyL+oGwez9GCsn1yrcrYhydwb7lXOwe4iPizS5Y0sUEHGGAyrvONap7dZkj5Rh2eTFvOku+j1bNIOgY4oCojzncUeToE4pQBAwj5cC2e2rrL+qnN5yLJUxlgNYRwVbHQuvWpCwohbBDRSwC8F8AYwFtCCDcQ0SsAXBNCuBzAmwG8vbI+7kUlwKp27wTwmepZL67qroHj6RzrQokrlb/doOzytLiXXNCu6s3piX0U95KrB3grMGkf4LkbQT7Axx3Nkg3KMv5/VSgUFo1W3iTXGLWD6d4j69V4hAe3+mkHZTmhwKPjJNcZx7Mdp7DuJfG7bO28iyYZ0AFCyYADq0sTv6un+sxnyVvu1S2uZUfMzXpHeT819yofb+TbxX76FLEH23XmETR3E9FjUb1zInoOADXgniiEcAWAK4rPfiX7+ThibIX731cCeKWHJ9PmFE//dpJmzs3ZJAO2g7Kau6PenJAXU7vMhRaUZYLX0pgYNJdnI0ntyn6qQVmmvInMs2dQtgUwcMyn4tMvy5tI9dMannbGu1Sqh+dZjF2K+wiuMwkl1brfSJinqOBwQkG3uleXZIs/tXVVRWbmSV93bfcRXwaGjxHxPLsHuNSuxdNYd+l+o6BsOs5K0hQxT3HcRZJH0LwYMXj+BCK6DcDNAIZrAwri7iWXkgFbh62BEAOyzRl4lFJs2w2yu1wDMExuj4tvq+4GR1C2ZdEo7oa7H4hGts/V4rGSugADPU7RCGSPC8NCG0aetvBM1kdT/p9v17n62OEOzK8+9mj1FlQ/H5M5985qA22e/JqXeHorKGturmPrbeXOmzCqW3MeqH431runwQBVzsozAJwN4AkhhKcC+MGF92yPEZsM6FhMLohvyy2j8CyvXdZcA5lLzBVPcR0izkTIPkHZfCOJPL1Z/P2CsltzYehBWV/JksLNpV1kN2lfPqbFPth15xhTENqltj7X2dbm3rqeot1P39xb1biBHNav5W9tbR97hUKffWxC9TvvaHcljQcMAAAIIRwJIRyufv25BfVnzxK7mKS2nM9URR95EE1dK0lHvWXuBrWfNvKJg7m6DhHN8mLcXLsflC0OJo+2ah4iEfGnhJKaq487KEKeJ9A/38jnkvLOvS9/K/LsGxBHxZNp18m5MSwaZ5WLyNMXx+taXh6L3xKIDqh+H6RnVsxVs84XSW5BU9Aud/Pkpw6ay7AU1jwlSwpUj3WIuC616oUQGzNZ3/JGysek83QgxJh8ox0LyjotxByZB1jvKHdFymMHYr6RBtutn++s9wW0q2yrEF/PmDh0nnKI1SAUBUXI5htJPJ1afXmlgGZNzibc/lDmM9vH0nzOpl3ItObmyudehOpn1SNc+9ihiM2mMd8ohKBaXosXVqbfAAAgAElEQVSkrQoaLUb2VUnbNY+10hVtK0k6RLqoGh7N1cfFt/MHuB8yzYEB5EOk6zrzuBsUnlm+kVY/rcw30tyGuRaqoQ1jX32Je93kToiT73XLcMABVwWF6jNfHM9blaGHdS60S23LS8pY65xJVNaVkR5C1oMQm/qg+qVFY7mrAbTy53Y7j0YEAxDRYfAChQCsLKxHe5S4zalpGDUkVXnx3M2ZWtJifi955Gn3U2rX9LONvHIFr4OuqfvuWe8XPO7UjuPGw+QbaQcTUGmhimYZ+zpyKxhAtCZTX8Q14kRzlZotDM22M/es27LfPHUVHJnnmstl29xv5Il9uHKt2DieMp8eIZu7lh2xpDVP3CdLadDijWVKg9VPIO6l3UabJRIFTQhhuE2zB3F5CtpG6hVodrgblqdj3HV4rX42YKG5HMFrNhGyS+XVx/MQMFEWfSebmh1PuTl1bXV9M5Yi0aykJhnQ4X/PNqem/cfn5+4OXVOPPOc4ZVnW1NOY1jwHfU/N1lV4lXOvijzHuK/KN9Ih8F0Xo8e9mph67jfSoPp5vlEvK8kpZLX5LFMarPkE4jxpAjG1bVuIOs/jJ+ZYqX4+GYtqDuQgPvYht/VURS7vJbcS0koftHqAu4LXzEbyxH2EdjVPR+A+bc41h4WYxxQ0/3tqu9ayEG2hoAVl6zE53TcAqgKk8TPtwOlzMLmC1240V7ekkRz34WDgmlCwrXNu3fFQ/a7rTK7KwFiI7LN7gHqyfCNNKHTuN9JimNmYNFRmfH4bNGG5zo6f2NxzYICBCuKylF2omuozHSvvQNWwiZDbPES2nEejH+CpFIkG2603pyeWlKGkrBIbbVePc3MarrMSSacdyjVPJJ5anMKzlrgD3LHu1EAzYyU5FAfdzdUPiNEL4ptb58ZaDiFkwqvbtrzfyNrHQLTkrQO8nHst1yqNSROy9ZicqMzYz03Vg7JIGgTNDlE3OAfxbc4mvpwXNtAroWqcORoldHWuqEI5LFLzv5f3kuv3aOTaVSRp0XdiH2K7ZiNplld8flt4agCDxFMLypb9VMEA2QGuvaP4/FFrPrV2kacDiMHmBnkUBx9PDeLbjWEqVZEnW4T4qv1s7jeylJHW3Cv102ace9Wx7lR03JRTmjRrMhey/LNzxUHzoCySBkGzQ8QFxPskj7Gos+IQgZZR3HJhePrZZB9rmmW6l9wKTJbWjx2ncPignUKhtZGMEhtlJrtvc6aBCm2Lubd95c3YJeq6zmxttSZt7J1bQ7V++qD6vgoK3fuNVFTmRmF9MG07+UYGGCA+P1NwtOc7k0CB9D4NodCKEXlig3NHPwuXLd9MsKSFxguiQdDsEPEJaVLbdgFOwLokzWdy15ePeVwDrthHLuhk/3v+/NRRzYWQxqQFZVNbl1CYMMKLb9pxN3hjH4C/rI4H/aOh+CLPdgJuH221T5kiF7TcEgoeqD5rJcnzGULMN9Kg+t3ne+Zp09Tq29cpexQHj1DoqYg5wACzUhlR2sV+NlbSbifSDIJmh4i7KMy3OR1BWQ9CbDrqbE5ZwynyPhwbyXYNFK4e6dnTrnbl8dV7hUI/K0nX1BNPX9zHUz+tEbKW66ysyuCxkjzouG4ZFsXNlSPERFdPdr+RwpO738h2BzbKmEcZic8WeNaAlXl/ZcThOrOE16yYe9u9mltJ0vPbSDrXfA4xmr1NZTa1WmYiu5dcW/TlveQet8zahk+7WvNo9VnWu+Z/b3jalQHqitQn7Oz4PN9IC8rOGNeZnvNiW4hN5WzdbZj6ueY4mHJ0nh2fGjvXUt5PI+6TV6RQxpQOpjY6TxpTlgxYfebKN1LatebJsPzKTHobmdggr9Rk3VZVZF0Zaa0RfkjtqgyeNeLdx85SOZGnDUJZFA2CZoeIKy+uJUIC5eZ0aODKRpoxWr2OJtvhA7xAvdmuM4elkOcbKUFZFlXjtCbN2IcHITYZu0rAzBi3oZzY2r69UTu8u/3k2+ZXH2tjKq8+Vq3zTHFoUISaMtJn7h3WeZZv5EMm5hB4hWcuvBTUF9BWHHRYvQ8uXvN07GNPqZw8pcGypBdFg6DZIWLvJbe0Kw/6KBMKaub1hFlMQtvWveRzuSHrlnH76v0HuG55OTTLlpVkBWXjYQvYOUSJp+nmckOmm0PEgpnm7hst4z2/+lhzw7aev+Fzr7a0+j5BdnFMI3f+VuynjSLsxj7kdoDTxehEiLUUHAWqn8bkhYvHfnpd4P5158nfWhQNgmYHKW1OOyjLJO5p2lVe2sWD5kLiqWngviA3kDa8dYiNnK4WRiioPD0uqexgqj7TNNudDsp2YkmuPBorIN3kG2lggPp+I4eLj8sNch3gjrl3WdIZrF+dewZ55QNi+Koy9AKhGO3KflpIutjPHUJlFkK2z9kwuM72MKUX79GYAC8ssn3ftwsB02MjWaVyGp5pTPLzfaXqmcNO4+lEPjU8ba2+FUBVnt3hKbb1BWXzfKO+QkE7FppkxIqnQwN3zVMfeHVeKFQVXh5oeTeHSaKtZMf3tc49ikPvnBexnwxP5z6W4feZlfQgZWwOgmYHKc8+BpywSMeB48LKb6e8iecAd2jgrtyDSe5qSZafxtNR/p6ZT295E63SQs2z+kyLU9T5Rkq7PN/ILRRObKqaenp++x0J7QoNXFNqy/wY0zpvWcgazx5xvBwGLsWyJj21+o0Gqu9R7uASChmay7GPrXaxnzaSrkxpkNrV+UYOL8aiaBA0O0hpMZlBWTaA69hIQa+KXPMMxgGeWx87eoA7SuXkGrBSWr3m6SooymirBsQ3lf+X2uWb0xOUTc/XgrL181tC1hhTBRjREuzqdWe9oyLfyNNPwAcGaGvgtpW0c2AAZx7NhFnL4hptK3c7so8L5U5D0dU8HesuhOp+I2UfN8+353NRNAiaHaRUWsbacDPW5BZ4esubtO46QfV8uW3f8iZ9cjT0C6i4EjRC207ugTWfDohvVorE0urTPHneUXq+FpRt84TRz/Ya0SyaVN7EUyqnxVPpZ1vBcZRMcUB826VlfOvOsiZbPJW5n/VQRjrlYqx9vGG7uWZZSoM29ymlwVumKI5prqIygUZpszwoi6JB0OwgJdeA4VZua6uWyV2WlnG5uSqersQ9mSnv0+efPyuC7HY/HbGkyahupx1M9eZ0lhdpxiQrA6mtC7BRlPgweTpLltQ8zRjNqLZ8fP20XWczp1Y/m3TXnX4PU48ikJ48mjyHSbPOWSCGMKYSHcc3ax30ppDNUxoU5S711SVkW/BqWRmIfR273tGiaBA0O0hlaZkdQYi1/MXOysDpEHO4BuDyv3tRNU39NNk14A/K5ptTC8oCzea0tfrCUhA5ZkLBTPDL4biy+wZoNHBLeM2mpVBQDpEy7qMcyoBPyHaKZTrWsqfaQXm/Ed9PpoKC0LYVEFeg+s078rkY88vHpHlKKQ1rzn0MNILOep8upGch6CzFYcijeYhQx4Xh8e1aB1OZkCa0Y60kpZ/NtbY7g+ZK95LP57proJ1vZAdlgSpD3RAKdb6RaX20s949c+8BVwDZhlf6uVwfIqmfuuKw5lgjKRnQg45r8VT7OWpXGzB5ZsVHlbZ1QNqx7vIDXEeyNUFuzWVLVNT7stbdRnJbSiNKVnefPW+7V5Pw9ORaAU2isgswYryjRdEgaHaQlqcjrDs0prK8iRXEy28ltCoDrGUa+HYziutSJB4tsNpI65uO4PWkfYBrQVmgSQa0AuLeDG3AGRCftIWCt7yJ/j6rsTvBAM0aEVlmJY1SPyWe7YoU5rrrAYSor5I22roQYq3io8ZanuSXj8nWeXO7au5x4NuWlwPq625cXPdtKA4bm6rF3/D0FbJt+imPPbZNY9d5LooGQbOD1OTReDV1b1DWn1Gc3EyArl0l14CmXeX3kvfJDbKC12kjeXkmk9/m6cnib2fH+zRLO5YEeIOy41oD9fSzcfHZcR9vZYDjJ2Imux089s9nnqOhrbt00Gtzn/KNWmtZ6SeQu1e1MfWcp40tzL3SDmiQdB5ouWcfN/30jl3nuSgaBM0Okjso2zufwS6Amd9L7s3RaGIK9kayczT6uY98VyQXMQWV56gYu9wu9nPT9Gs3Y4evn46gbOPC8CkjTTxF62cbVu8RiAHGfJYBabGd/yK7tvUh97Odb+QXnv65N/o59cc+Smi5y12utOP76bHOHQLRURx3UTQImh2kzgJxum+sgz5tTo1nfi+5eYCXB85ObyRzTG2h4MpT8PigW6gza+7nZlC2lRDn5blTQqFQRly5OT3XnZ130WjqUv5WfvmY182Wrj725H00rlCpXTs2KblhU9t2vpFtnbvnvu6nTyh45t4sZVWgCF25VobLdlE0CJodpC50laf8XnJPUBYAjlVWzc4IhULQuTShPhq4HbzuE5T1uRuKhDRzc27uYFC2jaTrBW92HHZ2GRbfuuskQhprKeUbWeSe+zKXxYg7efO3Wjx7uRjldg1PY54m5VqWeOZIOrVpZ93JQIhyPm1rzkqSXhQNgmYHKW3O9U0b0VTniFjtqkVfCxrHRnILBQ+iyYnp77gDZZatuBPgLEWitEttWyVLRM2y8WubQdka4uv36QNOyLQXUVQh6TxryfK/5/lG2rPz5x9d3wBgafUjF1Q/h5Z7AuKtW1iFkyrP47EC4ul+I0t05gm4WmmX1NYFV++RazWrraSe+1jtZ+muHlxne5bSYjq2ng5GYzE5zOiE/Gp4as9vV/HV7o4BfCZ3uZG8pTP0sY+2VEHBk5BmZvFP/FZSd+wSz7ZPX3Pf1O8Iiac09rZP31pLQIP80tdTtu6UF18/f90BA68UB08OEZDNvdw0QuAdArlTlcHaH7kyIl2416rH55tPGyGW128zBGK6osG77jZsdFwC9VjzuSgaBM0OUi83V6aFejRLn+usRJbYGriJ5nIe4OXVx76DCT6ejmxqr2bZJyib8o36uW8MgVjlG20aCK366mNHLketjJzwWCq+dTfrWNJa25ETsFFq9XpbT7JsJ9/IsPjXeqzlNcdaTkg6b0mhNeee9+zjPKXBg8r0xPEWRQsVNET0LCK6kYgOENFLmb/PiOjS6u9XEtG52d9eVn1+IxE90+JJRH9Uff5pInoLEU0XOTaOVpYmAIAja/bmXFka49iJDTPIvVItppqn1nZpjGPruckttwOilWQdDrGfdhmWlemk4am0A4DVpTGOrW+YVlKaz2PrG6ZmuZr6CYtnPnbdmqx5Gj79lAzoHTsAHF13rhHHO1ot153SeCXNvQGESP18YM12na1m685qB6CeUxdPQ6vP59Oyzlen47odIM9Tms+jrrkf42i2liVB1/TTMfd1P/V11xq7o58nNkPt1n/IgAGIaAzgtQCeDeACABcT0QX/s72rj5Hrqu6/szM7452xs3YSFyhJm6REQkFQCBaiFeIPUMtH/3CrUmFatagKQqXQUlWtSIVEKSKoQXy0KaE0KKkCpDVp+LIaSMhXAQEhrFvHJA5JNl8E4xCb+HPX3t3ZPf3j3vfendl995w7ec/j2ZyfZHn2zZlz77lf5+ve+wbILgNwmJlfBOCTAK70v70EwA4ALwHwRgCfJqKGwPMGAC8G8FIAUwDeUZdsZei2MqXgJmdsMHXbzXwwxRaGbjtbRDKesfKb/YO+hLYbTiSxnoOTc23abntg0Ct4SknZbjCRpKRsJruUlA0XESkp22030Vvh/OxHbDuuK18he8oYaTWCBTTennqeRT1ji023pR93nbztZTqgaHtJeYZ9VLaA5+MuPxsUWcCzsSwYON0+5SUpxGafUihr02wezy/Jbd9pN3FyaTn3estoJxsTaDUmgv6UlbymP+tAnR7NqwDMMvOjzLwIYCeA7QM02wFc7z/fBOD15EbKdgA7mXmBmR8DMOv5lfJk5q+zB4B7AJxXo2xrouMH0wnV5GxgfkFnifTxjFB3soVJyH102oF1JYQGOq0m5hd6olVfLCI9MTFZTM44woVJSsp2Wk2cWpJDUo0JwobJCW9Zxhfl9LbvibmHTCbVGGkHylOxgBf1jNE2ggVcvzDFkCtEYQdjrhQynqIx0lNY9c2cpzSeBo2RstKzeTy/2Iven1bUU3FvX3MCEwTML8ht3x3weqP91G6oxsgqg3UdnaN5IYAng79/6p+tScPMPQBHAZwT+a3I04fM/hjALWtViojeSUQzRDRz8ODBRJHi6CaEG7qtJub8ohxLynYHlFfUCm17nhJdEGqRLdsG5jRWYGZVayzwViOXHShPyraaE5hsEOYW5NDZoFWvaXspKdttpba9vGEjq6dujDRcOEywVpPGnR8j0g6t1eMuttg1fR8J3nlCe3Zazb5xJ3nnc4tyO3XajXx8Op4l4dWBcLXk0SyvcL4zMnbOrW/OCwYGoBx3raabxwLdKqNpHXk0o8KnAXybmb+z1pfMfA0zb2PmbVu3bq204NyyPKWwLNtNZQih0c9T4SVJicHQo5GS131WNSIJ1GYDRM6ylBKonXYTzLo8RRFCkT0vQOspNHLLMt5Hg20veDS57PGFqY9nrPzAApfGUn89Yzy97MJVOSkLU59Hk8JTsOr7POkS2iznphl33VYTi70VLAmh0MYEYWpS56F2B+dnhDYbd1I7reIpjbs8ihA3roCwP9ePR7MfwPnB3+f5Z2vSEFETwDSAX0R+G+VJRH8PYCuAv65EgkSsjpVHaFuNwAqUB4jG5c09GmkBDSw2KXndzZKIwuScmCB0JgvvRzM5k9pJ4jnY9sKkm1MkZVPyFIW1mlhPwfuZS1iYkuopXNeyOtRSjk7LezTSYpeQb+y0my7vIng0rabLU2SeisZo08jUbQdjWeF9aPNjmrbvrJrz8fJ19dTLXgfqVDQ/BHAxEV1IRC245P6uAZpdAN7uP78FwJ0+x7ILwA6/K+1CABfD5V1KeRLROwC8AcDbmHkFI0BurS5qLBFdUja3qhd11mqWp4i5+83GBNrNiXzjgMoCVxzcy7wf6f60wvtQhCaCTRNSCKOfZylpkMtShhsW5RBGmGhW1VPTntnmDkn2dkofJcqetWc0vFvkKaSzHBOk66Nuq9Hn9coy9fwZJkUYWtv2C3Lbd1f1Zylpf9tHVt5ue3DcKTw/kU4/5+pAsy7GzNwjovcAuBVAA8B1zHw/EX0IwAwz7wJwLYDPE9EsgGfgFAc83Y0A9gHoAXg3My8DwFo8fZGfAfAEgO/7Re7LzPyhuuRbC93BUItAO5eHpMopW/4uKY0b3eceC+Oo226qkseDMkV5tpQWeJ6nWJLL9/kcMSnbGuAp5VMWenJSdiAkFU/gNvHkM/Muvq/JU2Q8JY9GszuuldJHhew6Y0Duo04r2523HOWZ5SmKPlIoT9UYcXkKeSNGWhg6a3uNp1DM+Xibzi0owtWD4VUhZHx4/iQ2TE6kzbnT7NLUpmgAtxMMwNcHnn0g+HwKwB+U/PYKAFdoePrntcqiQZan0LjRnZbLU5xcXI52OhG5Qa9yowv3WBpHfTwVuY+Uba5OdWpCA/LZIGdZ6jwfPc8GDh5f8DxLyZK2hOZeEvR9BIk23/GnUwracbfQW8HyStzpz3bnadqzCN3FxzLgE/KK8z4hT4k2z1OI9UwLQxdHBaoKrzZw6MQiBJarwqsxZLvz2s1WlC7lrFUdWI+bAUaGLE+hCg20i9CEdHjKeR+acEPhHkuusbMs5QU8rKeG1nk00o4mx/O4wmILPT9N3ue4xpsM8ymC5wNoZXfWqrxDLIFnq+HzFPH2zHbnJY87YfZ3W01VH2VK/vgp2cDp46kwRo5rEuJ5nkLahNI/lsvuT3PlZwpR2CE2EGKU2sntdqxujHT6dp3JPDVtXwdM0VSMTrupck/D0ITU6Z1WQxduCMJHGstSGxbJeGpoUw7uqa36ipOy+e4fMSmrD/EV51PkK0uIdCGMYndeT+7PMCSlCctoxl27obP+g/6UdjP18VRa9ZLsxQ412cAA9OE4zVjuDob4xHxKQohPEwYOd51F6KYSvPM6YIqmYuRnHyBv3wR04QZnLSvCDUH4SGNZ6kIY+pBUvlNHu5tLk+xsF7vzNGdJ5hRJ2dCjiVn1KeGG8BYBKRTa1/baMSL0aN+4Uy3ginGXnTeCbsPK3GJP9M47Wp7BGJG88/zMDbRnrRTh1cA71/YRoNidl4f4yum6q8LAEZ7tYPt/pOxid5485+qAKZqKkSX8AL0FLk+khs5SD6yWyniuOggZ4xnmFCJ0q86nCDw1SdlV51PiVv2ppRX0VuKLSJGn0N33BaS3vWqxXVAs4O3mEDwVnrQmvBm0veTRdEOeilsETmjCccHp+BRPWjJGsryP5nCl6qBwcPWSdGP7BGkPH2fniORweaet6886YIqmYnTbDRxX3mMFIKeN8mw1C57RspsFT4WXdFyVwAx4QpapOEejiUHrDu4VtwiU0xV5irSwjNhOQdtLdICyP4O21ySFXX/KC3hRtm7cyQt4U9VH4UFMiWcn5KlRXorQWZ6nUIbOjmtkynb8CfenZbcIaOZScYvASrSemdermfOdYM5rPFTNTtM6YIqmYoQejdqyFHqh31qtw6rWeTRy3ic891FOl+Upco8qFr5qF7vzVGGZxJ10GiswtZ0qa/sUj0Y77tphPkXiqc2n6HM03SG886p4puQp8lsEluNeUnaLgHbXWVZ+Wi5LE7pT8FS2Ux0wRVMxMvcY0IUG5hd18fecp8IKnBe2TLvym7orYCbDego8/S0C0uQs7n2Syy9yL5qcQqPgqYnVq2RqVt/2LV3b948RgWc47jShlsX4mZesnqr2TOijjrbfM+WlkD277djVs5wuy1No+jN8pYE4P5VzvrisM3GMKIwmzRjJdudJPOuAKZqKkXU8oBsgEt0q2hidX5gkOsczoI0QZ7cIODo5KVuUL1tXRfmyVa/i2Va2U1Lb69qpjy7Osr+ftLIntX2sbB2do9XJlMKz29a1Z5ankOiAQtG58mVPQUPbbQ85PxXGiCtb4Klse217uvL1Y7RqmKKpGGFnas4zODrZYtLQZrcIqHgGg14uv+npomQD9ZRow/Ir4tnStZOWzpWvayctnStfKbuSzpUfyBS73iRF9rD8CM8sT6Hh2VGOu8zrVfFUjiVA36Yd5TxeTSuHVyU6x1PXTlq6VNqqYYqmYoSDPmY3TE02cgtE9j4CnkJIKhv0cj6lESfoK9/zFL2UsJ4JHo3i3IdjmWDVK/IUruw4tJ6K1vNJodVa346nzvMb3pMuJ87yFBqe3ZR2amfjrkKeSk+6mzDuusrx1E1oe61MSR5NAm3VMEVTMTYqXe7QYpNm0saEkFhRfpyyv55KWrGeCSEUbbghQSF2le0Utqdmd15OGqHdqDQwBnnGaPt4JtQzRpvlKTQYZhHT5AlynmL5TV92wkJflVIYop6u/BhdSnvqlHw3IbSs5VkHTNFUDK0bHdJW6fJ2lGGuTkJYpqinjs7RJsTKlbmslHbS5j5SwkeaSyAdzyhLdTgwzFNUFTYEirZPCRtqvUnNBgMtz27OM0qW5ikox11XmUdz5dfgJakNseq9pDpgiqZipCT8cotNpEtP+FWZGCzqmbIwSeUPYbFVtYjUkJQN8xRpyWNdnqKqjRCAfgFP2uAwRMhWW35aH1VvjFS1GSBps47aENMrxBSZqoYpmoox1ESqaCCHtEkLk9LzqmNhAnTXm0h0jqcygZqyEUO5MPXlKRTb1XOeUcogT1Fl7kNtjAyzYUTfnmqeQscnbZrQyp6gFLS0aTka3bhrNyfQ8EKn1NM2A4w5Nk8V13VLnbm5M5lEl8YzSobNUyFPibalLDuQXWAayhSbSNNTOjpXz4A2QjfZmFAvOP1tr6OVDuD2tZMok7bthxkjKX0UJc3bPqU9paUu5ynRpdSzo2zPlHk8pWv7TquByQbpeCrHHRHl5WvnsftdnLZqmKKpGOEAkWZI2PFV0IXli4tyXz11tNLYPGtDU6AIeQaDPkI32ZjIk+J1LDgSXZ+iE2ownS+MAl1CPac7ugV8ekrXniFtUntKPJVjpM8YUMuua3dXvo7WvXCwHBsmJ9DKz49J9dQt4ESkbvskmbTt1NHzrBqmaCpGLZZlN4WnTimdtSHFUnc8T/WWo3TNRjE5K7WW1coznPBK5SnQbVEuIiGtlk5Hq62n3vPT8kxpzy1aT2EInsvCK0Y3JOTHMtkFPQMiGqrtJfn1PEOPKko61LiTeFYNUzQVY4vSUh+kjWFTQpI9G8jZlRxlCENbknWT8VxaFmYn9CGULUMsONUu4DrLMolndwilILZ9HfXUtWeK95HJtLgcf3NnZogAGpkcz2OnlgTKMPcitGdXHx0Ypu3VPJVjSVOBXHkp57Gm/KphiqZipBzcy6zq7B0RZQgnjzSYMovx6El5cmoP2Wm9JCAMH0k8JwWK1bS1hHpScgpKa12up14paEN8ZynzU46no11YiiuFZkOvFFLGXc5TDPU4npL3EZafEpKScNYweSetd66cxym0afPYQmdjjXDyaMMIR+b1k1Ob5JfCDUAx6bRJbg0KnvpkqwR1AneIEEZVSVlA781taut3XmXln1yMe6iNoHLiRoypdKWgrWf2quAYNinPem1JGHdnKcdyivehHsspmztq2ayj4zmdwLNqmKKpEdrQgGbC5wlxZVhEA611lTI5Nys9hb7QgIAtWp5DhM4kbEw4TZ3xzN6MWIa+sKVysU3yFITv85BUkvch8UxYwIfoT5HnlO7AaNJYVnrnWxI8aW3oLGW3ZTbnF3sJYUvb3ryOUENISmKaYgXmPCu0LKeVnkqa8nK00k4h7f1pQCH7CcEC7wtbKj2/oycX44Qhf+H74UJSOp6al7RpDYfhQqH63WQizynlAj5MPaWw5Qa9Usg31lQatqzeGKkapmhqwKYNusNrwygF7Q6xNJ5SrDy9nvL2Uf0dZpllKYVlUpRCpuiO1KPFr8oAAAtPSURBVGDVV5mnSOFZXKsvWcDp/VmlpzCt3TCS4J1rc4MpW/AznvNC2DL0ULXhwCrDlsOMZTuwuQ6gT4gPMzmrswIz2p6wU6jd1CuFjOexk7K1rMVQFpsydJdmBeri79V6H7rFDkgYdwn5sTo2d2jHaHjbgcjTl39CCFumhIyyelZpOAwzRuTzeNWPu6phiqYG1KkUpDjspPJm3pBn0qAXeTZr4Fm9FTg9VEI8/n3K5g7te4Nq2YgxBE9p59dUgoc6PYSHquVZ5bjLNhikeAoSNg/lfeh4VplzqxqmaGpAvn1UOODYSNj6MZQlNBKe6Qu4zLO+hHi1Ya6s3+PGAJCwZTvF+/Bt30s44CjyzDxU4SxLmlKofozUoWiGWcAlbBlKKehCoRrZN0zq3pZbNUzR1IBs0KdsW1bzTEg063nWUc8aFpGE9lTnPhQ8s/e31HFGYynhgKOEImx5Zi/g9fKsYX5UOI8zY+TIfHUbRrKxrDlQXXioisNJFcIUTQ3YurENQD6dn4JzPU/pkF0Kzu46nlKOJgVZAnd5pTqemQUsbTAAipCUmBROUArPn94AAFgSZGomhC2ff5bjeUKx80uLX9rk+lOTz9EiG3enKhzL52zUbcdNwdl+3PUUi60WmVLQjDstUjbWnOvbSSo9xUPNxl2VY0QFZn7O/nvlK1/JdeDYyUW+4uZ9fGqpJ9Lect8BvvW+AyLdycUeX3HzPj5+akmk/Z8Hn+av7dkv0i31lvkfv/EAHzp+SqS9+5FD/MV7fiLSLS+v8Me/+SD/9PC8SHvvk4f5+u89JtKtrKzw1Xc9zLNPHxdpH3zqGP/bt2ZFOmbma7/zKN+3/4hI98ShOf7n2x/ilZUVkfaGu5/gmcefEel+fuwkf/SWB3h5Web5pd1P8ncfPijSHZlb5I/cvI8Xe8si7c17f8Z3PPCUSDe/4Mbd3II87u544Cn+73t/JtItLC3zR76+j4/MLYq03509yDfNPCnSLS+v8Mdu/TE/dfSkSLv7iWf4C3c/LtKtrKzwVbc/xI8fOiHS3r//KH/224+IdMzM13zrEf7xgWMi3aMHT/Cn7nxYxfNz33uM9/zksEj3syPz/LFbf6wayzEAmOGEtZb4NLtQZxK2bdvGMzMzo66GwWAwjBWIaDczb9PSW+jMYDAYDLXCFI3BYDAYakWtioaI3khEDxLRLBFdvsb3bSL6ov/+B0R0QfDd3/nnDxLRGySeRHSh5zHreer3hRoMBoOhNtSmaIioAeBqAG8CcAmAtxHRJQNklwE4zMwvAvBJAFf6314CYAeAlwB4I4BPE1FD4HklgE96Xoc9b4PBYDCMGHV6NK8CMMvMjzLzIoCdALYP0GwHcL3/fBOA15M7ALEdwE5mXmDmxwDMen5r8vS/eZ3nAc/zd2uUzWAwGAxK1KloXgjgyeDvn/pna9Iwcw/AUQDnRH5b9vwcAEc8j7KyAABE9E4imiGimYMHDw4hlsFgMBhS8JzbDMDM1zDzNmbetnXr1lFXx2AwGNY96lQ0+wGcH/x9nn+2Jg0RNQFMA/hF5Ldlz38BYLPnUVaWwWAwGEYA/csZ0vFDABcT0YVwi/4OAH84QLMLwNsBfB/AWwDcycxMRLsA/AcRfQLALwO4GMA9cDeLrOLpf3OX57HT8/yaVMHdu3cfIqInhpTvXACHhvztmYr1JtN6kwcwmcYB600eYLVMv5ry49oUDTP3iOg9AG4F0ABwHTPfT0Qfgru+YBeAawF8nohmATwDpzjg6W4EsA9AD8C7mXkZANbi6Yt8H4CdRPRhAP/neUt1HDp2RkQzKSdjxwHrTab1Jg9gMo0D1ps8wLOX6Tl9Bc2zgQ2mMx/rTR7AZBoHrDd5gGcv03NuM4DBYDAYTi9M0QyPa0ZdgRqw3mRab/IAJtM4YL3JAzxLmSx0ZjAYDIZaYR6NwWAwGGqFKRqDwWAw1ApTNENAupV6HEBEjxPRj4hoDxHN+GdnE9FtRPSw/3/LqOsZAxFdR0RPE9F9wbM1ZSCHq3yf7SWiS0dX83KUyPRBItrv+2oPEb05+G7NW87PFBDR+UR0FxHtI6L7iei9/vnY9lNEprHsJyLaQET3ENG9Xp5/8M/XvBGfIrfulyLldZz2jwF3fucRABcBaAG4F8Alo67XEHI8DuDcgWcfBXC5/3w5gCtHXU9BhtcCuBTAfZIMAN4M4Btwh35fDeAHo65/gkwfBPA3a9Be4sdfG8CFflw2Ri3DQB1fAOBS/3kTgId8vce2nyIyjWU/+bbe6D9PAviBb/sbAezwzz8D4F3+858D+Iz/vAPAF6UyzKNJh+ZW6nFFeJv2GX8DNjN/G+6gb4gyGbYD+Bw73A13ZdELTk9N9SiRqQxlt5yfMWDmA8z8v/7zcQAPwF14O7b9FJGpDGd0P/m2PuH/nPT/GOU34pfdul8KUzTp0NxKPQ5gAN8kot1E9E7/7HnMfMB/fgrA80ZTtWeFMhnGvd/e40NJ1wUhzbGSyYdYXgFnMa+LfhqQCRjTfiL3vq89AJ4GcBuc11V2I37ZrfulMEXz3MVrmPlSuJfIvZuIXht+yc4vHuu97+tBBo9/BfBrAF4O4ACAj4+2Oukgoo0AvgTgr5j5WPjduPbTGjKNbT8x8zIzvxzuQuJXAXhxlfxN0aRDcyv1GQ9m3u//fxrAV+AG18+zMIX//+nR1XBolMkwtv3GzD/3C8EKgM+iCLuMhUxENAm3IN/AzF/2j8e6n9aSadz7CQCY+QiAuwD8BspvxC+7db8UpmjSkd9K7Xdh7IC7hXpsQERdItqUfQbw2wDuQ3GbNqC8AfsMRJkMuwD8id/V9GoAR4PQzRmNgRzF78H1FeBk2uF3AV2I4pbzMwY+dn8tgAeY+RPBV2PbT2UyjWs/EdFWItrsP08B+C24vFN2Iz6wuo+yvstv3Y8WMuodD+P4D25nzENwccz3j7o+Q9T/IrhdMPcCuD+TAS7OegeAhwHcDuDsUddVkOM/4UIUS3Ax5MvKZIDbWXO177MfAdg26vonyPR5X+e9fpK/IKB/v5fpQQBvGnX915DnNXBhsb0A9vh/bx7nforINJb9BOBlcDfe74VTjh/wzy+CU4izAP4LQNs/3+D/nvXfXySVYVfQGAwGg6FWWOjMYDAYDLXCFI3BYDAYaoUpGoPBYDDUClM0BoPBYKgVpmgMBoPBUCtM0RgMFYOIloMbfPdQhTd8E9EF4c3OBsM4oCmTGAyGRJxkd52HwWCAeTQGw2kDuXcAfZTce4DuIaIX+ecXENGd/jLGO4joV/zz5xHRV/x7Qu4lot/0rBpE9Fn/7pBv+tPcIKK/9O9I2UtEO0ckpsGwCqZoDIbqMTUQOntr8N1RZn4pgE8B+Cf/7F8AXM/MLwNwA4Cr/POrAHyLmX8d7h019/vnFwO4mplfAuAIgN/3zy8H8ArP58/qEs5gSIXdDGAwVAwiOsHMG9d4/jiA1zHzo/5SxqeY+RwiOgR3XcmSf36Amc8looMAzmPmhYDHBQBuY+aL/d/vAzDJzB8molsAnADwVQBf5eIdIwbDSGEejcFwesEln1OwEHxeRpFr/R24e8IuBfDD4OZdg2GkMEVjMJxevDX4//v+8/fgbgEHgD8C8B3/+Q4A7wLyF1NNlzElogkA5zPzXQDeB3d1+yqvymAYBcziMRiqx5R/W2GGW5g52+K8hYj2wnklb/PP/gLAvxPR3wI4COBP/fP3AriGiC6D81zeBXez81poAPiCV0YE4Cp27xYxGEYOy9EYDKcJPkezjZkPjbouBsPphIXODAaDwVArzKMxGAwGQ60wj8ZgMBgMtcIUjcFgMBhqhSkag8FgMNQKUzQGg8FgqBWmaAwGg8FQK/4fIHfyUDbR4hwAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms         \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import numpy as np\n",
        "import os\n",
        "from torchsummary import summary \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "batch_size = 128\n",
        "root_dir = 'drive/app/cifar10/'\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)),   \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
        "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
        "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
        "])\n",
        "\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root=root_dir,\n",
        "                                        train=True, \n",
        "                                        download=True, \n",
        "                                        transform=transform_train)\n",
        "testset = torchvision.datasets.CIFAR10(root=root_dir,\n",
        "                                       train=False, \n",
        "                                       download=True, \n",
        "                                       transform=transform_test)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=True, \n",
        "                                          num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(testset, \n",
        "                                         batch_size=batch_size, \n",
        "                                         shuffle=False, \n",
        "                                         num_workers=2)\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    def __init__(self, inplace: bool=False):\n",
        "        super().__init__()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.inplace=inplace\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.sigmoid(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    '''expand + depthwise + pointwise + squeeze-excitation'''\n",
        "    def __init__(self, in_planes, out_planes, expansion, stride):\n",
        "        super(Block, self).__init__()\n",
        "        self.stride = stride\n",
        "\n",
        "        planes = expansion * in_planes\n",
        "        self.conv1 = nn.Conv2d( in_planes, planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d( planes, planes, kernel_size=3, stride=stride, padding=1, groups=planes, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d( planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_planes)\n",
        "        self.swish=Swish(inplace=True)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride == 1 and in_planes != out_planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, out_planes, kernel_size=1,\n",
        "                          stride=1, padding=0, bias=False),\n",
        "                nn.BatchNorm2d(out_planes),\n",
        "            )\n",
        "\n",
        "        # SE layers\n",
        "        self.fc1 = nn.Conv2d(out_planes, out_planes//16, kernel_size=1)\n",
        "        self.fc2 = nn.Conv2d(out_planes//16, out_planes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.swish(self.bn1(self.conv1(x)))\n",
        "        out = self.swish(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        shortcut = self.shortcut(x) if self.stride == 1 else out\n",
        "        # Squeeze-Excitation\n",
        "        w = F.avg_pool2d(out, out.size(2))\n",
        "        w = self.swish(self.fc1(w))\n",
        "        w = self.fc2(w).sigmoid()\n",
        "        out = out * w + shortcut\n",
        "        return out\n",
        "\n",
        "\n",
        "class EfficientNet(nn.Module):\n",
        "    def __init__(self, cfg, num_classes=10):\n",
        "        super(EfficientNet, self).__init__()\n",
        "        self.cfg = cfg\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.layers = self._make_layers(in_planes=32)\n",
        "        self.linear = nn.Linear(cfg[-1][1], num_classes)\n",
        "        self.swish = Swish(inplace=True)\n",
        "\n",
        "    def _make_layers(self, in_planes):\n",
        "        layers = []\n",
        "        for expansion, out_planes, num_blocks, stride in self.cfg:\n",
        "            strides = [stride] + [1]*(num_blocks-1)\n",
        "            for stride in strides:\n",
        "                layers.append(Block(in_planes, out_planes, expansion, stride))\n",
        "                in_planes = out_planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.swish(self.bn1(self.conv1(x)))\n",
        "        out = self.layers(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def EfficientNetB0():\n",
        "    # (expansion, out_planes, num_blocks, stride)\n",
        "    cfg = [(1,  16, 1, 2),\n",
        "           (6,  24, 2, 1),\n",
        "           (6,  40, 2, 2),\n",
        "           (6,  80, 3, 2),\n",
        "           (6, 112, 3, 1),\n",
        "           (6, 192, 4, 2),\n",
        "           (6, 320, 1, 2)]\n",
        "    return EfficientNet(cfg)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "net = EfficientNetB0()\n",
        "net = net.to(device)\n",
        "# Load checkpoint.\n",
        "#assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
        "best_acc = 0.0\n",
        "start_epoch = 0\n",
        "if os.path.exists('./checkpoint/EfficientNet.pth'):\n",
        "    checkpoint = torch.load('./checkpoint/EfficientNet.pth')\n",
        "    net.load_state_dict(checkpoint['net'])\n",
        "    best_acc = checkpoint['acc']\n",
        "    start_epoch = checkpoint['epoch']\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=30, T_mult=1, eta_min=0.00001)\n",
        "#scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-2,   steps_per_epoch=len(trainloader), epochs=290)\n",
        "\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        if batch_idx % 40 == 39:\n",
        "            print('train: [%d/%d] Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                % (batch_idx, len(trainloader), train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "        if batch_idx == 390:\n",
        "            train_loss_values.append(100. * correct / total)    \n",
        "\n",
        "def test(epoch):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            if batch_idx % 40 == 39:\n",
        "                print('val: [%d/%d] Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                    % (batch_idx, len(testloader), test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "        test_loss_values.append(100. * correct / total)\n",
        "\n",
        "    # Save checkpoint.\n",
        "    acc = 100.*correct/total\n",
        "    if acc > best_acc:\n",
        "        print('Saving..')\n",
        "        state = {\n",
        "            'net': net.state_dict(),\n",
        "            'acc': acc,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        if not os.path.isdir('checkpoint'):\n",
        "            os.mkdir('checkpoint')\n",
        "        torch.save(state, './checkpoint/EfficientNet.pth')\n",
        "        print(\".pth saved\")\n",
        "        best_acc = acc\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "train_loss_values=[]\n",
        "test_loss_values=[]\n",
        "lrs  = []\n",
        "\n",
        "#for epoch in range(start_epoch, start_epoch+1):\n",
        "for epoch in range(start_epoch, 290):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    scheduler.step()\n",
        "    lrs.append(optimizer.param_groups[0][\"lr\"])  \n",
        "    print(\"Learing rate: \",get_lr(optimizer))\n",
        "\n",
        "print(\"best accurary %f%%\"%best_acc)\n",
        "\n",
        "summary(net.cuda(), (3,32,32),batch_size = 128)\n",
        "\n",
        "plt.plot(test_loss_values, 'g-', label='Test loss')\n",
        "plt.show()\n",
        "plt.plot(train_loss_values, 'r-', label='Train loss')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(lrs)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.title(\"OneCycleLR Scheduler\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "@torch.no_grad()\n",
        "\n",
        "#def get_all_preds(modell, loader):\n",
        "#  all_preds = torch.tensor([])\n",
        "#  for batch in loader:\n",
        "#    images, labels = batch\n",
        "#    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "#    preds = modell(images)\n",
        "#    all_preds = torch.cat((all_preds.to(device), preds.to(device)) ,dim=0)\n",
        "\n",
        "#  return all_preds\n",
        "\n",
        "\n",
        "def get_all_preds(model, loader):\n",
        "  all_preds = torch.tensor([])\n",
        "  for batch in loader:\n",
        "    images, labels = batch\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    preds = model(images)\n",
        "    all_preds = torch.cat((all_preds.to(device), preds.to(device)) ,dim=0)\n",
        "\n",
        "  return all_preds\n",
        "\n",
        "test_preds = get_all_preds(net, testloader)   \n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#cm = confusion_matrix(testset.targets, test_preds.argmax(dim=1).cpu())\n",
        "cm = confusion_matrix(testset.targets, test_preds.argmax(dim=1).cpu())\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "plt.figure(figsize=(10,10))\n",
        "plot_confusion_matrix(cm, classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "DaK6P5K_p1qU",
        "outputId": "000b9c0a-62ec-4264-8d92-070b5073e9b0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[923   3  19  11   2   1   2   4  25  10]\n",
            " [ 11 955   1   1   0   0   0   1   5  26]\n",
            " [ 21   0 859  27  29  20  26   9   6   3]\n",
            " [  8   2  35 772  20 112  31  12   3   5]\n",
            " [  7   2  19  18 899  20  16  18   1   0]\n",
            " [  3   2  16  71  20 868   4  14   0   2]\n",
            " [  4   0  21  24   5   7 937   0   1   1]\n",
            " [ 11   0  12  13  18  21   1 920   2   2]\n",
            " [ 28   6   5  10   2   1   0   1 937  10]\n",
            " [ 11  36   4   4   0   0   2   3  13 927]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}