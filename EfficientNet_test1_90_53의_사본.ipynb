{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Terizea/EfficientNet-CIFAR10/blob/main/EfficientNet_test1_90_53%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "910d1f54b70c4d50a114c92154e675ed",
            "b7a80f95dde54de698d00ac1c9ac2dad",
            "6c16c16634c4443b929f6017b9a904db",
            "2a461138a32345b687b4760a5a060356",
            "cc0279d3c53c40b7933b3fcc6aba975f",
            "95c67122963e43a8a3c2453b9497cefe",
            "ee4d99d2af2c405e8c536f4fd293dde2",
            "58d592630acd4c66852b676f54ced6e5",
            "c91995eac94a4f7eab8995b06449ad07",
            "9fa7a98e7f484eb2b3dd442ae4685197",
            "39acdd793f4443aaa25bf6ab7ea98d90"
          ]
        },
        "id": "a5AhFG4gWZe6",
        "outputId": "c5921e46-986a-49ff-8e0a-c58d7cab0cd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to drive/app/cifar10/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "910d1f54b70c4d50a114c92154e675ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting drive/app/cifar10/cifar-10-python.tar.gz to drive/app/cifar10/\n",
            "Files already downloaded and verified\n",
            "\n",
            "Epoch: 0\n",
            "train: [39/391] Loss: 2.237 | Acc: 22.109% (1132/5120)\n",
            "train: [79/391] Loss: 2.025 | Acc: 27.900% (2857/10240)\n",
            "train: [119/391] Loss: 1.921 | Acc: 31.029% (4766/15360)\n",
            "train: [159/391] Loss: 1.851 | Acc: 33.340% (6828/20480)\n",
            "train: [199/391] Loss: 1.800 | Acc: 35.031% (8968/25600)\n",
            "train: [239/391] Loss: 1.760 | Acc: 36.253% (11137/30720)\n",
            "train: [279/391] Loss: 1.727 | Acc: 37.288% (13364/35840)\n",
            "train: [319/391] Loss: 1.701 | Acc: 38.230% (15659/40960)\n",
            "train: [359/391] Loss: 1.669 | Acc: 39.427% (18168/46080)\n",
            "val: [39/79] Loss: 1.366 | Acc: 51.973% (2661/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 1\n",
            "train: [39/391] Loss: 1.390 | Acc: 48.867% (2502/5120)\n",
            "train: [79/391] Loss: 1.363 | Acc: 50.137% (5134/10240)\n",
            "train: [119/391] Loss: 1.361 | Acc: 50.241% (7717/15360)\n",
            "train: [159/391] Loss: 1.350 | Acc: 50.771% (10398/20480)\n",
            "train: [199/391] Loss: 1.339 | Acc: 51.441% (13169/25600)\n",
            "train: [239/391] Loss: 1.326 | Acc: 51.823% (15920/30720)\n",
            "train: [279/391] Loss: 1.313 | Acc: 52.324% (18753/35840)\n",
            "train: [319/391] Loss: 1.303 | Acc: 52.737% (21601/40960)\n",
            "train: [359/391] Loss: 1.292 | Acc: 53.173% (24502/46080)\n",
            "val: [39/79] Loss: 1.040 | Acc: 62.344% (3192/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 2\n",
            "train: [39/391] Loss: 1.165 | Acc: 58.555% (2998/5120)\n",
            "train: [79/391] Loss: 1.141 | Acc: 59.336% (6076/10240)\n",
            "train: [119/391] Loss: 1.139 | Acc: 59.297% (9108/15360)\n",
            "train: [159/391] Loss: 1.136 | Acc: 59.644% (12215/20480)\n",
            "train: [199/391] Loss: 1.133 | Acc: 59.703% (15284/25600)\n",
            "train: [239/391] Loss: 1.126 | Acc: 59.987% (18428/30720)\n",
            "train: [279/391] Loss: 1.118 | Acc: 60.349% (21629/35840)\n",
            "train: [319/391] Loss: 1.111 | Acc: 60.564% (24807/40960)\n",
            "train: [359/391] Loss: 1.104 | Acc: 60.779% (28007/46080)\n",
            "val: [39/79] Loss: 0.918 | Acc: 68.027% (3483/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 3\n",
            "train: [39/391] Loss: 1.038 | Acc: 63.633% (3258/5120)\n",
            "train: [79/391] Loss: 1.014 | Acc: 64.385% (6593/10240)\n",
            "train: [119/391] Loss: 1.003 | Acc: 64.486% (9905/15360)\n",
            "train: [159/391] Loss: 0.998 | Acc: 64.561% (13222/20480)\n",
            "train: [199/391] Loss: 0.993 | Acc: 64.832% (16597/25600)\n",
            "train: [239/391] Loss: 0.987 | Acc: 65.117% (20004/30720)\n",
            "train: [279/391] Loss: 0.982 | Acc: 65.301% (23404/35840)\n",
            "train: [319/391] Loss: 0.977 | Acc: 65.549% (26849/40960)\n",
            "train: [359/391] Loss: 0.973 | Acc: 65.692% (30271/46080)\n",
            "val: [39/79] Loss: 0.826 | Acc: 71.133% (3642/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 4\n",
            "train: [39/391] Loss: 0.882 | Acc: 69.004% (3533/5120)\n",
            "train: [79/391] Loss: 0.884 | Acc: 68.857% (7051/10240)\n",
            "train: [119/391] Loss: 0.881 | Acc: 68.913% (10585/15360)\n",
            "train: [159/391] Loss: 0.881 | Acc: 68.892% (14109/20480)\n",
            "train: [199/391] Loss: 0.879 | Acc: 69.086% (17686/25600)\n",
            "train: [239/391] Loss: 0.880 | Acc: 68.926% (21174/30720)\n",
            "train: [279/391] Loss: 0.875 | Acc: 69.146% (24782/35840)\n",
            "train: [319/391] Loss: 0.870 | Acc: 69.253% (28366/40960)\n",
            "train: [359/391] Loss: 0.865 | Acc: 69.436% (31996/46080)\n",
            "val: [39/79] Loss: 0.757 | Acc: 74.043% (3791/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 5\n",
            "train: [39/391] Loss: 0.804 | Acc: 71.738% (3673/5120)\n",
            "train: [79/391] Loss: 0.810 | Acc: 71.670% (7339/10240)\n",
            "train: [119/391] Loss: 0.803 | Acc: 71.634% (11003/15360)\n",
            "train: [159/391] Loss: 0.805 | Acc: 71.445% (14632/20480)\n",
            "train: [199/391] Loss: 0.799 | Acc: 71.660% (18345/25600)\n",
            "train: [239/391] Loss: 0.791 | Acc: 71.934% (22098/30720)\n",
            "train: [279/391] Loss: 0.787 | Acc: 72.182% (25870/35840)\n",
            "train: [319/391] Loss: 0.782 | Acc: 72.383% (29648/40960)\n",
            "train: [359/391] Loss: 0.780 | Acc: 72.426% (33374/46080)\n",
            "val: [39/79] Loss: 0.715 | Acc: 74.902% (3835/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 6\n",
            "train: [39/391] Loss: 0.720 | Acc: 74.824% (3831/5120)\n",
            "train: [79/391] Loss: 0.719 | Acc: 74.619% (7641/10240)\n",
            "train: [119/391] Loss: 0.719 | Acc: 74.609% (11460/15360)\n",
            "train: [159/391] Loss: 0.715 | Acc: 74.775% (15314/20480)\n",
            "train: [199/391] Loss: 0.720 | Acc: 74.656% (19112/25600)\n",
            "train: [239/391] Loss: 0.718 | Acc: 74.775% (22971/30720)\n",
            "train: [279/391] Loss: 0.717 | Acc: 74.908% (26847/35840)\n",
            "train: [319/391] Loss: 0.716 | Acc: 74.937% (30694/40960)\n",
            "train: [359/391] Loss: 0.718 | Acc: 74.907% (34517/46080)\n",
            "val: [39/79] Loss: 0.635 | Acc: 77.910% (3989/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 7\n",
            "train: [39/391] Loss: 0.687 | Acc: 76.484% (3916/5120)\n",
            "train: [79/391] Loss: 0.674 | Acc: 76.846% (7869/10240)\n",
            "train: [119/391] Loss: 0.674 | Acc: 76.751% (11789/15360)\n",
            "train: [159/391] Loss: 0.674 | Acc: 76.626% (15693/20480)\n",
            "train: [199/391] Loss: 0.670 | Acc: 76.809% (19663/25600)\n",
            "train: [239/391] Loss: 0.669 | Acc: 76.768% (23583/30720)\n",
            "train: [279/391] Loss: 0.664 | Acc: 76.830% (27536/35840)\n",
            "train: [319/391] Loss: 0.662 | Acc: 76.853% (31479/40960)\n",
            "train: [359/391] Loss: 0.659 | Acc: 76.897% (35434/46080)\n",
            "val: [39/79] Loss: 0.586 | Acc: 79.336% (4062/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 8\n",
            "train: [39/391] Loss: 0.641 | Acc: 77.148% (3950/5120)\n",
            "train: [79/391] Loss: 0.633 | Acc: 77.764% (7963/10240)\n",
            "train: [119/391] Loss: 0.630 | Acc: 77.858% (11959/15360)\n",
            "train: [159/391] Loss: 0.624 | Acc: 78.032% (15981/20480)\n",
            "train: [199/391] Loss: 0.617 | Acc: 78.211% (20022/25600)\n",
            "train: [239/391] Loss: 0.619 | Acc: 78.089% (23989/30720)\n",
            "train: [279/391] Loss: 0.616 | Acc: 78.284% (28057/35840)\n",
            "train: [319/391] Loss: 0.617 | Acc: 78.259% (32055/40960)\n",
            "train: [359/391] Loss: 0.616 | Acc: 78.270% (36067/46080)\n",
            "val: [39/79] Loss: 0.556 | Acc: 80.801% (4137/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 9\n",
            "train: [39/391] Loss: 0.607 | Acc: 78.438% (4016/5120)\n",
            "train: [79/391] Loss: 0.603 | Acc: 78.877% (8077/10240)\n",
            "train: [119/391] Loss: 0.604 | Acc: 78.815% (12106/15360)\n",
            "train: [159/391] Loss: 0.602 | Acc: 78.784% (16135/20480)\n",
            "train: [199/391] Loss: 0.598 | Acc: 78.996% (20223/25600)\n",
            "train: [239/391] Loss: 0.596 | Acc: 78.939% (24250/30720)\n",
            "train: [279/391] Loss: 0.592 | Acc: 79.160% (28371/35840)\n",
            "train: [319/391] Loss: 0.591 | Acc: 79.194% (32438/40960)\n",
            "train: [359/391] Loss: 0.588 | Acc: 79.264% (36525/46080)\n",
            "val: [39/79] Loss: 0.549 | Acc: 80.859% (4140/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 10\n",
            "train: [39/391] Loss: 0.726 | Acc: 73.594% (3768/5120)\n",
            "train: [79/391] Loss: 0.764 | Acc: 72.754% (7450/10240)\n",
            "train: [119/391] Loss: 0.775 | Acc: 72.533% (11141/15360)\n",
            "train: [159/391] Loss: 0.779 | Acc: 72.446% (14837/20480)\n",
            "train: [199/391] Loss: 0.779 | Acc: 72.527% (18567/25600)\n",
            "train: [239/391] Loss: 0.779 | Acc: 72.708% (22336/30720)\n",
            "train: [279/391] Loss: 0.779 | Acc: 72.679% (26048/35840)\n",
            "train: [319/391] Loss: 0.777 | Acc: 72.786% (29813/40960)\n",
            "train: [359/391] Loss: 0.774 | Acc: 72.891% (33588/46080)\n",
            "val: [39/79] Loss: 0.712 | Acc: 76.289% (3906/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 11\n",
            "train: [39/391] Loss: 0.720 | Acc: 74.805% (3830/5120)\n",
            "train: [79/391] Loss: 0.724 | Acc: 74.717% (7651/10240)\n",
            "train: [119/391] Loss: 0.732 | Acc: 74.453% (11436/15360)\n",
            "train: [159/391] Loss: 0.729 | Acc: 74.399% (15237/20480)\n",
            "train: [199/391] Loss: 0.732 | Acc: 74.301% (19021/25600)\n",
            "train: [239/391] Loss: 0.733 | Acc: 74.261% (22813/30720)\n",
            "train: [279/391] Loss: 0.731 | Acc: 74.252% (26612/35840)\n",
            "train: [319/391] Loss: 0.731 | Acc: 74.299% (30433/40960)\n",
            "train: [359/391] Loss: 0.729 | Acc: 74.477% (34319/46080)\n",
            "val: [39/79] Loss: 0.633 | Acc: 78.262% (4007/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 12\n",
            "train: [39/391] Loss: 0.652 | Acc: 77.520% (3969/5120)\n",
            "train: [79/391] Loss: 0.674 | Acc: 76.514% (7835/10240)\n",
            "train: [119/391] Loss: 0.669 | Acc: 76.699% (11781/15360)\n",
            "train: [159/391] Loss: 0.678 | Acc: 76.484% (15664/20480)\n",
            "train: [199/391] Loss: 0.678 | Acc: 76.414% (19562/25600)\n",
            "train: [239/391] Loss: 0.678 | Acc: 76.439% (23482/30720)\n",
            "train: [279/391] Loss: 0.674 | Acc: 76.518% (27424/35840)\n",
            "train: [319/391] Loss: 0.677 | Acc: 76.409% (31297/40960)\n",
            "train: [359/391] Loss: 0.676 | Acc: 76.387% (35199/46080)\n",
            "val: [39/79] Loss: 0.573 | Acc: 80.078% (4100/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 13\n",
            "train: [39/391] Loss: 0.632 | Acc: 78.086% (3998/5120)\n",
            "train: [79/391] Loss: 0.632 | Acc: 77.959% (7983/10240)\n",
            "train: [119/391] Loss: 0.620 | Acc: 78.288% (12025/15360)\n",
            "train: [159/391] Loss: 0.624 | Acc: 78.203% (16016/20480)\n",
            "train: [199/391] Loss: 0.628 | Acc: 78.086% (19990/25600)\n",
            "train: [239/391] Loss: 0.634 | Acc: 77.939% (23943/30720)\n",
            "train: [279/391] Loss: 0.633 | Acc: 77.994% (27953/35840)\n",
            "train: [319/391] Loss: 0.630 | Acc: 78.040% (31965/40960)\n",
            "train: [359/391] Loss: 0.631 | Acc: 78.014% (35949/46080)\n",
            "val: [39/79] Loss: 0.677 | Acc: 78.379% (4013/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 14\n",
            "train: [39/391] Loss: 0.591 | Acc: 79.590% (4075/5120)\n",
            "train: [79/391] Loss: 0.605 | Acc: 79.219% (8112/10240)\n",
            "train: [119/391] Loss: 0.600 | Acc: 79.271% (12176/15360)\n",
            "train: [159/391] Loss: 0.599 | Acc: 79.165% (16213/20480)\n",
            "train: [199/391] Loss: 0.595 | Acc: 79.199% (20275/25600)\n",
            "train: [239/391] Loss: 0.587 | Acc: 79.479% (24416/30720)\n",
            "train: [279/391] Loss: 0.586 | Acc: 79.515% (28498/35840)\n",
            "train: [319/391] Loss: 0.585 | Acc: 79.595% (32602/40960)\n",
            "train: [359/391] Loss: 0.583 | Acc: 79.635% (36696/46080)\n",
            "val: [39/79] Loss: 0.528 | Acc: 81.641% (4180/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 15\n",
            "train: [39/391] Loss: 0.535 | Acc: 81.699% (4183/5120)\n",
            "train: [79/391] Loss: 0.539 | Acc: 81.406% (8336/10240)\n",
            "train: [119/391] Loss: 0.535 | Acc: 81.465% (12513/15360)\n",
            "train: [159/391] Loss: 0.543 | Acc: 81.265% (16643/20480)\n",
            "train: [199/391] Loss: 0.538 | Acc: 81.316% (20817/25600)\n",
            "train: [239/391] Loss: 0.542 | Acc: 81.156% (24931/30720)\n",
            "train: [279/391] Loss: 0.542 | Acc: 81.133% (29078/35840)\n",
            "train: [319/391] Loss: 0.543 | Acc: 81.045% (33196/40960)\n",
            "train: [359/391] Loss: 0.543 | Acc: 81.029% (37338/46080)\n",
            "val: [39/79] Loss: 0.517 | Acc: 82.676% (4233/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 16\n",
            "train: [39/391] Loss: 0.498 | Acc: 82.773% (4238/5120)\n",
            "train: [79/391] Loss: 0.498 | Acc: 82.871% (8486/10240)\n",
            "train: [119/391] Loss: 0.500 | Acc: 82.910% (12735/15360)\n",
            "train: [159/391] Loss: 0.499 | Acc: 82.861% (16970/20480)\n",
            "train: [199/391] Loss: 0.497 | Acc: 82.910% (21225/25600)\n",
            "train: [239/391] Loss: 0.499 | Acc: 82.799% (25436/30720)\n",
            "train: [279/391] Loss: 0.501 | Acc: 82.679% (29632/35840)\n",
            "train: [319/391] Loss: 0.500 | Acc: 82.769% (33902/40960)\n",
            "train: [359/391] Loss: 0.500 | Acc: 82.732% (38123/46080)\n",
            "val: [39/79] Loss: 0.478 | Acc: 84.590% (4331/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 17\n",
            "train: [39/391] Loss: 0.465 | Acc: 83.750% (4288/5120)\n",
            "train: [79/391] Loss: 0.444 | Acc: 84.219% (8624/10240)\n",
            "train: [119/391] Loss: 0.451 | Acc: 84.049% (12910/15360)\n",
            "train: [159/391] Loss: 0.455 | Acc: 83.906% (17184/20480)\n",
            "train: [199/391] Loss: 0.457 | Acc: 83.887% (21475/25600)\n",
            "train: [239/391] Loss: 0.458 | Acc: 83.861% (25762/30720)\n",
            "train: [279/391] Loss: 0.456 | Acc: 83.906% (30072/35840)\n",
            "train: [319/391] Loss: 0.457 | Acc: 83.896% (34364/40960)\n",
            "train: [359/391] Loss: 0.456 | Acc: 83.932% (38676/46080)\n",
            "val: [39/79] Loss: 0.455 | Acc: 84.844% (4344/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 18\n",
            "train: [39/391] Loss: 0.442 | Acc: 84.668% (4335/5120)\n",
            "train: [79/391] Loss: 0.446 | Acc: 84.619% (8665/10240)\n",
            "train: [119/391] Loss: 0.450 | Acc: 84.577% (12991/15360)\n",
            "train: [159/391] Loss: 0.448 | Acc: 84.595% (17325/20480)\n",
            "train: [199/391] Loss: 0.443 | Acc: 84.734% (21692/25600)\n",
            "train: [239/391] Loss: 0.439 | Acc: 84.863% (26070/30720)\n",
            "train: [279/391] Loss: 0.439 | Acc: 84.791% (30389/35840)\n",
            "train: [319/391] Loss: 0.434 | Acc: 84.888% (34770/40960)\n",
            "train: [359/391] Loss: 0.434 | Acc: 84.870% (39108/46080)\n",
            "val: [39/79] Loss: 0.444 | Acc: 85.234% (4364/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 19\n",
            "train: [39/391] Loss: 0.418 | Acc: 85.098% (4357/5120)\n",
            "train: [79/391] Loss: 0.426 | Acc: 84.873% (8691/10240)\n",
            "train: [119/391] Loss: 0.428 | Acc: 84.746% (13017/15360)\n",
            "train: [159/391] Loss: 0.428 | Acc: 84.917% (17391/20480)\n",
            "train: [199/391] Loss: 0.424 | Acc: 85.020% (21765/25600)\n",
            "train: [239/391] Loss: 0.422 | Acc: 85.020% (26118/30720)\n",
            "train: [279/391] Loss: 0.422 | Acc: 85.081% (30493/35840)\n",
            "train: [319/391] Loss: 0.421 | Acc: 85.112% (34862/40960)\n",
            "train: [359/391] Loss: 0.419 | Acc: 85.180% (39251/46080)\n",
            "val: [39/79] Loss: 0.438 | Acc: 85.391% (4372/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 20\n",
            "train: [39/391] Loss: 0.514 | Acc: 82.031% (4200/5120)\n",
            "train: [79/391] Loss: 0.531 | Acc: 81.895% (8386/10240)\n",
            "train: [119/391] Loss: 0.559 | Acc: 80.840% (12417/15360)\n",
            "train: [159/391] Loss: 0.562 | Acc: 80.562% (16499/20480)\n",
            "train: [199/391] Loss: 0.568 | Acc: 80.355% (20571/25600)\n",
            "train: [239/391] Loss: 0.577 | Acc: 80.081% (24601/30720)\n",
            "train: [279/391] Loss: 0.578 | Acc: 80.056% (28692/35840)\n",
            "train: [319/391] Loss: 0.577 | Acc: 80.061% (32793/40960)\n",
            "train: [359/391] Loss: 0.580 | Acc: 79.870% (36804/46080)\n",
            "val: [39/79] Loss: 0.590 | Acc: 79.609% (4076/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 21\n",
            "train: [39/391] Loss: 0.603 | Acc: 79.492% (4070/5120)\n",
            "train: [79/391] Loss: 0.594 | Acc: 79.414% (8132/10240)\n",
            "train: [119/391] Loss: 0.584 | Acc: 79.831% (12262/15360)\n",
            "train: [159/391] Loss: 0.580 | Acc: 80.024% (16389/20480)\n",
            "train: [199/391] Loss: 0.575 | Acc: 80.137% (20515/25600)\n",
            "train: [239/391] Loss: 0.569 | Acc: 80.221% (24644/30720)\n",
            "train: [279/391] Loss: 0.568 | Acc: 80.335% (28792/35840)\n",
            "train: [319/391] Loss: 0.566 | Acc: 80.303% (32892/40960)\n",
            "train: [359/391] Loss: 0.564 | Acc: 80.445% (37069/46080)\n",
            "val: [39/79] Loss: 0.532 | Acc: 81.953% (4196/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 22\n",
            "train: [39/391] Loss: 0.524 | Acc: 81.641% (4180/5120)\n",
            "train: [79/391] Loss: 0.525 | Acc: 81.650% (8361/10240)\n",
            "train: [119/391] Loss: 0.532 | Acc: 81.497% (12518/15360)\n",
            "train: [159/391] Loss: 0.538 | Acc: 81.353% (16661/20480)\n",
            "train: [199/391] Loss: 0.533 | Acc: 81.613% (20893/25600)\n",
            "train: [239/391] Loss: 0.540 | Acc: 81.426% (25014/30720)\n",
            "train: [279/391] Loss: 0.538 | Acc: 81.487% (29205/35840)\n",
            "train: [319/391] Loss: 0.537 | Acc: 81.445% (33360/40960)\n",
            "train: [359/391] Loss: 0.539 | Acc: 81.419% (37518/46080)\n",
            "val: [39/79] Loss: 0.505 | Acc: 83.379% (4269/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 23\n",
            "train: [39/391] Loss: 0.509 | Acc: 82.500% (4224/5120)\n",
            "train: [79/391] Loss: 0.494 | Acc: 82.939% (8493/10240)\n",
            "train: [119/391] Loss: 0.490 | Acc: 83.125% (12768/15360)\n",
            "train: [159/391] Loss: 0.494 | Acc: 83.022% (17003/20480)\n",
            "train: [199/391] Loss: 0.495 | Acc: 82.852% (21210/25600)\n",
            "train: [239/391] Loss: 0.498 | Acc: 82.728% (25414/30720)\n",
            "train: [279/391] Loss: 0.495 | Acc: 82.801% (29676/35840)\n",
            "train: [319/391] Loss: 0.497 | Acc: 82.771% (33903/40960)\n",
            "train: [359/391] Loss: 0.499 | Acc: 82.724% (38119/46080)\n",
            "val: [39/79] Loss: 0.489 | Acc: 83.770% (4289/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 24\n",
            "train: [39/391] Loss: 0.452 | Acc: 84.297% (4316/5120)\n",
            "train: [79/391] Loss: 0.454 | Acc: 84.570% (8660/10240)\n",
            "train: [119/391] Loss: 0.456 | Acc: 84.284% (12946/15360)\n",
            "train: [159/391] Loss: 0.456 | Acc: 84.272% (17259/20480)\n",
            "train: [199/391] Loss: 0.457 | Acc: 84.191% (21553/25600)\n",
            "train: [239/391] Loss: 0.453 | Acc: 84.268% (25887/30720)\n",
            "train: [279/391] Loss: 0.457 | Acc: 84.191% (30174/35840)\n",
            "train: [319/391] Loss: 0.457 | Acc: 84.136% (34462/40960)\n",
            "train: [359/391] Loss: 0.457 | Acc: 84.084% (38746/46080)\n",
            "val: [39/79] Loss: 0.462 | Acc: 84.688% (4336/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 25\n",
            "train: [39/391] Loss: 0.431 | Acc: 85.059% (4355/5120)\n",
            "train: [79/391] Loss: 0.419 | Acc: 85.557% (8761/10240)\n",
            "train: [119/391] Loss: 0.428 | Acc: 85.221% (13090/15360)\n",
            "train: [159/391] Loss: 0.427 | Acc: 85.195% (17448/20480)\n",
            "train: [199/391] Loss: 0.429 | Acc: 84.965% (21751/25600)\n",
            "train: [239/391] Loss: 0.429 | Acc: 85.020% (26118/30720)\n",
            "train: [279/391] Loss: 0.429 | Acc: 85.070% (30489/35840)\n",
            "train: [319/391] Loss: 0.428 | Acc: 85.068% (34844/40960)\n",
            "train: [359/391] Loss: 0.427 | Acc: 85.141% (39233/46080)\n",
            "val: [39/79] Loss: 0.440 | Acc: 85.254% (4365/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 26\n",
            "train: [39/391] Loss: 0.393 | Acc: 86.641% (4436/5120)\n",
            "train: [79/391] Loss: 0.394 | Acc: 86.719% (8880/10240)\n",
            "train: [119/391] Loss: 0.400 | Acc: 86.107% (13226/15360)\n",
            "train: [159/391] Loss: 0.399 | Acc: 86.187% (17651/20480)\n",
            "train: [199/391] Loss: 0.398 | Acc: 86.078% (22036/25600)\n",
            "train: [239/391] Loss: 0.397 | Acc: 86.126% (26458/30720)\n",
            "train: [279/391] Loss: 0.400 | Acc: 86.057% (30843/35840)\n",
            "train: [319/391] Loss: 0.399 | Acc: 86.128% (35278/40960)\n",
            "train: [359/391] Loss: 0.399 | Acc: 86.115% (39682/46080)\n",
            "val: [39/79] Loss: 0.418 | Acc: 86.016% (4404/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 27\n",
            "train: [39/391] Loss: 0.378 | Acc: 86.230% (4415/5120)\n",
            "train: [79/391] Loss: 0.379 | Acc: 86.572% (8865/10240)\n",
            "train: [119/391] Loss: 0.369 | Acc: 86.960% (13357/15360)\n",
            "train: [159/391] Loss: 0.370 | Acc: 86.958% (17809/20480)\n",
            "train: [199/391] Loss: 0.370 | Acc: 87.004% (22273/25600)\n",
            "train: [239/391] Loss: 0.371 | Acc: 86.953% (26712/30720)\n",
            "train: [279/391] Loss: 0.372 | Acc: 86.908% (31148/35840)\n",
            "train: [319/391] Loss: 0.370 | Acc: 87.012% (35640/40960)\n",
            "train: [359/391] Loss: 0.369 | Acc: 86.970% (40076/46080)\n",
            "val: [39/79] Loss: 0.398 | Acc: 86.816% (4445/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 28\n",
            "train: [39/391] Loss: 0.346 | Acc: 87.910% (4501/5120)\n",
            "train: [79/391] Loss: 0.345 | Acc: 88.086% (9020/10240)\n",
            "train: [119/391] Loss: 0.343 | Acc: 88.053% (13525/15360)\n",
            "train: [159/391] Loss: 0.344 | Acc: 87.974% (18017/20480)\n",
            "train: [199/391] Loss: 0.343 | Acc: 88.020% (22533/25600)\n",
            "train: [239/391] Loss: 0.342 | Acc: 88.076% (27057/30720)\n",
            "train: [279/391] Loss: 0.341 | Acc: 88.092% (31572/35840)\n",
            "train: [319/391] Loss: 0.340 | Acc: 88.167% (36113/40960)\n",
            "train: [359/391] Loss: 0.341 | Acc: 88.118% (40605/46080)\n",
            "val: [39/79] Loss: 0.396 | Acc: 87.324% (4471/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 29\n",
            "train: [39/391] Loss: 0.318 | Acc: 88.965% (4555/5120)\n",
            "train: [79/391] Loss: 0.322 | Acc: 88.789% (9092/10240)\n",
            "train: [119/391] Loss: 0.317 | Acc: 89.017% (13673/15360)\n",
            "train: [159/391] Loss: 0.321 | Acc: 88.950% (18217/20480)\n",
            "train: [199/391] Loss: 0.323 | Acc: 88.891% (22756/25600)\n",
            "train: [239/391] Loss: 0.323 | Acc: 88.844% (27293/30720)\n",
            "train: [279/391] Loss: 0.323 | Acc: 88.797% (31825/35840)\n",
            "train: [319/391] Loss: 0.322 | Acc: 88.853% (36394/40960)\n",
            "train: [359/391] Loss: 0.324 | Acc: 88.796% (40917/46080)\n",
            "val: [39/79] Loss: 0.393 | Acc: 87.344% (4472/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 30\n",
            "train: [39/391] Loss: 0.389 | Acc: 86.328% (4420/5120)\n",
            "train: [79/391] Loss: 0.424 | Acc: 84.824% (8686/10240)\n",
            "train: [119/391] Loss: 0.437 | Acc: 84.382% (12961/15360)\n",
            "train: [159/391] Loss: 0.444 | Acc: 84.111% (17226/20480)\n",
            "train: [199/391] Loss: 0.447 | Acc: 84.066% (21521/25600)\n",
            "train: [239/391] Loss: 0.452 | Acc: 83.939% (25786/30720)\n",
            "train: [279/391] Loss: 0.456 | Acc: 83.811% (30038/35840)\n",
            "train: [319/391] Loss: 0.458 | Acc: 83.772% (34313/40960)\n",
            "train: [359/391] Loss: 0.462 | Acc: 83.689% (38564/46080)\n",
            "val: [39/79] Loss: 0.495 | Acc: 83.418% (4271/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 31\n",
            "train: [39/391] Loss: 0.446 | Acc: 84.160% (4309/5120)\n",
            "train: [79/391] Loss: 0.445 | Acc: 84.102% (8612/10240)\n",
            "train: [119/391] Loss: 0.452 | Acc: 83.841% (12878/15360)\n",
            "train: [159/391] Loss: 0.452 | Acc: 83.906% (17184/20480)\n",
            "train: [199/391] Loss: 0.454 | Acc: 83.879% (21473/25600)\n",
            "train: [239/391] Loss: 0.462 | Acc: 83.734% (25723/30720)\n",
            "train: [279/391] Loss: 0.464 | Acc: 83.697% (29997/35840)\n",
            "train: [319/391] Loss: 0.476 | Acc: 83.381% (34153/40960)\n",
            "train: [359/391] Loss: 0.503 | Acc: 82.643% (38082/46080)\n",
            "val: [39/79] Loss: 0.531 | Acc: 82.637% (4231/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 32\n",
            "train: [39/391] Loss: 0.517 | Acc: 81.914% (4194/5120)\n",
            "train: [79/391] Loss: 0.511 | Acc: 82.295% (8427/10240)\n",
            "train: [119/391] Loss: 0.498 | Acc: 82.591% (12686/15360)\n",
            "train: [159/391] Loss: 0.487 | Acc: 82.935% (16985/20480)\n",
            "train: [199/391] Loss: 0.485 | Acc: 82.988% (21245/25600)\n",
            "train: [239/391] Loss: 0.480 | Acc: 83.115% (25533/30720)\n",
            "train: [279/391] Loss: 0.481 | Acc: 83.186% (29814/35840)\n",
            "train: [319/391] Loss: 0.484 | Acc: 83.044% (34015/40960)\n",
            "train: [359/391] Loss: 0.482 | Acc: 83.186% (38332/46080)\n",
            "val: [39/79] Loss: 0.462 | Acc: 84.492% (4326/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 33\n",
            "train: [39/391] Loss: 0.440 | Acc: 84.512% (4327/5120)\n",
            "train: [79/391] Loss: 0.457 | Acc: 84.199% (8622/10240)\n",
            "train: [119/391] Loss: 0.453 | Acc: 84.232% (12938/15360)\n",
            "train: [159/391] Loss: 0.449 | Acc: 84.404% (17286/20480)\n",
            "train: [199/391] Loss: 0.444 | Acc: 84.535% (21641/25600)\n",
            "train: [239/391] Loss: 0.442 | Acc: 84.590% (25986/30720)\n",
            "train: [279/391] Loss: 0.441 | Acc: 84.646% (30337/35840)\n",
            "train: [319/391] Loss: 0.440 | Acc: 84.722% (34702/40960)\n",
            "train: [359/391] Loss: 0.436 | Acc: 84.900% (39122/46080)\n",
            "val: [39/79] Loss: 0.432 | Acc: 85.508% (4378/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 34\n",
            "train: [39/391] Loss: 0.410 | Acc: 85.645% (4385/5120)\n",
            "train: [79/391] Loss: 0.403 | Acc: 85.889% (8795/10240)\n",
            "train: [119/391] Loss: 0.404 | Acc: 85.911% (13196/15360)\n",
            "train: [159/391] Loss: 0.399 | Acc: 86.143% (17642/20480)\n",
            "train: [199/391] Loss: 0.399 | Acc: 86.102% (22042/25600)\n",
            "train: [239/391] Loss: 0.399 | Acc: 86.025% (26427/30720)\n",
            "train: [279/391] Loss: 0.399 | Acc: 86.032% (30834/35840)\n",
            "train: [319/391] Loss: 0.398 | Acc: 86.055% (35248/40960)\n",
            "train: [359/391] Loss: 0.395 | Acc: 86.168% (39706/46080)\n",
            "val: [39/79] Loss: 0.437 | Acc: 85.742% (4390/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 35\n",
            "train: [39/391] Loss: 0.383 | Acc: 86.504% (4429/5120)\n",
            "train: [79/391] Loss: 0.375 | Acc: 86.973% (8906/10240)\n",
            "train: [119/391] Loss: 0.380 | Acc: 86.693% (13316/15360)\n",
            "train: [159/391] Loss: 0.376 | Acc: 86.777% (17772/20480)\n",
            "train: [199/391] Loss: 0.379 | Acc: 86.684% (22191/25600)\n",
            "train: [239/391] Loss: 0.373 | Acc: 86.917% (26701/30720)\n",
            "train: [279/391] Loss: 0.370 | Acc: 87.012% (31185/35840)\n",
            "train: [319/391] Loss: 0.367 | Acc: 87.085% (35670/40960)\n",
            "train: [359/391] Loss: 0.367 | Acc: 87.090% (40131/46080)\n",
            "val: [39/79] Loss: 0.417 | Acc: 85.859% (4396/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 36\n",
            "train: [39/391] Loss: 0.319 | Acc: 88.945% (4554/5120)\n",
            "train: [79/391] Loss: 0.335 | Acc: 88.369% (9049/10240)\n",
            "train: [119/391] Loss: 0.332 | Acc: 88.451% (13586/15360)\n",
            "train: [159/391] Loss: 0.331 | Acc: 88.579% (18141/20480)\n",
            "train: [199/391] Loss: 0.331 | Acc: 88.566% (22673/25600)\n",
            "train: [239/391] Loss: 0.329 | Acc: 88.574% (27210/30720)\n",
            "train: [279/391] Loss: 0.330 | Acc: 88.491% (31715/35840)\n",
            "train: [319/391] Loss: 0.329 | Acc: 88.477% (36240/40960)\n",
            "train: [359/391] Loss: 0.327 | Acc: 88.522% (40791/46080)\n",
            "val: [39/79] Loss: 0.400 | Acc: 86.895% (4449/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 37\n",
            "train: [39/391] Loss: 0.304 | Acc: 89.102% (4562/5120)\n",
            "train: [79/391] Loss: 0.308 | Acc: 89.102% (9124/10240)\n",
            "train: [119/391] Loss: 0.308 | Acc: 89.141% (13692/15360)\n",
            "train: [159/391] Loss: 0.304 | Acc: 89.312% (18291/20480)\n",
            "train: [199/391] Loss: 0.308 | Acc: 89.184% (22831/25600)\n",
            "train: [239/391] Loss: 0.304 | Acc: 89.251% (27418/30720)\n",
            "train: [279/391] Loss: 0.305 | Acc: 89.160% (31955/35840)\n",
            "train: [319/391] Loss: 0.304 | Acc: 89.263% (36562/40960)\n",
            "train: [359/391] Loss: 0.303 | Acc: 89.327% (41162/46080)\n",
            "val: [39/79] Loss: 0.394 | Acc: 87.266% (4468/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 38\n",
            "train: [39/391] Loss: 0.291 | Acc: 89.824% (4599/5120)\n",
            "train: [79/391] Loss: 0.288 | Acc: 89.873% (9203/10240)\n",
            "train: [119/391] Loss: 0.283 | Acc: 90.007% (13825/15360)\n",
            "train: [159/391] Loss: 0.283 | Acc: 89.927% (18417/20480)\n",
            "train: [199/391] Loss: 0.285 | Acc: 89.953% (23028/25600)\n",
            "train: [239/391] Loss: 0.287 | Acc: 89.867% (27607/30720)\n",
            "train: [279/391] Loss: 0.286 | Acc: 89.911% (32224/35840)\n",
            "train: [319/391] Loss: 0.286 | Acc: 89.902% (36824/40960)\n",
            "train: [359/391] Loss: 0.287 | Acc: 89.881% (41417/46080)\n",
            "val: [39/79] Loss: 0.387 | Acc: 87.383% (4474/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 39\n",
            "train: [39/391] Loss: 0.277 | Acc: 90.312% (4624/5120)\n",
            "train: [79/391] Loss: 0.286 | Acc: 89.863% (9202/10240)\n",
            "train: [119/391] Loss: 0.282 | Acc: 89.974% (13820/15360)\n",
            "train: [159/391] Loss: 0.283 | Acc: 89.976% (18427/20480)\n",
            "train: [199/391] Loss: 0.281 | Acc: 90.055% (23054/25600)\n",
            "train: [239/391] Loss: 0.279 | Acc: 90.094% (27677/30720)\n",
            "train: [279/391] Loss: 0.278 | Acc: 90.193% (32325/35840)\n",
            "train: [319/391] Loss: 0.274 | Acc: 90.288% (36982/40960)\n",
            "train: [359/391] Loss: 0.274 | Acc: 90.217% (41572/46080)\n",
            "val: [39/79] Loss: 0.385 | Acc: 87.539% (4482/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 40\n",
            "train: [39/391] Loss: 0.339 | Acc: 88.477% (4530/5120)\n",
            "train: [79/391] Loss: 0.381 | Acc: 86.484% (8856/10240)\n",
            "train: [119/391] Loss: 0.411 | Acc: 85.553% (13141/15360)\n",
            "train: [159/391] Loss: 0.418 | Acc: 85.303% (17470/20480)\n",
            "train: [199/391] Loss: 0.424 | Acc: 85.137% (21795/25600)\n",
            "train: [239/391] Loss: 0.434 | Acc: 84.756% (26037/30720)\n",
            "train: [279/391] Loss: 0.435 | Acc: 84.738% (30370/35840)\n",
            "train: [319/391] Loss: 0.434 | Acc: 84.758% (34717/40960)\n",
            "train: [359/391] Loss: 0.431 | Acc: 84.798% (39075/46080)\n",
            "val: [39/79] Loss: 0.623 | Acc: 81.074% (4151/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 41\n",
            "train: [39/391] Loss: 0.537 | Acc: 82.598% (4229/5120)\n",
            "train: [79/391] Loss: 0.523 | Acc: 82.441% (8442/10240)\n",
            "train: [119/391] Loss: 0.497 | Acc: 83.158% (12773/15360)\n",
            "train: [159/391] Loss: 0.480 | Acc: 83.613% (17124/20480)\n",
            "train: [199/391] Loss: 0.464 | Acc: 84.086% (21526/25600)\n",
            "train: [239/391] Loss: 0.460 | Acc: 84.180% (25860/30720)\n",
            "train: [279/391] Loss: 0.453 | Acc: 84.414% (30254/35840)\n",
            "train: [319/391] Loss: 0.456 | Acc: 84.363% (34555/40960)\n",
            "train: [359/391] Loss: 0.465 | Acc: 84.078% (38743/46080)\n",
            "val: [39/79] Loss: 0.461 | Acc: 84.668% (4335/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 42\n",
            "train: [39/391] Loss: 0.403 | Acc: 85.684% (4387/5120)\n",
            "train: [79/391] Loss: 0.407 | Acc: 85.576% (8763/10240)\n",
            "train: [119/391] Loss: 0.400 | Acc: 85.833% (13184/15360)\n",
            "train: [159/391] Loss: 0.395 | Acc: 86.006% (17614/20480)\n",
            "train: [199/391] Loss: 0.394 | Acc: 86.035% (22025/25600)\n",
            "train: [239/391] Loss: 0.404 | Acc: 85.729% (26336/30720)\n",
            "train: [279/391] Loss: 0.404 | Acc: 85.778% (30743/35840)\n",
            "train: [319/391] Loss: 0.403 | Acc: 85.786% (35138/40960)\n",
            "train: [359/391] Loss: 0.403 | Acc: 85.838% (39554/46080)\n",
            "val: [39/79] Loss: 0.417 | Acc: 85.918% (4399/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 43\n",
            "train: [39/391] Loss: 0.442 | Acc: 85.469% (4376/5120)\n",
            "train: [79/391] Loss: 0.496 | Acc: 83.105% (8510/10240)\n",
            "train: [119/391] Loss: 0.481 | Acc: 83.503% (12826/15360)\n",
            "train: [159/391] Loss: 0.463 | Acc: 83.989% (17201/20480)\n",
            "train: [199/391] Loss: 0.452 | Acc: 84.371% (21599/25600)\n",
            "train: [239/391] Loss: 0.444 | Acc: 84.587% (25985/30720)\n",
            "train: [279/391] Loss: 0.433 | Acc: 84.961% (30450/35840)\n",
            "train: [319/391] Loss: 0.429 | Acc: 85.103% (34858/40960)\n",
            "train: [359/391] Loss: 0.425 | Acc: 85.315% (39313/46080)\n",
            "val: [39/79] Loss: 0.407 | Acc: 86.777% (4443/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 44\n",
            "train: [39/391] Loss: 0.341 | Acc: 87.480% (4479/5120)\n",
            "train: [79/391] Loss: 0.335 | Acc: 87.979% (9009/10240)\n",
            "train: [119/391] Loss: 0.341 | Acc: 87.865% (13496/15360)\n",
            "train: [159/391] Loss: 0.343 | Acc: 87.847% (17991/20480)\n",
            "train: [199/391] Loss: 0.340 | Acc: 87.996% (22527/25600)\n",
            "train: [239/391] Loss: 0.346 | Acc: 87.796% (26971/30720)\n",
            "train: [279/391] Loss: 0.345 | Acc: 87.879% (31496/35840)\n",
            "train: [319/391] Loss: 0.344 | Acc: 87.871% (35992/40960)\n",
            "train: [359/391] Loss: 0.344 | Acc: 87.867% (40489/46080)\n",
            "val: [39/79] Loss: 0.384 | Acc: 87.383% (4474/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 45\n",
            "train: [39/391] Loss: 0.310 | Acc: 89.023% (4558/5120)\n",
            "train: [79/391] Loss: 0.301 | Acc: 89.346% (9149/10240)\n",
            "train: [119/391] Loss: 0.313 | Acc: 88.828% (13644/15360)\n",
            "train: [159/391] Loss: 0.313 | Acc: 88.862% (18199/20480)\n",
            "train: [199/391] Loss: 0.311 | Acc: 88.969% (22776/25600)\n",
            "train: [239/391] Loss: 0.312 | Acc: 88.991% (27338/30720)\n",
            "train: [279/391] Loss: 0.314 | Acc: 88.951% (31880/35840)\n",
            "train: [319/391] Loss: 0.315 | Acc: 88.911% (36418/40960)\n",
            "train: [359/391] Loss: 0.314 | Acc: 88.969% (40997/46080)\n",
            "val: [39/79] Loss: 0.394 | Acc: 87.852% (4498/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 46\n",
            "train: [39/391] Loss: 0.307 | Acc: 89.160% (4565/5120)\n",
            "train: [79/391] Loss: 0.305 | Acc: 89.258% (9140/10240)\n",
            "train: [119/391] Loss: 0.298 | Acc: 89.492% (13746/15360)\n",
            "train: [159/391] Loss: 0.294 | Acc: 89.683% (18367/20480)\n",
            "train: [199/391] Loss: 0.289 | Acc: 89.875% (23008/25600)\n",
            "train: [239/391] Loss: 0.289 | Acc: 89.847% (27601/30720)\n",
            "train: [279/391] Loss: 0.286 | Acc: 90.028% (32266/35840)\n",
            "train: [319/391] Loss: 0.286 | Acc: 89.968% (36851/40960)\n",
            "train: [359/391] Loss: 0.285 | Acc: 90.000% (41472/46080)\n",
            "val: [39/79] Loss: 0.378 | Acc: 88.008% (4506/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 47\n",
            "train: [39/391] Loss: 0.279 | Acc: 90.410% (4629/5120)\n",
            "train: [79/391] Loss: 0.272 | Acc: 90.576% (9275/10240)\n",
            "train: [119/391] Loss: 0.266 | Acc: 90.645% (13923/15360)\n",
            "train: [159/391] Loss: 0.266 | Acc: 90.566% (18548/20480)\n",
            "train: [199/391] Loss: 0.263 | Acc: 90.730% (23227/25600)\n",
            "train: [239/391] Loss: 0.263 | Acc: 90.719% (27869/30720)\n",
            "train: [279/391] Loss: 0.262 | Acc: 90.762% (32529/35840)\n",
            "train: [319/391] Loss: 0.263 | Acc: 90.806% (37194/40960)\n",
            "train: [359/391] Loss: 0.262 | Acc: 90.799% (41840/46080)\n",
            "val: [39/79] Loss: 0.373 | Acc: 88.164% (4514/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 48\n",
            "train: [39/391] Loss: 0.251 | Acc: 90.918% (4655/5120)\n",
            "train: [79/391] Loss: 0.249 | Acc: 90.879% (9306/10240)\n",
            "train: [119/391] Loss: 0.244 | Acc: 91.224% (14012/15360)\n",
            "train: [159/391] Loss: 0.250 | Acc: 91.001% (18637/20480)\n",
            "train: [199/391] Loss: 0.249 | Acc: 91.012% (23299/25600)\n",
            "train: [239/391] Loss: 0.247 | Acc: 91.110% (27989/30720)\n",
            "train: [279/391] Loss: 0.248 | Acc: 91.085% (32645/35840)\n",
            "train: [319/391] Loss: 0.248 | Acc: 91.094% (37312/40960)\n",
            "train: [359/391] Loss: 0.247 | Acc: 91.159% (42006/46080)\n",
            "val: [39/79] Loss: 0.373 | Acc: 88.203% (4516/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 49\n",
            "train: [39/391] Loss: 0.248 | Acc: 91.211% (4670/5120)\n",
            "train: [79/391] Loss: 0.245 | Acc: 91.357% (9355/10240)\n",
            "train: [119/391] Loss: 0.239 | Acc: 91.465% (14049/15360)\n",
            "train: [159/391] Loss: 0.236 | Acc: 91.611% (18762/20480)\n",
            "train: [199/391] Loss: 0.236 | Acc: 91.586% (23446/25600)\n",
            "train: [239/391] Loss: 0.235 | Acc: 91.611% (28143/30720)\n",
            "train: [279/391] Loss: 0.233 | Acc: 91.616% (32835/35840)\n",
            "train: [319/391] Loss: 0.232 | Acc: 91.633% (37533/40960)\n",
            "train: [359/391] Loss: 0.234 | Acc: 91.591% (42205/46080)\n",
            "val: [39/79] Loss: 0.368 | Acc: 88.516% (4532/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 50\n",
            "train: [39/391] Loss: 0.292 | Acc: 89.688% (4592/5120)\n",
            "train: [79/391] Loss: 0.324 | Acc: 88.506% (9063/10240)\n",
            "train: [119/391] Loss: 0.340 | Acc: 87.943% (13508/15360)\n",
            "train: [159/391] Loss: 0.347 | Acc: 87.676% (17956/20480)\n",
            "train: [199/391] Loss: 0.351 | Acc: 87.434% (22383/25600)\n",
            "train: [239/391] Loss: 0.354 | Acc: 87.441% (26862/30720)\n",
            "train: [279/391] Loss: 0.352 | Acc: 87.506% (31362/35840)\n",
            "train: [319/391] Loss: 0.354 | Acc: 87.395% (35797/40960)\n",
            "train: [359/391] Loss: 0.359 | Acc: 87.237% (40199/46080)\n",
            "val: [39/79] Loss: 0.457 | Acc: 86.055% (4406/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 51\n",
            "train: [39/391] Loss: 0.330 | Acc: 88.574% (4535/5120)\n",
            "train: [79/391] Loss: 0.338 | Acc: 88.018% (9013/10240)\n",
            "train: [119/391] Loss: 0.341 | Acc: 87.871% (13497/15360)\n",
            "train: [159/391] Loss: 0.343 | Acc: 87.778% (17977/20480)\n",
            "train: [199/391] Loss: 0.343 | Acc: 87.801% (22477/25600)\n",
            "train: [239/391] Loss: 0.346 | Acc: 87.777% (26965/30720)\n",
            "train: [279/391] Loss: 0.349 | Acc: 87.670% (31421/35840)\n",
            "train: [319/391] Loss: 0.354 | Acc: 87.485% (35834/40960)\n",
            "train: [359/391] Loss: 0.357 | Acc: 87.411% (40279/46080)\n",
            "val: [39/79] Loss: 0.412 | Acc: 86.602% (4434/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 52\n",
            "train: [39/391] Loss: 0.321 | Acc: 88.535% (4533/5120)\n",
            "train: [79/391] Loss: 0.333 | Acc: 88.066% (9018/10240)\n",
            "train: [119/391] Loss: 0.335 | Acc: 87.949% (13509/15360)\n",
            "train: [159/391] Loss: 0.335 | Acc: 87.949% (18012/20480)\n",
            "train: [199/391] Loss: 0.338 | Acc: 87.934% (22511/25600)\n",
            "train: [239/391] Loss: 0.337 | Acc: 88.027% (27042/30720)\n",
            "train: [279/391] Loss: 0.343 | Acc: 87.779% (31460/35840)\n",
            "train: [319/391] Loss: 0.342 | Acc: 87.837% (35978/40960)\n",
            "train: [359/391] Loss: 0.341 | Acc: 87.923% (40515/46080)\n",
            "val: [39/79] Loss: 0.428 | Acc: 86.777% (4443/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 53\n",
            "train: [39/391] Loss: 0.314 | Acc: 88.926% (4553/5120)\n",
            "train: [79/391] Loss: 0.312 | Acc: 89.268% (9141/10240)\n",
            "train: [119/391] Loss: 0.315 | Acc: 89.082% (13683/15360)\n",
            "train: [159/391] Loss: 0.318 | Acc: 88.931% (18213/20480)\n",
            "train: [199/391] Loss: 0.314 | Acc: 89.012% (22787/25600)\n",
            "train: [239/391] Loss: 0.313 | Acc: 88.971% (27332/30720)\n",
            "train: [279/391] Loss: 0.315 | Acc: 88.937% (31875/35840)\n",
            "train: [319/391] Loss: 0.316 | Acc: 88.926% (36424/40960)\n",
            "train: [359/391] Loss: 0.319 | Acc: 88.872% (40952/46080)\n",
            "val: [39/79] Loss: 0.401 | Acc: 87.012% (4455/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 54\n",
            "train: [39/391] Loss: 0.302 | Acc: 89.473% (4581/5120)\n",
            "train: [79/391] Loss: 0.292 | Acc: 89.727% (9188/10240)\n",
            "train: [119/391] Loss: 0.291 | Acc: 89.707% (13779/15360)\n",
            "train: [159/391] Loss: 0.292 | Acc: 89.673% (18365/20480)\n",
            "train: [199/391] Loss: 0.293 | Acc: 89.695% (22962/25600)\n",
            "train: [239/391] Loss: 0.292 | Acc: 89.710% (27559/30720)\n",
            "train: [279/391] Loss: 0.291 | Acc: 89.685% (32143/35840)\n",
            "train: [319/391] Loss: 0.294 | Acc: 89.578% (36691/40960)\n",
            "train: [359/391] Loss: 0.293 | Acc: 89.583% (41280/46080)\n",
            "val: [39/79] Loss: 0.386 | Acc: 87.891% (4500/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 55\n",
            "train: [39/391] Loss: 0.266 | Acc: 90.742% (4646/5120)\n",
            "train: [79/391] Loss: 0.270 | Acc: 90.654% (9283/10240)\n",
            "train: [119/391] Loss: 0.270 | Acc: 90.605% (13917/15360)\n",
            "train: [159/391] Loss: 0.268 | Acc: 90.654% (18566/20480)\n",
            "train: [199/391] Loss: 0.268 | Acc: 90.656% (23208/25600)\n",
            "train: [239/391] Loss: 0.267 | Acc: 90.697% (27862/30720)\n",
            "train: [279/391] Loss: 0.267 | Acc: 90.723% (32515/35840)\n",
            "train: [319/391] Loss: 0.264 | Acc: 90.854% (37214/40960)\n",
            "train: [359/391] Loss: 0.264 | Acc: 90.840% (41859/46080)\n",
            "val: [39/79] Loss: 0.392 | Acc: 87.734% (4492/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 56\n",
            "train: [39/391] Loss: 0.222 | Acc: 92.227% (4722/5120)\n",
            "train: [79/391] Loss: 0.229 | Acc: 92.031% (9424/10240)\n",
            "train: [119/391] Loss: 0.232 | Acc: 91.823% (14104/15360)\n",
            "train: [159/391] Loss: 0.234 | Acc: 91.743% (18789/20480)\n",
            "train: [199/391] Loss: 0.234 | Acc: 91.758% (23490/25600)\n",
            "train: [239/391] Loss: 0.235 | Acc: 91.725% (28178/30720)\n",
            "train: [279/391] Loss: 0.237 | Acc: 91.562% (32816/35840)\n",
            "train: [319/391] Loss: 0.238 | Acc: 91.494% (37476/40960)\n",
            "train: [359/391] Loss: 0.240 | Acc: 91.450% (42140/46080)\n",
            "val: [39/79] Loss: 0.363 | Acc: 88.848% (4549/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 57\n",
            "train: [39/391] Loss: 0.237 | Acc: 91.973% (4709/5120)\n",
            "train: [79/391] Loss: 0.229 | Acc: 91.992% (9420/10240)\n",
            "train: [119/391] Loss: 0.229 | Acc: 92.077% (14143/15360)\n",
            "train: [159/391] Loss: 0.225 | Acc: 92.192% (18881/20480)\n",
            "train: [199/391] Loss: 0.223 | Acc: 92.262% (23619/25600)\n",
            "train: [239/391] Loss: 0.223 | Acc: 92.262% (28343/30720)\n",
            "train: [279/391] Loss: 0.221 | Acc: 92.280% (33073/35840)\n",
            "train: [319/391] Loss: 0.221 | Acc: 92.212% (37770/40960)\n",
            "train: [359/391] Loss: 0.221 | Acc: 92.253% (42510/46080)\n",
            "val: [39/79] Loss: 0.364 | Acc: 88.730% (4543/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 58\n",
            "train: [39/391] Loss: 0.201 | Acc: 93.066% (4765/5120)\n",
            "train: [79/391] Loss: 0.196 | Acc: 93.096% (9533/10240)\n",
            "train: [119/391] Loss: 0.197 | Acc: 93.105% (14301/15360)\n",
            "train: [159/391] Loss: 0.196 | Acc: 93.057% (19058/20480)\n",
            "train: [199/391] Loss: 0.198 | Acc: 93.066% (23825/25600)\n",
            "train: [239/391] Loss: 0.198 | Acc: 93.073% (28592/30720)\n",
            "train: [279/391] Loss: 0.199 | Acc: 93.058% (33352/35840)\n",
            "train: [319/391] Loss: 0.198 | Acc: 93.088% (38129/40960)\n",
            "train: [359/391] Loss: 0.197 | Acc: 93.134% (42916/46080)\n",
            "val: [39/79] Loss: 0.366 | Acc: 89.062% (4560/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 59\n",
            "train: [39/391] Loss: 0.190 | Acc: 93.555% (4790/5120)\n",
            "train: [79/391] Loss: 0.193 | Acc: 93.262% (9550/10240)\n",
            "train: [119/391] Loss: 0.186 | Acc: 93.490% (14360/15360)\n",
            "train: [159/391] Loss: 0.184 | Acc: 93.574% (19164/20480)\n",
            "train: [199/391] Loss: 0.185 | Acc: 93.512% (23939/25600)\n",
            "train: [239/391] Loss: 0.183 | Acc: 93.503% (28724/30720)\n",
            "train: [279/391] Loss: 0.185 | Acc: 93.401% (33475/35840)\n",
            "train: [319/391] Loss: 0.187 | Acc: 93.340% (38232/40960)\n",
            "train: [359/391] Loss: 0.188 | Acc: 93.331% (43007/46080)\n",
            "val: [39/79] Loss: 0.363 | Acc: 89.023% (4558/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 60\n",
            "train: [39/391] Loss: 0.254 | Acc: 91.016% (4660/5120)\n",
            "train: [79/391] Loss: 0.275 | Acc: 90.264% (9243/10240)\n",
            "train: [119/391] Loss: 0.284 | Acc: 89.987% (13822/15360)\n",
            "train: [159/391] Loss: 0.293 | Acc: 89.565% (18343/20480)\n",
            "train: [199/391] Loss: 0.299 | Acc: 89.277% (22855/25600)\n",
            "train: [239/391] Loss: 0.302 | Acc: 89.154% (27388/30720)\n",
            "train: [279/391] Loss: 0.305 | Acc: 89.060% (31919/35840)\n",
            "train: [319/391] Loss: 0.308 | Acc: 88.914% (36419/40960)\n",
            "train: [359/391] Loss: 0.311 | Acc: 88.765% (40903/46080)\n",
            "val: [39/79] Loss: 0.404 | Acc: 87.422% (4476/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 61\n",
            "train: [39/391] Loss: 0.283 | Acc: 90.410% (4629/5120)\n",
            "train: [79/391] Loss: 0.297 | Acc: 89.639% (9179/10240)\n",
            "train: [119/391] Loss: 0.294 | Acc: 89.688% (13776/15360)\n",
            "train: [159/391] Loss: 0.296 | Acc: 89.668% (18364/20480)\n",
            "train: [199/391] Loss: 0.301 | Acc: 89.367% (22878/25600)\n",
            "train: [239/391] Loss: 0.298 | Acc: 89.388% (27460/30720)\n",
            "train: [279/391] Loss: 0.300 | Acc: 89.375% (32032/35840)\n",
            "train: [319/391] Loss: 0.300 | Acc: 89.321% (36586/40960)\n",
            "train: [359/391] Loss: 0.302 | Acc: 89.280% (41140/46080)\n",
            "val: [39/79] Loss: 0.403 | Acc: 87.227% (4466/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 62\n",
            "train: [39/391] Loss: 0.292 | Acc: 89.824% (4599/5120)\n",
            "train: [79/391] Loss: 0.281 | Acc: 90.156% (9232/10240)\n",
            "train: [119/391] Loss: 0.282 | Acc: 90.046% (13831/15360)\n",
            "train: [159/391] Loss: 0.287 | Acc: 89.888% (18409/20480)\n",
            "train: [199/391] Loss: 0.289 | Acc: 89.863% (23005/25600)\n",
            "train: [239/391] Loss: 0.288 | Acc: 89.808% (27589/30720)\n",
            "train: [279/391] Loss: 0.289 | Acc: 89.774% (32175/35840)\n",
            "train: [319/391] Loss: 0.289 | Acc: 89.792% (36779/40960)\n",
            "train: [359/391] Loss: 0.291 | Acc: 89.733% (41349/46080)\n",
            "val: [39/79] Loss: 0.399 | Acc: 87.168% (4463/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 63\n",
            "train: [39/391] Loss: 0.273 | Acc: 90.684% (4643/5120)\n",
            "train: [79/391] Loss: 0.270 | Acc: 90.654% (9283/10240)\n",
            "train: [119/391] Loss: 0.266 | Acc: 90.716% (13934/15360)\n",
            "train: [159/391] Loss: 0.267 | Acc: 90.649% (18565/20480)\n",
            "train: [199/391] Loss: 0.270 | Acc: 90.469% (23160/25600)\n",
            "train: [239/391] Loss: 0.268 | Acc: 90.605% (27834/30720)\n",
            "train: [279/391] Loss: 0.268 | Acc: 90.541% (32450/35840)\n",
            "train: [319/391] Loss: 0.269 | Acc: 90.493% (37066/40960)\n",
            "train: [359/391] Loss: 0.270 | Acc: 90.425% (41668/46080)\n",
            "val: [39/79] Loss: 0.408 | Acc: 86.992% (4454/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 64\n",
            "train: [39/391] Loss: 0.247 | Acc: 91.035% (4661/5120)\n",
            "train: [79/391] Loss: 0.254 | Acc: 90.732% (9291/10240)\n",
            "train: [119/391] Loss: 0.247 | Acc: 90.977% (13974/15360)\n",
            "train: [159/391] Loss: 0.248 | Acc: 91.001% (18637/20480)\n",
            "train: [199/391] Loss: 0.250 | Acc: 91.102% (23322/25600)\n",
            "train: [239/391] Loss: 0.249 | Acc: 91.182% (28011/30720)\n",
            "train: [279/391] Loss: 0.249 | Acc: 91.138% (32664/35840)\n",
            "train: [319/391] Loss: 0.252 | Acc: 91.057% (37297/40960)\n",
            "train: [359/391] Loss: 0.251 | Acc: 91.087% (41973/46080)\n",
            "val: [39/79] Loss: 0.400 | Acc: 87.832% (4497/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 65\n",
            "train: [39/391] Loss: 0.243 | Acc: 91.602% (4690/5120)\n",
            "train: [79/391] Loss: 0.229 | Acc: 92.012% (9422/10240)\n",
            "train: [119/391] Loss: 0.223 | Acc: 92.194% (14161/15360)\n",
            "train: [159/391] Loss: 0.222 | Acc: 92.100% (18862/20480)\n",
            "train: [199/391] Loss: 0.223 | Acc: 92.008% (23554/25600)\n",
            "train: [239/391] Loss: 0.224 | Acc: 91.921% (28238/30720)\n",
            "train: [279/391] Loss: 0.225 | Acc: 91.897% (32936/35840)\n",
            "train: [319/391] Loss: 0.226 | Acc: 91.853% (37623/40960)\n",
            "train: [359/391] Loss: 0.226 | Acc: 91.825% (42313/46080)\n",
            "val: [39/79] Loss: 0.383 | Acc: 88.379% (4525/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 66\n",
            "train: [39/391] Loss: 0.198 | Acc: 93.047% (4764/5120)\n",
            "train: [79/391] Loss: 0.203 | Acc: 92.734% (9496/10240)\n",
            "train: [119/391] Loss: 0.199 | Acc: 92.943% (14276/15360)\n",
            "train: [159/391] Loss: 0.198 | Acc: 93.032% (19053/20480)\n",
            "train: [199/391] Loss: 0.197 | Acc: 93.035% (23817/25600)\n",
            "train: [239/391] Loss: 0.200 | Acc: 92.988% (28566/30720)\n",
            "train: [279/391] Loss: 0.199 | Acc: 92.935% (33308/35840)\n",
            "train: [319/391] Loss: 0.200 | Acc: 92.874% (38041/40960)\n",
            "train: [359/391] Loss: 0.201 | Acc: 92.834% (42778/46080)\n",
            "val: [39/79] Loss: 0.365 | Acc: 88.711% (4542/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 67\n",
            "train: [39/391] Loss: 0.176 | Acc: 93.711% (4798/5120)\n",
            "train: [79/391] Loss: 0.176 | Acc: 93.740% (9599/10240)\n",
            "train: [119/391] Loss: 0.179 | Acc: 93.587% (14375/15360)\n",
            "train: [159/391] Loss: 0.181 | Acc: 93.545% (19158/20480)\n",
            "train: [199/391] Loss: 0.181 | Acc: 93.562% (23952/25600)\n",
            "train: [239/391] Loss: 0.181 | Acc: 93.535% (28734/30720)\n",
            "train: [279/391] Loss: 0.177 | Acc: 93.619% (33553/35840)\n",
            "train: [319/391] Loss: 0.177 | Acc: 93.640% (38355/40960)\n",
            "train: [359/391] Loss: 0.178 | Acc: 93.579% (43121/46080)\n",
            "val: [39/79] Loss: 0.368 | Acc: 89.434% (4579/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 68\n",
            "train: [39/391] Loss: 0.170 | Acc: 94.414% (4834/5120)\n",
            "train: [79/391] Loss: 0.167 | Acc: 94.150% (9641/10240)\n",
            "train: [119/391] Loss: 0.164 | Acc: 94.167% (14464/15360)\n",
            "train: [159/391] Loss: 0.165 | Acc: 94.106% (19273/20480)\n",
            "train: [199/391] Loss: 0.166 | Acc: 94.152% (24103/25600)\n",
            "train: [239/391] Loss: 0.164 | Acc: 94.222% (28945/30720)\n",
            "train: [279/391] Loss: 0.162 | Acc: 94.277% (33789/35840)\n",
            "train: [319/391] Loss: 0.161 | Acc: 94.333% (38639/40960)\n",
            "train: [359/391] Loss: 0.162 | Acc: 94.262% (43436/46080)\n",
            "val: [39/79] Loss: 0.373 | Acc: 89.492% (4582/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 69\n",
            "train: [39/391] Loss: 0.157 | Acc: 94.219% (4824/5120)\n",
            "train: [79/391] Loss: 0.159 | Acc: 94.072% (9633/10240)\n",
            "train: [119/391] Loss: 0.157 | Acc: 94.264% (14479/15360)\n",
            "train: [159/391] Loss: 0.155 | Acc: 94.370% (19327/20480)\n",
            "train: [199/391] Loss: 0.155 | Acc: 94.414% (24170/25600)\n",
            "train: [239/391] Loss: 0.157 | Acc: 94.339% (28981/30720)\n",
            "train: [279/391] Loss: 0.156 | Acc: 94.422% (33841/35840)\n",
            "train: [319/391] Loss: 0.156 | Acc: 94.431% (38679/40960)\n",
            "train: [359/391] Loss: 0.156 | Acc: 94.421% (43509/46080)\n",
            "val: [39/79] Loss: 0.368 | Acc: 89.707% (4593/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 70\n",
            "train: [39/391] Loss: 0.224 | Acc: 92.402% (4731/5120)\n",
            "train: [79/391] Loss: 0.246 | Acc: 91.523% (9372/10240)\n",
            "train: [119/391] Loss: 0.255 | Acc: 91.146% (14000/15360)\n",
            "train: [159/391] Loss: 0.261 | Acc: 90.889% (18614/20480)\n",
            "train: [199/391] Loss: 0.265 | Acc: 90.707% (23221/25600)\n",
            "train: [239/391] Loss: 0.266 | Acc: 90.514% (27806/30720)\n",
            "train: [279/391] Loss: 0.267 | Acc: 90.519% (32442/35840)\n",
            "train: [319/391] Loss: 0.268 | Acc: 90.549% (37089/40960)\n",
            "train: [359/391] Loss: 0.269 | Acc: 90.525% (41714/46080)\n",
            "val: [39/79] Loss: 0.413 | Acc: 86.895% (4449/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 71\n",
            "train: [39/391] Loss: 0.265 | Acc: 90.703% (4644/5120)\n",
            "train: [79/391] Loss: 0.262 | Acc: 90.801% (9298/10240)\n",
            "train: [119/391] Loss: 0.259 | Acc: 90.840% (13953/15360)\n",
            "train: [159/391] Loss: 0.258 | Acc: 90.952% (18627/20480)\n",
            "train: [199/391] Loss: 0.265 | Acc: 90.719% (23224/25600)\n",
            "train: [239/391] Loss: 0.269 | Acc: 90.557% (27819/30720)\n",
            "train: [279/391] Loss: 0.268 | Acc: 90.513% (32440/35840)\n",
            "train: [319/391] Loss: 0.271 | Acc: 90.413% (37033/40960)\n",
            "train: [359/391] Loss: 0.270 | Acc: 90.447% (41678/46080)\n",
            "val: [39/79] Loss: 0.404 | Acc: 87.168% (4463/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 72\n",
            "train: [39/391] Loss: 0.252 | Acc: 91.035% (4661/5120)\n",
            "train: [79/391] Loss: 0.250 | Acc: 91.191% (9338/10240)\n",
            "train: [119/391] Loss: 0.243 | Acc: 91.426% (14043/15360)\n",
            "train: [159/391] Loss: 0.244 | Acc: 91.313% (18701/20480)\n",
            "train: [199/391] Loss: 0.248 | Acc: 91.156% (23336/25600)\n",
            "train: [239/391] Loss: 0.250 | Acc: 91.087% (27982/30720)\n",
            "train: [279/391] Loss: 0.253 | Acc: 90.932% (32590/35840)\n",
            "train: [319/391] Loss: 0.253 | Acc: 90.930% (37245/40960)\n",
            "train: [359/391] Loss: 0.256 | Acc: 90.825% (41852/46080)\n",
            "val: [39/79] Loss: 0.395 | Acc: 87.305% (4470/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 73\n",
            "train: [39/391] Loss: 0.225 | Acc: 91.758% (4698/5120)\n",
            "train: [79/391] Loss: 0.231 | Acc: 91.543% (9374/10240)\n",
            "train: [119/391] Loss: 0.240 | Acc: 91.458% (14048/15360)\n",
            "train: [159/391] Loss: 0.243 | Acc: 91.523% (18744/20480)\n",
            "train: [199/391] Loss: 0.245 | Acc: 91.398% (23398/25600)\n",
            "train: [239/391] Loss: 0.247 | Acc: 91.270% (28038/30720)\n",
            "train: [279/391] Loss: 0.248 | Acc: 91.258% (32707/35840)\n",
            "train: [319/391] Loss: 0.247 | Acc: 91.230% (37368/40960)\n",
            "train: [359/391] Loss: 0.249 | Acc: 91.198% (42024/46080)\n",
            "val: [39/79] Loss: 0.403 | Acc: 87.656% (4488/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 74\n",
            "train: [39/391] Loss: 0.230 | Acc: 92.070% (4714/5120)\n",
            "train: [79/391] Loss: 0.219 | Acc: 92.324% (9454/10240)\n",
            "train: [119/391] Loss: 0.218 | Acc: 92.363% (14187/15360)\n",
            "train: [159/391] Loss: 0.217 | Acc: 92.285% (18900/20480)\n",
            "train: [199/391] Loss: 0.218 | Acc: 92.273% (23622/25600)\n",
            "train: [239/391] Loss: 0.216 | Acc: 92.354% (28371/30720)\n",
            "train: [279/391] Loss: 0.218 | Acc: 92.291% (33077/35840)\n",
            "train: [319/391] Loss: 0.217 | Acc: 92.297% (37805/40960)\n",
            "train: [359/391] Loss: 0.219 | Acc: 92.198% (42485/46080)\n",
            "val: [39/79] Loss: 0.399 | Acc: 87.500% (4480/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 75\n",
            "train: [39/391] Loss: 0.193 | Acc: 92.871% (4755/5120)\n",
            "train: [79/391] Loss: 0.190 | Acc: 93.135% (9537/10240)\n",
            "train: [119/391] Loss: 0.195 | Acc: 92.865% (14264/15360)\n",
            "train: [159/391] Loss: 0.192 | Acc: 93.008% (19048/20480)\n",
            "train: [199/391] Loss: 0.194 | Acc: 93.039% (23818/25600)\n",
            "train: [239/391] Loss: 0.195 | Acc: 93.024% (28577/30720)\n",
            "train: [279/391] Loss: 0.196 | Acc: 93.013% (33336/35840)\n",
            "train: [319/391] Loss: 0.197 | Acc: 93.010% (38097/40960)\n",
            "train: [359/391] Loss: 0.199 | Acc: 92.949% (42831/46080)\n",
            "val: [39/79] Loss: 0.394 | Acc: 88.066% (4509/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 76\n",
            "train: [39/391] Loss: 0.185 | Acc: 93.242% (4774/5120)\n",
            "train: [79/391] Loss: 0.177 | Acc: 93.584% (9583/10240)\n",
            "train: [119/391] Loss: 0.174 | Acc: 93.737% (14398/15360)\n",
            "train: [159/391] Loss: 0.171 | Acc: 93.755% (19201/20480)\n",
            "train: [199/391] Loss: 0.172 | Acc: 93.766% (24004/25600)\n",
            "train: [239/391] Loss: 0.171 | Acc: 93.789% (28812/30720)\n",
            "train: [279/391] Loss: 0.171 | Acc: 93.823% (33626/35840)\n",
            "train: [319/391] Loss: 0.170 | Acc: 93.889% (38457/40960)\n",
            "train: [359/391] Loss: 0.171 | Acc: 93.850% (43246/46080)\n",
            "val: [39/79] Loss: 0.393 | Acc: 88.438% (4528/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 77\n",
            "train: [39/391] Loss: 0.155 | Acc: 94.473% (4837/5120)\n",
            "train: [79/391] Loss: 0.154 | Acc: 94.512% (9678/10240)\n",
            "train: [119/391] Loss: 0.155 | Acc: 94.642% (14537/15360)\n",
            "train: [159/391] Loss: 0.156 | Acc: 94.497% (19353/20480)\n",
            "train: [199/391] Loss: 0.154 | Acc: 94.496% (24191/25600)\n",
            "train: [239/391] Loss: 0.151 | Acc: 94.596% (29060/30720)\n",
            "train: [279/391] Loss: 0.151 | Acc: 94.623% (33913/35840)\n",
            "train: [319/391] Loss: 0.151 | Acc: 94.602% (38749/40960)\n",
            "train: [359/391] Loss: 0.151 | Acc: 94.618% (43600/46080)\n",
            "val: [39/79] Loss: 0.397 | Acc: 88.887% (4551/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 78\n",
            "train: [39/391] Loss: 0.139 | Acc: 95.195% (4874/5120)\n",
            "train: [79/391] Loss: 0.137 | Acc: 95.205% (9749/10240)\n",
            "train: [119/391] Loss: 0.141 | Acc: 95.150% (14615/15360)\n",
            "train: [159/391] Loss: 0.138 | Acc: 95.176% (19492/20480)\n",
            "train: [199/391] Loss: 0.140 | Acc: 95.074% (24339/25600)\n",
            "train: [239/391] Loss: 0.140 | Acc: 95.062% (29203/30720)\n",
            "train: [279/391] Loss: 0.141 | Acc: 95.000% (34048/35840)\n",
            "train: [319/391] Loss: 0.142 | Acc: 95.007% (38915/40960)\n",
            "train: [359/391] Loss: 0.140 | Acc: 95.069% (43808/46080)\n",
            "val: [39/79] Loss: 0.386 | Acc: 89.121% (4563/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 79\n",
            "train: [39/391] Loss: 0.132 | Acc: 95.449% (4887/5120)\n",
            "train: [79/391] Loss: 0.132 | Acc: 95.283% (9757/10240)\n",
            "train: [119/391] Loss: 0.132 | Acc: 95.299% (14638/15360)\n",
            "train: [159/391] Loss: 0.135 | Acc: 95.249% (19507/20480)\n",
            "train: [199/391] Loss: 0.134 | Acc: 95.285% (24393/25600)\n",
            "train: [239/391] Loss: 0.132 | Acc: 95.361% (29295/30720)\n",
            "train: [279/391] Loss: 0.131 | Acc: 95.357% (34176/35840)\n",
            "train: [319/391] Loss: 0.130 | Acc: 95.398% (39075/40960)\n",
            "train: [359/391] Loss: 0.128 | Acc: 95.460% (43988/46080)\n",
            "val: [39/79] Loss: 0.385 | Acc: 89.160% (4565/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 80\n",
            "train: [39/391] Loss: 0.186 | Acc: 93.711% (4798/5120)\n",
            "train: [79/391] Loss: 0.212 | Acc: 92.539% (9476/10240)\n",
            "train: [119/391] Loss: 0.221 | Acc: 92.096% (14146/15360)\n",
            "train: [159/391] Loss: 0.229 | Acc: 91.851% (18811/20480)\n",
            "train: [199/391] Loss: 0.236 | Acc: 91.527% (23431/25600)\n",
            "train: [239/391] Loss: 0.237 | Acc: 91.543% (28122/30720)\n",
            "train: [279/391] Loss: 0.238 | Acc: 91.509% (32797/35840)\n",
            "train: [319/391] Loss: 0.240 | Acc: 91.428% (37449/40960)\n",
            "train: [359/391] Loss: 0.240 | Acc: 91.400% (42117/46080)\n",
            "val: [39/79] Loss: 0.416 | Acc: 87.070% (4458/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 81\n",
            "train: [39/391] Loss: 0.229 | Acc: 91.387% (4679/5120)\n",
            "train: [79/391] Loss: 0.232 | Acc: 91.582% (9378/10240)\n",
            "train: [119/391] Loss: 0.234 | Acc: 91.621% (14073/15360)\n",
            "train: [159/391] Loss: 0.236 | Acc: 91.426% (18724/20480)\n",
            "train: [199/391] Loss: 0.237 | Acc: 91.426% (23405/25600)\n",
            "train: [239/391] Loss: 0.237 | Acc: 91.488% (28105/30720)\n",
            "train: [279/391] Loss: 0.242 | Acc: 91.295% (32720/35840)\n",
            "train: [319/391] Loss: 0.241 | Acc: 91.321% (37405/40960)\n",
            "train: [359/391] Loss: 0.243 | Acc: 91.270% (42057/46080)\n",
            "val: [39/79] Loss: 0.397 | Acc: 87.520% (4481/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 82\n",
            "train: [39/391] Loss: 0.226 | Acc: 91.660% (4693/5120)\n",
            "train: [79/391] Loss: 0.232 | Acc: 91.523% (9372/10240)\n",
            "train: [119/391] Loss: 0.226 | Acc: 91.842% (14107/15360)\n",
            "train: [159/391] Loss: 0.227 | Acc: 91.934% (18828/20480)\n",
            "train: [199/391] Loss: 0.226 | Acc: 92.047% (23564/25600)\n",
            "train: [239/391] Loss: 0.226 | Acc: 92.021% (28269/30720)\n",
            "train: [279/391] Loss: 0.227 | Acc: 91.936% (32950/35840)\n",
            "train: [319/391] Loss: 0.227 | Acc: 91.885% (37636/40960)\n",
            "train: [359/391] Loss: 0.229 | Acc: 91.821% (42311/46080)\n",
            "val: [39/79] Loss: 0.393 | Acc: 87.656% (4488/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 83\n",
            "train: [39/391] Loss: 0.208 | Acc: 92.598% (4741/5120)\n",
            "train: [79/391] Loss: 0.207 | Acc: 92.734% (9496/10240)\n",
            "train: [119/391] Loss: 0.206 | Acc: 92.760% (14248/15360)\n",
            "train: [159/391] Loss: 0.207 | Acc: 92.793% (19004/20480)\n",
            "train: [199/391] Loss: 0.207 | Acc: 92.770% (23749/25600)\n",
            "train: [239/391] Loss: 0.208 | Acc: 92.708% (28480/30720)\n",
            "train: [279/391] Loss: 0.211 | Acc: 92.592% (33185/35840)\n",
            "train: [319/391] Loss: 0.213 | Acc: 92.512% (37893/40960)\n",
            "train: [359/391] Loss: 0.215 | Acc: 92.437% (42595/46080)\n",
            "val: [39/79] Loss: 0.390 | Acc: 88.125% (4512/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 84\n",
            "train: [39/391] Loss: 0.201 | Acc: 92.500% (4736/5120)\n",
            "train: [79/391] Loss: 0.195 | Acc: 92.842% (9507/10240)\n",
            "train: [119/391] Loss: 0.195 | Acc: 92.975% (14281/15360)\n",
            "train: [159/391] Loss: 0.194 | Acc: 92.998% (19046/20480)\n",
            "train: [199/391] Loss: 0.191 | Acc: 93.113% (23837/25600)\n",
            "train: [239/391] Loss: 0.191 | Acc: 93.109% (28603/30720)\n",
            "train: [279/391] Loss: 0.192 | Acc: 93.080% (33360/35840)\n",
            "train: [319/391] Loss: 0.194 | Acc: 92.988% (38088/40960)\n",
            "train: [359/391] Loss: 0.194 | Acc: 92.984% (42847/46080)\n",
            "val: [39/79] Loss: 0.396 | Acc: 88.105% (4511/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 85\n",
            "train: [39/391] Loss: 0.187 | Acc: 93.359% (4780/5120)\n",
            "train: [79/391] Loss: 0.181 | Acc: 93.662% (9591/10240)\n",
            "train: [119/391] Loss: 0.176 | Acc: 93.802% (14408/15360)\n",
            "train: [159/391] Loss: 0.171 | Acc: 94.102% (19272/20480)\n",
            "train: [199/391] Loss: 0.172 | Acc: 94.098% (24089/25600)\n",
            "train: [239/391] Loss: 0.171 | Acc: 94.098% (28907/30720)\n",
            "train: [279/391] Loss: 0.171 | Acc: 94.057% (33710/35840)\n",
            "train: [319/391] Loss: 0.170 | Acc: 94.102% (38544/40960)\n",
            "train: [359/391] Loss: 0.171 | Acc: 94.095% (43359/46080)\n",
            "val: [39/79] Loss: 0.394 | Acc: 88.398% (4526/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 86\n",
            "train: [39/391] Loss: 0.146 | Acc: 94.590% (4843/5120)\n",
            "train: [79/391] Loss: 0.143 | Acc: 94.492% (9676/10240)\n",
            "train: [119/391] Loss: 0.143 | Acc: 94.668% (14541/15360)\n",
            "train: [159/391] Loss: 0.148 | Acc: 94.570% (19368/20480)\n",
            "train: [199/391] Loss: 0.148 | Acc: 94.617% (24222/25600)\n",
            "train: [239/391] Loss: 0.149 | Acc: 94.593% (29059/30720)\n",
            "train: [279/391] Loss: 0.150 | Acc: 94.604% (33906/35840)\n",
            "train: [319/391] Loss: 0.150 | Acc: 94.563% (38733/40960)\n",
            "train: [359/391] Loss: 0.152 | Acc: 94.544% (43566/46080)\n",
            "val: [39/79] Loss: 0.379 | Acc: 88.926% (4553/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 87\n",
            "train: [39/391] Loss: 0.136 | Acc: 95.332% (4881/5120)\n",
            "train: [79/391] Loss: 0.136 | Acc: 95.391% (9768/10240)\n",
            "train: [119/391] Loss: 0.136 | Acc: 95.326% (14642/15360)\n",
            "train: [159/391] Loss: 0.136 | Acc: 95.312% (19520/20480)\n",
            "train: [199/391] Loss: 0.134 | Acc: 95.371% (24415/25600)\n",
            "train: [239/391] Loss: 0.134 | Acc: 95.319% (29282/30720)\n",
            "train: [279/391] Loss: 0.133 | Acc: 95.335% (34168/35840)\n",
            "train: [319/391] Loss: 0.134 | Acc: 95.308% (39038/40960)\n",
            "train: [359/391] Loss: 0.134 | Acc: 95.302% (43915/46080)\n",
            "val: [39/79] Loss: 0.376 | Acc: 89.316% (4573/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 88\n",
            "train: [39/391] Loss: 0.120 | Acc: 95.879% (4909/5120)\n",
            "train: [79/391] Loss: 0.115 | Acc: 96.064% (9837/10240)\n",
            "train: [119/391] Loss: 0.118 | Acc: 95.801% (14715/15360)\n",
            "train: [159/391] Loss: 0.118 | Acc: 95.791% (19618/20480)\n",
            "train: [199/391] Loss: 0.118 | Acc: 95.789% (24522/25600)\n",
            "train: [239/391] Loss: 0.118 | Acc: 95.768% (29420/30720)\n",
            "train: [279/391] Loss: 0.119 | Acc: 95.806% (34337/35840)\n",
            "train: [319/391] Loss: 0.117 | Acc: 95.864% (39266/40960)\n",
            "train: [359/391] Loss: 0.118 | Acc: 95.855% (44170/46080)\n",
            "val: [39/79] Loss: 0.375 | Acc: 89.746% (4595/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 89\n",
            "train: [39/391] Loss: 0.112 | Acc: 95.684% (4899/5120)\n",
            "train: [79/391] Loss: 0.114 | Acc: 95.781% (9808/10240)\n",
            "train: [119/391] Loss: 0.112 | Acc: 95.833% (14720/15360)\n",
            "train: [159/391] Loss: 0.109 | Acc: 95.967% (19654/20480)\n",
            "train: [199/391] Loss: 0.109 | Acc: 96.078% (24596/25600)\n",
            "train: [239/391] Loss: 0.108 | Acc: 96.126% (29530/30720)\n",
            "train: [279/391] Loss: 0.108 | Acc: 96.102% (34443/35840)\n",
            "train: [319/391] Loss: 0.107 | Acc: 96.128% (39374/40960)\n",
            "train: [359/391] Loss: 0.108 | Acc: 96.098% (44282/46080)\n",
            "val: [39/79] Loss: 0.379 | Acc: 89.707% (4593/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 90\n",
            "train: [39/391] Loss: 0.151 | Acc: 94.336% (4830/5120)\n",
            "train: [79/391] Loss: 0.192 | Acc: 93.057% (9529/10240)\n",
            "train: [119/391] Loss: 0.205 | Acc: 92.643% (14230/15360)\n",
            "train: [159/391] Loss: 0.209 | Acc: 92.524% (18949/20480)\n",
            "train: [199/391] Loss: 0.216 | Acc: 92.328% (23636/25600)\n",
            "train: [239/391] Loss: 0.217 | Acc: 92.285% (28350/30720)\n",
            "train: [279/391] Loss: 0.218 | Acc: 92.238% (33058/35840)\n",
            "train: [319/391] Loss: 0.220 | Acc: 92.168% (37752/40960)\n",
            "train: [359/391] Loss: 0.222 | Acc: 92.092% (42436/46080)\n",
            "val: [39/79] Loss: 0.409 | Acc: 87.441% (4477/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 91\n",
            "train: [39/391] Loss: 0.214 | Acc: 92.285% (4725/5120)\n",
            "train: [79/391] Loss: 0.214 | Acc: 92.275% (9449/10240)\n",
            "train: [119/391] Loss: 0.213 | Acc: 92.409% (14194/15360)\n",
            "train: [159/391] Loss: 0.214 | Acc: 92.344% (18912/20480)\n",
            "train: [199/391] Loss: 0.213 | Acc: 92.328% (23636/25600)\n",
            "train: [239/391] Loss: 0.213 | Acc: 92.438% (28397/30720)\n",
            "train: [279/391] Loss: 0.214 | Acc: 92.400% (33116/35840)\n",
            "train: [319/391] Loss: 0.217 | Acc: 92.285% (37800/40960)\n",
            "train: [359/391] Loss: 0.218 | Acc: 92.244% (42506/46080)\n",
            "val: [39/79] Loss: 0.429 | Acc: 86.973% (4453/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 92\n",
            "train: [39/391] Loss: 0.199 | Acc: 93.086% (4766/5120)\n",
            "train: [79/391] Loss: 0.194 | Acc: 93.252% (9549/10240)\n",
            "train: [119/391] Loss: 0.193 | Acc: 93.164% (14310/15360)\n",
            "train: [159/391] Loss: 0.196 | Acc: 93.130% (19073/20480)\n",
            "train: [199/391] Loss: 0.198 | Acc: 93.039% (23818/25600)\n",
            "train: [239/391] Loss: 0.199 | Acc: 92.969% (28560/30720)\n",
            "train: [279/391] Loss: 0.202 | Acc: 92.893% (33293/35840)\n",
            "train: [319/391] Loss: 0.203 | Acc: 92.891% (38048/40960)\n",
            "train: [359/391] Loss: 0.204 | Acc: 92.852% (42786/46080)\n",
            "val: [39/79] Loss: 0.392 | Acc: 88.086% (4510/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 93\n",
            "train: [39/391] Loss: 0.210 | Acc: 92.539% (4738/5120)\n",
            "train: [79/391] Loss: 0.199 | Acc: 93.018% (9525/10240)\n",
            "train: [119/391] Loss: 0.195 | Acc: 93.171% (14311/15360)\n",
            "train: [159/391] Loss: 0.193 | Acc: 93.154% (19078/20480)\n",
            "train: [199/391] Loss: 0.193 | Acc: 93.188% (23856/25600)\n",
            "train: [239/391] Loss: 0.192 | Acc: 93.236% (28642/30720)\n",
            "train: [279/391] Loss: 0.193 | Acc: 93.119% (33374/35840)\n",
            "train: [319/391] Loss: 0.195 | Acc: 93.083% (38127/40960)\n",
            "train: [359/391] Loss: 0.193 | Acc: 93.138% (42918/46080)\n",
            "val: [39/79] Loss: 0.402 | Acc: 88.027% (4507/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 94\n",
            "train: [39/391] Loss: 0.175 | Acc: 93.652% (4795/5120)\n",
            "train: [79/391] Loss: 0.173 | Acc: 93.740% (9599/10240)\n",
            "train: [119/391] Loss: 0.172 | Acc: 93.874% (14419/15360)\n",
            "train: [159/391] Loss: 0.172 | Acc: 93.877% (19226/20480)\n",
            "train: [199/391] Loss: 0.171 | Acc: 93.910% (24041/25600)\n",
            "train: [239/391] Loss: 0.169 | Acc: 94.014% (28881/30720)\n",
            "train: [279/391] Loss: 0.170 | Acc: 93.951% (33672/35840)\n",
            "train: [319/391] Loss: 0.171 | Acc: 93.862% (38446/40960)\n",
            "train: [359/391] Loss: 0.170 | Acc: 93.893% (43266/46080)\n",
            "val: [39/79] Loss: 0.393 | Acc: 88.652% (4539/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 95\n",
            "train: [39/391] Loss: 0.169 | Acc: 93.965% (4811/5120)\n",
            "train: [79/391] Loss: 0.161 | Acc: 94.238% (9650/10240)\n",
            "train: [119/391] Loss: 0.158 | Acc: 94.388% (14498/15360)\n",
            "train: [159/391] Loss: 0.157 | Acc: 94.375% (19328/20480)\n",
            "train: [199/391] Loss: 0.155 | Acc: 94.484% (24188/25600)\n",
            "train: [239/391] Loss: 0.153 | Acc: 94.583% (29056/30720)\n",
            "train: [279/391] Loss: 0.152 | Acc: 94.581% (33898/35840)\n",
            "train: [319/391] Loss: 0.151 | Acc: 94.619% (38756/40960)\n",
            "train: [359/391] Loss: 0.150 | Acc: 94.666% (43622/46080)\n",
            "val: [39/79] Loss: 0.411 | Acc: 88.340% (4523/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 96\n",
            "train: [39/391] Loss: 0.144 | Acc: 94.688% (4848/5120)\n",
            "train: [79/391] Loss: 0.140 | Acc: 94.834% (9711/10240)\n",
            "train: [119/391] Loss: 0.138 | Acc: 94.922% (14580/15360)\n",
            "train: [159/391] Loss: 0.134 | Acc: 95.093% (19475/20480)\n",
            "train: [199/391] Loss: 0.132 | Acc: 95.199% (24371/25600)\n",
            "train: [239/391] Loss: 0.130 | Acc: 95.234% (29256/30720)\n",
            "train: [279/391] Loss: 0.131 | Acc: 95.246% (34136/35840)\n",
            "train: [319/391] Loss: 0.131 | Acc: 95.261% (39019/40960)\n",
            "train: [359/391] Loss: 0.130 | Acc: 95.291% (43910/46080)\n",
            "val: [39/79] Loss: 0.410 | Acc: 88.965% (4555/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 97\n",
            "train: [39/391] Loss: 0.109 | Acc: 96.152% (4923/5120)\n",
            "train: [79/391] Loss: 0.111 | Acc: 96.143% (9845/10240)\n",
            "train: [119/391] Loss: 0.110 | Acc: 96.133% (14766/15360)\n",
            "train: [159/391] Loss: 0.112 | Acc: 95.991% (19659/20480)\n",
            "train: [199/391] Loss: 0.111 | Acc: 96.027% (24583/25600)\n",
            "train: [239/391] Loss: 0.112 | Acc: 95.999% (29491/30720)\n",
            "train: [279/391] Loss: 0.111 | Acc: 96.046% (34423/35840)\n",
            "train: [319/391] Loss: 0.110 | Acc: 96.040% (39338/40960)\n",
            "train: [359/391] Loss: 0.110 | Acc: 96.035% (44253/46080)\n",
            "val: [39/79] Loss: 0.404 | Acc: 89.004% (4557/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 98\n",
            "train: [39/391] Loss: 0.113 | Acc: 95.664% (4898/5120)\n",
            "train: [79/391] Loss: 0.111 | Acc: 95.967% (9827/10240)\n",
            "train: [119/391] Loss: 0.107 | Acc: 96.016% (14748/15360)\n",
            "train: [159/391] Loss: 0.107 | Acc: 96.055% (19672/20480)\n",
            "train: [199/391] Loss: 0.105 | Acc: 96.180% (24622/25600)\n",
            "train: [239/391] Loss: 0.105 | Acc: 96.204% (29554/30720)\n",
            "train: [279/391] Loss: 0.104 | Acc: 96.203% (34479/35840)\n",
            "train: [319/391] Loss: 0.103 | Acc: 96.208% (39407/40960)\n",
            "train: [359/391] Loss: 0.103 | Acc: 96.220% (44338/46080)\n",
            "val: [39/79] Loss: 0.406 | Acc: 89.121% (4563/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 99\n",
            "train: [39/391] Loss: 0.090 | Acc: 96.523% (4942/5120)\n",
            "train: [79/391] Loss: 0.091 | Acc: 96.494% (9881/10240)\n",
            "train: [119/391] Loss: 0.088 | Acc: 96.654% (14846/15360)\n",
            "train: [159/391] Loss: 0.090 | Acc: 96.636% (19791/20480)\n",
            "train: [199/391] Loss: 0.091 | Acc: 96.645% (24741/25600)\n",
            "train: [239/391] Loss: 0.091 | Acc: 96.719% (29712/30720)\n",
            "train: [279/391] Loss: 0.090 | Acc: 96.738% (34671/35840)\n",
            "train: [319/391] Loss: 0.091 | Acc: 96.741% (39625/40960)\n",
            "train: [359/391] Loss: 0.092 | Acc: 96.760% (44587/46080)\n",
            "val: [39/79] Loss: 0.402 | Acc: 89.277% (4571/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 100\n",
            "train: [39/391] Loss: 0.142 | Acc: 95.078% (4868/5120)\n",
            "train: [79/391] Loss: 0.168 | Acc: 93.984% (9624/10240)\n",
            "train: [119/391] Loss: 0.180 | Acc: 93.639% (14383/15360)\n",
            "train: [159/391] Loss: 0.189 | Acc: 93.262% (19100/20480)\n",
            "train: [199/391] Loss: 0.189 | Acc: 93.262% (23875/25600)\n",
            "train: [239/391] Loss: 0.192 | Acc: 93.115% (28605/30720)\n",
            "train: [279/391] Loss: 0.195 | Acc: 92.955% (33315/35840)\n",
            "train: [319/391] Loss: 0.198 | Acc: 92.832% (38024/40960)\n",
            "train: [359/391] Loss: 0.199 | Acc: 92.791% (42758/46080)\n",
            "val: [39/79] Loss: 0.411 | Acc: 87.949% (4503/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 101\n",
            "train: [39/391] Loss: 0.184 | Acc: 92.988% (4761/5120)\n",
            "train: [79/391] Loss: 0.179 | Acc: 93.438% (9568/10240)\n",
            "train: [119/391] Loss: 0.185 | Acc: 93.255% (14324/15360)\n",
            "train: [159/391] Loss: 0.190 | Acc: 93.125% (19072/20480)\n",
            "train: [199/391] Loss: 0.192 | Acc: 93.023% (23814/25600)\n",
            "train: [239/391] Loss: 0.197 | Acc: 92.848% (28523/30720)\n",
            "train: [279/391] Loss: 0.198 | Acc: 92.860% (33281/35840)\n",
            "train: [319/391] Loss: 0.199 | Acc: 92.861% (38036/40960)\n",
            "train: [359/391] Loss: 0.200 | Acc: 92.830% (42776/46080)\n",
            "val: [39/79] Loss: 0.405 | Acc: 88.496% (4531/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 102\n",
            "train: [39/391] Loss: 0.175 | Acc: 93.750% (4800/5120)\n",
            "train: [79/391] Loss: 0.188 | Acc: 93.291% (9553/10240)\n",
            "train: [119/391] Loss: 0.188 | Acc: 93.320% (14334/15360)\n",
            "train: [159/391] Loss: 0.193 | Acc: 93.154% (19078/20480)\n",
            "train: [199/391] Loss: 0.191 | Acc: 93.215% (23863/25600)\n",
            "train: [239/391] Loss: 0.188 | Acc: 93.327% (28670/30720)\n",
            "train: [279/391] Loss: 0.187 | Acc: 93.348% (33456/35840)\n",
            "train: [319/391] Loss: 0.190 | Acc: 93.276% (38206/40960)\n",
            "train: [359/391] Loss: 0.198 | Acc: 93.056% (42880/46080)\n",
            "val: [39/79] Loss: 0.400 | Acc: 87.773% (4494/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 103\n",
            "train: [39/391] Loss: 0.178 | Acc: 93.359% (4780/5120)\n",
            "train: [79/391] Loss: 0.181 | Acc: 93.262% (9550/10240)\n",
            "train: [119/391] Loss: 0.181 | Acc: 93.372% (14342/15360)\n",
            "train: [159/391] Loss: 0.181 | Acc: 93.486% (19146/20480)\n",
            "train: [199/391] Loss: 0.177 | Acc: 93.535% (23945/25600)\n",
            "train: [239/391] Loss: 0.175 | Acc: 93.662% (28773/30720)\n",
            "train: [279/391] Loss: 0.174 | Acc: 93.661% (33568/35840)\n",
            "train: [319/391] Loss: 0.173 | Acc: 93.679% (38371/40960)\n",
            "train: [359/391] Loss: 0.172 | Acc: 93.720% (43186/46080)\n",
            "val: [39/79] Loss: 0.431 | Acc: 88.008% (4506/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 104\n",
            "train: [39/391] Loss: 0.164 | Acc: 94.258% (4826/5120)\n",
            "train: [79/391] Loss: 0.158 | Acc: 94.424% (9669/10240)\n",
            "train: [119/391] Loss: 0.152 | Acc: 94.596% (14530/15360)\n",
            "train: [159/391] Loss: 0.153 | Acc: 94.585% (19371/20480)\n",
            "train: [199/391] Loss: 0.154 | Acc: 94.543% (24203/25600)\n",
            "train: [239/391] Loss: 0.152 | Acc: 94.570% (29052/30720)\n",
            "train: [279/391] Loss: 0.153 | Acc: 94.531% (33880/35840)\n",
            "train: [319/391] Loss: 0.152 | Acc: 94.563% (38733/40960)\n",
            "train: [359/391] Loss: 0.154 | Acc: 94.499% (43545/46080)\n",
            "val: [39/79] Loss: 0.408 | Acc: 88.359% (4524/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 105\n",
            "train: [39/391] Loss: 0.149 | Acc: 94.512% (4839/5120)\n",
            "train: [79/391] Loss: 0.145 | Acc: 94.609% (9688/10240)\n",
            "train: [119/391] Loss: 0.149 | Acc: 94.642% (14537/15360)\n",
            "train: [159/391] Loss: 0.152 | Acc: 94.609% (19376/20480)\n",
            "train: [199/391] Loss: 0.150 | Acc: 94.723% (24249/25600)\n",
            "train: [239/391] Loss: 0.149 | Acc: 94.707% (29094/30720)\n",
            "train: [279/391] Loss: 0.147 | Acc: 94.774% (33967/35840)\n",
            "train: [319/391] Loss: 0.147 | Acc: 94.734% (38803/40960)\n",
            "train: [359/391] Loss: 0.146 | Acc: 94.796% (43682/46080)\n",
            "val: [39/79] Loss: 0.401 | Acc: 88.848% (4549/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 106\n",
            "train: [39/391] Loss: 0.119 | Acc: 95.840% (4907/5120)\n",
            "train: [79/391] Loss: 0.114 | Acc: 96.094% (9840/10240)\n",
            "train: [119/391] Loss: 0.117 | Acc: 95.977% (14742/15360)\n",
            "train: [159/391] Loss: 0.113 | Acc: 96.064% (19674/20480)\n",
            "train: [199/391] Loss: 0.116 | Acc: 96.000% (24576/25600)\n",
            "train: [239/391] Loss: 0.115 | Acc: 96.058% (29509/30720)\n",
            "train: [279/391] Loss: 0.115 | Acc: 96.052% (34425/35840)\n",
            "train: [319/391] Loss: 0.115 | Acc: 96.047% (39341/40960)\n",
            "train: [359/391] Loss: 0.115 | Acc: 96.048% (44259/46080)\n",
            "val: [39/79] Loss: 0.394 | Acc: 89.082% (4561/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 107\n",
            "train: [39/391] Loss: 0.104 | Acc: 96.230% (4927/5120)\n",
            "train: [79/391] Loss: 0.103 | Acc: 96.338% (9865/10240)\n",
            "train: [119/391] Loss: 0.102 | Acc: 96.322% (14795/15360)\n",
            "train: [159/391] Loss: 0.102 | Acc: 96.323% (19727/20480)\n",
            "train: [199/391] Loss: 0.103 | Acc: 96.242% (24638/25600)\n",
            "train: [239/391] Loss: 0.102 | Acc: 96.318% (29589/30720)\n",
            "train: [279/391] Loss: 0.103 | Acc: 96.317% (34520/35840)\n",
            "train: [319/391] Loss: 0.102 | Acc: 96.345% (39463/40960)\n",
            "train: [359/391] Loss: 0.101 | Acc: 96.378% (44411/46080)\n",
            "val: [39/79] Loss: 0.410 | Acc: 89.355% (4575/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 108\n",
            "train: [39/391] Loss: 0.088 | Acc: 96.914% (4962/5120)\n",
            "train: [79/391] Loss: 0.088 | Acc: 96.807% (9913/10240)\n",
            "train: [119/391] Loss: 0.089 | Acc: 96.803% (14869/15360)\n",
            "train: [159/391] Loss: 0.091 | Acc: 96.738% (19812/20480)\n",
            "train: [199/391] Loss: 0.090 | Acc: 96.785% (24777/25600)\n",
            "train: [239/391] Loss: 0.089 | Acc: 96.803% (29738/30720)\n",
            "train: [279/391] Loss: 0.089 | Acc: 96.825% (34702/35840)\n",
            "train: [319/391] Loss: 0.090 | Acc: 96.821% (39658/40960)\n",
            "train: [359/391] Loss: 0.089 | Acc: 96.819% (44614/46080)\n",
            "val: [39/79] Loss: 0.408 | Acc: 89.746% (4595/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 109\n",
            "train: [39/391] Loss: 0.087 | Acc: 96.855% (4959/5120)\n",
            "train: [79/391] Loss: 0.090 | Acc: 96.875% (9920/10240)\n",
            "train: [119/391] Loss: 0.087 | Acc: 96.927% (14888/15360)\n",
            "train: [159/391] Loss: 0.089 | Acc: 96.855% (19836/20480)\n",
            "train: [199/391] Loss: 0.088 | Acc: 96.887% (24803/25600)\n",
            "train: [239/391] Loss: 0.087 | Acc: 96.937% (29779/30720)\n",
            "train: [279/391] Loss: 0.087 | Acc: 96.970% (34754/35840)\n",
            "train: [319/391] Loss: 0.086 | Acc: 97.002% (39732/40960)\n",
            "train: [359/391] Loss: 0.085 | Acc: 97.029% (44711/46080)\n",
            "val: [39/79] Loss: 0.409 | Acc: 89.844% (4600/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 110\n",
            "train: [39/391] Loss: 0.126 | Acc: 95.703% (4900/5120)\n",
            "train: [79/391] Loss: 0.160 | Acc: 94.346% (9661/10240)\n",
            "train: [119/391] Loss: 0.172 | Acc: 93.945% (14430/15360)\n",
            "train: [159/391] Loss: 0.173 | Acc: 93.857% (19222/20480)\n",
            "train: [199/391] Loss: 0.178 | Acc: 93.699% (23987/25600)\n",
            "train: [239/391] Loss: 0.180 | Acc: 93.636% (28765/30720)\n",
            "train: [279/391] Loss: 0.184 | Acc: 93.518% (33517/35840)\n",
            "train: [319/391] Loss: 0.182 | Acc: 93.545% (38316/40960)\n",
            "train: [359/391] Loss: 0.182 | Acc: 93.596% (43129/46080)\n",
            "val: [39/79] Loss: 0.433 | Acc: 88.340% (4523/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 111\n",
            "train: [39/391] Loss: 0.167 | Acc: 93.984% (4812/5120)\n",
            "train: [79/391] Loss: 0.175 | Acc: 93.760% (9601/10240)\n",
            "train: [119/391] Loss: 0.171 | Acc: 93.854% (14416/15360)\n",
            "train: [159/391] Loss: 0.173 | Acc: 93.726% (19195/20480)\n",
            "train: [199/391] Loss: 0.174 | Acc: 93.746% (23999/25600)\n",
            "train: [239/391] Loss: 0.176 | Acc: 93.590% (28751/30720)\n",
            "train: [279/391] Loss: 0.180 | Acc: 93.499% (33510/35840)\n",
            "train: [319/391] Loss: 0.181 | Acc: 93.481% (38290/40960)\n",
            "train: [359/391] Loss: 0.182 | Acc: 93.470% (43071/46080)\n",
            "val: [39/79] Loss: 0.429 | Acc: 87.637% (4487/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 112\n",
            "train: [39/391] Loss: 0.174 | Acc: 94.023% (4814/5120)\n",
            "train: [79/391] Loss: 0.168 | Acc: 93.945% (9620/10240)\n",
            "train: [119/391] Loss: 0.167 | Acc: 94.082% (14451/15360)\n",
            "train: [159/391] Loss: 0.170 | Acc: 93.950% (19241/20480)\n",
            "train: [199/391] Loss: 0.173 | Acc: 93.816% (24017/25600)\n",
            "train: [239/391] Loss: 0.171 | Acc: 93.923% (28853/30720)\n",
            "train: [279/391] Loss: 0.170 | Acc: 93.951% (33672/35840)\n",
            "train: [319/391] Loss: 0.170 | Acc: 93.994% (38500/40960)\n",
            "train: [359/391] Loss: 0.172 | Acc: 93.891% (43265/46080)\n",
            "val: [39/79] Loss: 0.418 | Acc: 88.125% (4512/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 113\n",
            "train: [39/391] Loss: 0.172 | Acc: 93.945% (4810/5120)\n",
            "train: [79/391] Loss: 0.158 | Acc: 94.521% (9679/10240)\n",
            "train: [119/391] Loss: 0.155 | Acc: 94.531% (14520/15360)\n",
            "train: [159/391] Loss: 0.154 | Acc: 94.556% (19365/20480)\n",
            "train: [199/391] Loss: 0.156 | Acc: 94.383% (24162/25600)\n",
            "train: [239/391] Loss: 0.157 | Acc: 94.365% (28989/30720)\n",
            "train: [279/391] Loss: 0.158 | Acc: 94.291% (33794/35840)\n",
            "train: [319/391] Loss: 0.158 | Acc: 94.246% (38603/40960)\n",
            "train: [359/391] Loss: 0.160 | Acc: 94.243% (43427/46080)\n",
            "val: [39/79] Loss: 0.418 | Acc: 88.359% (4524/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 114\n",
            "train: [39/391] Loss: 0.141 | Acc: 95.059% (4867/5120)\n",
            "train: [79/391] Loss: 0.140 | Acc: 95.078% (9736/10240)\n",
            "train: [119/391] Loss: 0.138 | Acc: 95.202% (14623/15360)\n",
            "train: [159/391] Loss: 0.138 | Acc: 95.156% (19488/20480)\n",
            "train: [199/391] Loss: 0.137 | Acc: 95.152% (24359/25600)\n",
            "train: [239/391] Loss: 0.136 | Acc: 95.182% (29240/30720)\n",
            "train: [279/391] Loss: 0.135 | Acc: 95.226% (34129/35840)\n",
            "train: [319/391] Loss: 0.134 | Acc: 95.212% (38999/40960)\n",
            "train: [359/391] Loss: 0.135 | Acc: 95.187% (43862/46080)\n",
            "val: [39/79] Loss: 0.415 | Acc: 88.711% (4542/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 115\n",
            "train: [39/391] Loss: 0.127 | Acc: 95.508% (4890/5120)\n",
            "train: [79/391] Loss: 0.124 | Acc: 95.586% (9788/10240)\n",
            "train: [119/391] Loss: 0.117 | Acc: 95.879% (14727/15360)\n",
            "train: [159/391] Loss: 0.117 | Acc: 95.825% (19625/20480)\n",
            "train: [199/391] Loss: 0.115 | Acc: 95.902% (24551/25600)\n",
            "train: [239/391] Loss: 0.117 | Acc: 95.889% (29457/30720)\n",
            "train: [279/391] Loss: 0.117 | Acc: 95.848% (34352/35840)\n",
            "train: [319/391] Loss: 0.117 | Acc: 95.818% (39247/40960)\n",
            "train: [359/391] Loss: 0.118 | Acc: 95.827% (44157/46080)\n",
            "val: [39/79] Loss: 0.436 | Acc: 88.359% (4524/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 116\n",
            "train: [39/391] Loss: 0.109 | Acc: 96.094% (4920/5120)\n",
            "train: [79/391] Loss: 0.106 | Acc: 96.279% (9859/10240)\n",
            "train: [119/391] Loss: 0.107 | Acc: 96.237% (14782/15360)\n",
            "train: [159/391] Loss: 0.106 | Acc: 96.265% (19715/20480)\n",
            "train: [199/391] Loss: 0.107 | Acc: 96.363% (24669/25600)\n",
            "train: [239/391] Loss: 0.105 | Acc: 96.361% (29602/30720)\n",
            "train: [279/391] Loss: 0.105 | Acc: 96.415% (34555/35840)\n",
            "train: [319/391] Loss: 0.105 | Acc: 96.411% (39490/40960)\n",
            "train: [359/391] Loss: 0.104 | Acc: 96.419% (44430/46080)\n",
            "val: [39/79] Loss: 0.416 | Acc: 89.395% (4577/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 117\n",
            "train: [39/391] Loss: 0.095 | Acc: 96.836% (4958/5120)\n",
            "train: [79/391] Loss: 0.090 | Acc: 96.914% (9924/10240)\n",
            "train: [119/391] Loss: 0.092 | Acc: 96.732% (14858/15360)\n",
            "train: [159/391] Loss: 0.094 | Acc: 96.685% (19801/20480)\n",
            "train: [199/391] Loss: 0.092 | Acc: 96.762% (24771/25600)\n",
            "train: [239/391] Loss: 0.091 | Acc: 96.807% (29739/30720)\n",
            "train: [279/391] Loss: 0.090 | Acc: 96.858% (34714/35840)\n",
            "train: [319/391] Loss: 0.091 | Acc: 96.812% (39654/40960)\n",
            "train: [359/391] Loss: 0.090 | Acc: 96.816% (44613/46080)\n",
            "val: [39/79] Loss: 0.429 | Acc: 89.375% (4576/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 118\n",
            "train: [39/391] Loss: 0.075 | Acc: 97.344% (4984/5120)\n",
            "train: [79/391] Loss: 0.081 | Acc: 97.236% (9957/10240)\n",
            "train: [119/391] Loss: 0.076 | Acc: 97.389% (14959/15360)\n",
            "train: [159/391] Loss: 0.077 | Acc: 97.305% (19928/20480)\n",
            "train: [199/391] Loss: 0.076 | Acc: 97.297% (24908/25600)\n",
            "train: [239/391] Loss: 0.076 | Acc: 97.344% (29904/30720)\n",
            "train: [279/391] Loss: 0.076 | Acc: 97.338% (34886/35840)\n",
            "train: [319/391] Loss: 0.075 | Acc: 97.397% (39894/40960)\n",
            "train: [359/391] Loss: 0.075 | Acc: 97.348% (44858/46080)\n",
            "val: [39/79] Loss: 0.428 | Acc: 89.434% (4579/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 119\n",
            "train: [39/391] Loss: 0.068 | Acc: 97.812% (5008/5120)\n",
            "train: [79/391] Loss: 0.070 | Acc: 97.676% (10002/10240)\n",
            "train: [119/391] Loss: 0.069 | Acc: 97.734% (15012/15360)\n",
            "train: [159/391] Loss: 0.071 | Acc: 97.642% (19997/20480)\n",
            "train: [199/391] Loss: 0.073 | Acc: 97.520% (24965/25600)\n",
            "train: [239/391] Loss: 0.072 | Acc: 97.588% (29979/30720)\n",
            "train: [279/391] Loss: 0.071 | Acc: 97.567% (34968/35840)\n",
            "train: [319/391] Loss: 0.072 | Acc: 97.520% (39944/40960)\n",
            "train: [359/391] Loss: 0.072 | Acc: 97.478% (44918/46080)\n",
            "val: [39/79] Loss: 0.424 | Acc: 89.512% (4583/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 120\n",
            "train: [39/391] Loss: 0.119 | Acc: 95.703% (4900/5120)\n",
            "train: [79/391] Loss: 0.140 | Acc: 95.010% (9729/10240)\n",
            "train: [119/391] Loss: 0.151 | Acc: 94.642% (14537/15360)\n",
            "train: [159/391] Loss: 0.159 | Acc: 94.419% (19337/20480)\n",
            "train: [199/391] Loss: 0.159 | Acc: 94.422% (24172/25600)\n",
            "train: [239/391] Loss: 0.161 | Acc: 94.395% (28998/30720)\n",
            "train: [279/391] Loss: 0.164 | Acc: 94.244% (33777/35840)\n",
            "train: [319/391] Loss: 0.167 | Acc: 94.104% (38545/40960)\n",
            "train: [359/391] Loss: 0.168 | Acc: 94.091% (43357/46080)\n",
            "val: [39/79] Loss: 0.443 | Acc: 87.480% (4479/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 121\n",
            "train: [39/391] Loss: 0.156 | Acc: 94.570% (4842/5120)\n",
            "train: [79/391] Loss: 0.156 | Acc: 94.551% (9682/10240)\n",
            "train: [119/391] Loss: 0.166 | Acc: 94.225% (14473/15360)\n",
            "train: [159/391] Loss: 0.168 | Acc: 94.233% (19299/20480)\n",
            "train: [199/391] Loss: 0.169 | Acc: 94.086% (24086/25600)\n",
            "train: [239/391] Loss: 0.171 | Acc: 93.994% (28875/30720)\n",
            "train: [279/391] Loss: 0.172 | Acc: 93.943% (33669/35840)\n",
            "train: [319/391] Loss: 0.172 | Acc: 93.938% (38477/40960)\n",
            "train: [359/391] Loss: 0.173 | Acc: 93.845% (43244/46080)\n",
            "val: [39/79] Loss: 0.429 | Acc: 88.203% (4516/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 122\n",
            "train: [39/391] Loss: 0.155 | Acc: 94.434% (4835/5120)\n",
            "train: [79/391] Loss: 0.150 | Acc: 94.756% (9703/10240)\n",
            "train: [119/391] Loss: 0.153 | Acc: 94.707% (14547/15360)\n",
            "train: [159/391] Loss: 0.153 | Acc: 94.619% (19378/20480)\n",
            "train: [199/391] Loss: 0.154 | Acc: 94.551% (24205/25600)\n",
            "train: [239/391] Loss: 0.158 | Acc: 94.395% (28998/30720)\n",
            "train: [279/391] Loss: 0.158 | Acc: 94.414% (33838/35840)\n",
            "train: [319/391] Loss: 0.159 | Acc: 94.412% (38671/40960)\n",
            "train: [359/391] Loss: 0.158 | Acc: 94.444% (43520/46080)\n",
            "val: [39/79] Loss: 0.424 | Acc: 88.281% (4520/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 123\n",
            "train: [39/391] Loss: 0.137 | Acc: 94.922% (4860/5120)\n",
            "train: [79/391] Loss: 0.148 | Acc: 94.551% (9682/10240)\n",
            "train: [119/391] Loss: 0.149 | Acc: 94.622% (14534/15360)\n",
            "train: [159/391] Loss: 0.147 | Acc: 94.658% (19386/20480)\n",
            "train: [199/391] Loss: 0.146 | Acc: 94.758% (24258/25600)\n",
            "train: [239/391] Loss: 0.146 | Acc: 94.753% (29108/30720)\n",
            "train: [279/391] Loss: 0.144 | Acc: 94.777% (33968/35840)\n",
            "train: [319/391] Loss: 0.145 | Acc: 94.736% (38804/40960)\n",
            "train: [359/391] Loss: 0.145 | Acc: 94.740% (43656/46080)\n",
            "val: [39/79] Loss: 0.418 | Acc: 88.086% (4510/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 124\n",
            "train: [39/391] Loss: 0.132 | Acc: 95.078% (4868/5120)\n",
            "train: [79/391] Loss: 0.129 | Acc: 95.576% (9787/10240)\n",
            "train: [119/391] Loss: 0.131 | Acc: 95.560% (14678/15360)\n",
            "train: [159/391] Loss: 0.129 | Acc: 95.508% (19560/20480)\n",
            "train: [199/391] Loss: 0.128 | Acc: 95.590% (24471/25600)\n",
            "train: [239/391] Loss: 0.127 | Acc: 95.602% (29369/30720)\n",
            "train: [279/391] Loss: 0.126 | Acc: 95.628% (34273/35840)\n",
            "train: [319/391] Loss: 0.128 | Acc: 95.527% (39128/40960)\n",
            "train: [359/391] Loss: 0.128 | Acc: 95.493% (44003/46080)\n",
            "val: [39/79] Loss: 0.420 | Acc: 88.770% (4545/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 125\n",
            "train: [39/391] Loss: 0.113 | Acc: 95.918% (4911/5120)\n",
            "train: [79/391] Loss: 0.110 | Acc: 95.986% (9829/10240)\n",
            "train: [119/391] Loss: 0.105 | Acc: 96.250% (14784/15360)\n",
            "train: [159/391] Loss: 0.105 | Acc: 96.138% (19689/20480)\n",
            "train: [199/391] Loss: 0.107 | Acc: 96.113% (24605/25600)\n",
            "train: [239/391] Loss: 0.111 | Acc: 95.921% (29467/30720)\n",
            "train: [279/391] Loss: 0.109 | Acc: 95.918% (34377/35840)\n",
            "train: [319/391] Loss: 0.109 | Acc: 95.979% (39313/40960)\n",
            "train: [359/391] Loss: 0.108 | Acc: 95.994% (44234/46080)\n",
            "val: [39/79] Loss: 0.423 | Acc: 89.375% (4576/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 126\n",
            "train: [39/391] Loss: 0.094 | Acc: 96.602% (4946/5120)\n",
            "train: [79/391] Loss: 0.096 | Acc: 96.592% (9891/10240)\n",
            "train: [119/391] Loss: 0.098 | Acc: 96.426% (14811/15360)\n",
            "train: [159/391] Loss: 0.100 | Acc: 96.392% (19741/20480)\n",
            "train: [199/391] Loss: 0.100 | Acc: 96.418% (24683/25600)\n",
            "train: [239/391] Loss: 0.097 | Acc: 96.527% (29653/30720)\n",
            "train: [279/391] Loss: 0.095 | Acc: 96.635% (34634/35840)\n",
            "train: [319/391] Loss: 0.094 | Acc: 96.707% (39611/40960)\n",
            "train: [359/391] Loss: 0.094 | Acc: 96.717% (44567/46080)\n",
            "val: [39/79] Loss: 0.428 | Acc: 89.336% (4574/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 127\n",
            "train: [39/391] Loss: 0.078 | Acc: 97.090% (4971/5120)\n",
            "train: [79/391] Loss: 0.076 | Acc: 97.197% (9953/10240)\n",
            "train: [119/391] Loss: 0.077 | Acc: 97.122% (14918/15360)\n",
            "train: [159/391] Loss: 0.078 | Acc: 97.188% (19904/20480)\n",
            "train: [199/391] Loss: 0.079 | Acc: 97.168% (24875/25600)\n",
            "train: [239/391] Loss: 0.078 | Acc: 97.210% (29863/30720)\n",
            "train: [279/391] Loss: 0.078 | Acc: 97.221% (34844/35840)\n",
            "train: [319/391] Loss: 0.078 | Acc: 97.249% (39833/40960)\n",
            "train: [359/391] Loss: 0.077 | Acc: 97.287% (44830/46080)\n",
            "val: [39/79] Loss: 0.430 | Acc: 89.453% (4580/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 128\n",
            "train: [39/391] Loss: 0.070 | Acc: 97.793% (5007/5120)\n",
            "train: [79/391] Loss: 0.071 | Acc: 97.568% (9991/10240)\n",
            "train: [119/391] Loss: 0.069 | Acc: 97.643% (14998/15360)\n",
            "train: [159/391] Loss: 0.068 | Acc: 97.661% (20001/20480)\n",
            "train: [199/391] Loss: 0.068 | Acc: 97.664% (25002/25600)\n",
            "train: [239/391] Loss: 0.068 | Acc: 97.676% (30006/30720)\n",
            "train: [279/391] Loss: 0.068 | Acc: 97.690% (35012/35840)\n",
            "train: [319/391] Loss: 0.068 | Acc: 97.654% (39999/40960)\n",
            "train: [359/391] Loss: 0.069 | Acc: 97.667% (45005/46080)\n",
            "val: [39/79] Loss: 0.420 | Acc: 89.707% (4593/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 129\n",
            "train: [39/391] Loss: 0.068 | Acc: 97.480% (4991/5120)\n",
            "train: [79/391] Loss: 0.064 | Acc: 97.725% (10007/10240)\n",
            "train: [119/391] Loss: 0.066 | Acc: 97.689% (15005/15360)\n",
            "train: [159/391] Loss: 0.065 | Acc: 97.725% (20014/20480)\n",
            "train: [199/391] Loss: 0.063 | Acc: 97.789% (25034/25600)\n",
            "train: [239/391] Loss: 0.064 | Acc: 97.731% (30023/30720)\n",
            "train: [279/391] Loss: 0.063 | Acc: 97.785% (35046/35840)\n",
            "train: [319/391] Loss: 0.064 | Acc: 97.764% (40044/40960)\n",
            "train: [359/391] Loss: 0.064 | Acc: 97.793% (45063/46080)\n",
            "val: [39/79] Loss: 0.421 | Acc: 89.902% (4603/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 130\n",
            "train: [39/391] Loss: 0.089 | Acc: 96.758% (4954/5120)\n",
            "train: [79/391] Loss: 0.118 | Acc: 95.879% (9818/10240)\n",
            "train: [119/391] Loss: 0.131 | Acc: 95.358% (14647/15360)\n",
            "train: [159/391] Loss: 0.143 | Acc: 94.893% (19434/20480)\n",
            "train: [199/391] Loss: 0.148 | Acc: 94.676% (24237/25600)\n",
            "train: [239/391] Loss: 0.152 | Acc: 94.587% (29057/30720)\n",
            "train: [279/391] Loss: 0.156 | Acc: 94.442% (33848/35840)\n",
            "train: [319/391] Loss: 0.157 | Acc: 94.377% (38657/40960)\n",
            "train: [359/391] Loss: 0.159 | Acc: 94.308% (43457/46080)\n",
            "val: [39/79] Loss: 0.429 | Acc: 88.320% (4522/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 131\n",
            "train: [39/391] Loss: 0.140 | Acc: 94.961% (4862/5120)\n",
            "train: [79/391] Loss: 0.142 | Acc: 94.961% (9724/10240)\n",
            "train: [119/391] Loss: 0.142 | Acc: 94.896% (14576/15360)\n",
            "train: [159/391] Loss: 0.146 | Acc: 94.800% (19415/20480)\n",
            "train: [199/391] Loss: 0.146 | Acc: 94.836% (24278/25600)\n",
            "train: [239/391] Loss: 0.146 | Acc: 94.785% (29118/30720)\n",
            "train: [279/391] Loss: 0.147 | Acc: 94.788% (33972/35840)\n",
            "train: [319/391] Loss: 0.150 | Acc: 94.656% (38771/40960)\n",
            "train: [359/391] Loss: 0.152 | Acc: 94.557% (43572/46080)\n",
            "val: [39/79] Loss: 0.428 | Acc: 87.695% (4490/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 132\n",
            "train: [39/391] Loss: 0.139 | Acc: 94.922% (4860/5120)\n",
            "train: [79/391] Loss: 0.139 | Acc: 95.078% (9736/10240)\n",
            "train: [119/391] Loss: 0.136 | Acc: 95.176% (14619/15360)\n",
            "train: [159/391] Loss: 0.137 | Acc: 95.073% (19471/20480)\n",
            "train: [199/391] Loss: 0.138 | Acc: 94.977% (24314/25600)\n",
            "train: [239/391] Loss: 0.139 | Acc: 94.954% (29170/30720)\n",
            "train: [279/391] Loss: 0.140 | Acc: 94.880% (34005/35840)\n",
            "train: [319/391] Loss: 0.140 | Acc: 94.858% (38854/40960)\n",
            "train: [359/391] Loss: 0.140 | Acc: 94.865% (43714/46080)\n",
            "val: [39/79] Loss: 0.407 | Acc: 88.730% (4543/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 133\n",
            "train: [39/391] Loss: 0.132 | Acc: 95.391% (4884/5120)\n",
            "train: [79/391] Loss: 0.127 | Acc: 95.635% (9793/10240)\n",
            "train: [119/391] Loss: 0.125 | Acc: 95.690% (14698/15360)\n",
            "train: [159/391] Loss: 0.126 | Acc: 95.537% (19566/20480)\n",
            "train: [199/391] Loss: 0.128 | Acc: 95.449% (24435/25600)\n",
            "train: [239/391] Loss: 0.127 | Acc: 95.482% (29332/30720)\n",
            "train: [279/391] Loss: 0.128 | Acc: 95.421% (34199/35840)\n",
            "train: [319/391] Loss: 0.129 | Acc: 95.410% (39080/40960)\n",
            "train: [359/391] Loss: 0.132 | Acc: 95.339% (43932/46080)\n",
            "val: [39/79] Loss: 0.429 | Acc: 88.574% (4535/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 134\n",
            "train: [39/391] Loss: 0.115 | Acc: 96.172% (4924/5120)\n",
            "train: [79/391] Loss: 0.112 | Acc: 96.104% (9841/10240)\n",
            "train: [119/391] Loss: 0.109 | Acc: 96.204% (14777/15360)\n",
            "train: [159/391] Loss: 0.106 | Acc: 96.221% (19706/20480)\n",
            "train: [199/391] Loss: 0.109 | Acc: 96.137% (24611/25600)\n",
            "train: [239/391] Loss: 0.111 | Acc: 96.087% (29518/30720)\n",
            "train: [279/391] Loss: 0.111 | Acc: 96.071% (34432/35840)\n",
            "train: [319/391] Loss: 0.113 | Acc: 96.001% (39322/40960)\n",
            "train: [359/391] Loss: 0.114 | Acc: 95.959% (44218/46080)\n",
            "val: [39/79] Loss: 0.448 | Acc: 88.066% (4509/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 135\n",
            "train: [39/391] Loss: 0.090 | Acc: 96.914% (4962/5120)\n",
            "train: [79/391] Loss: 0.096 | Acc: 96.611% (9893/10240)\n",
            "train: [119/391] Loss: 0.092 | Acc: 96.823% (14872/15360)\n",
            "train: [159/391] Loss: 0.095 | Acc: 96.694% (19803/20480)\n",
            "train: [199/391] Loss: 0.095 | Acc: 96.633% (24738/25600)\n",
            "train: [239/391] Loss: 0.095 | Acc: 96.628% (29684/30720)\n",
            "train: [279/391] Loss: 0.096 | Acc: 96.571% (34611/35840)\n",
            "train: [319/391] Loss: 0.096 | Acc: 96.558% (39550/40960)\n",
            "train: [359/391] Loss: 0.096 | Acc: 96.569% (44499/46080)\n",
            "val: [39/79] Loss: 0.437 | Acc: 88.867% (4550/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 136\n",
            "train: [39/391] Loss: 0.075 | Acc: 97.461% (4990/5120)\n",
            "train: [79/391] Loss: 0.076 | Acc: 97.529% (9987/10240)\n",
            "train: [119/391] Loss: 0.082 | Acc: 97.214% (14932/15360)\n",
            "train: [159/391] Loss: 0.083 | Acc: 97.163% (19899/20480)\n",
            "train: [199/391] Loss: 0.085 | Acc: 97.070% (24850/25600)\n",
            "train: [239/391] Loss: 0.084 | Acc: 97.061% (29817/30720)\n",
            "train: [279/391] Loss: 0.085 | Acc: 97.051% (34783/35840)\n",
            "train: [319/391] Loss: 0.084 | Acc: 97.065% (39758/40960)\n",
            "train: [359/391] Loss: 0.084 | Acc: 97.062% (44726/46080)\n",
            "val: [39/79] Loss: 0.429 | Acc: 89.453% (4580/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 137\n",
            "train: [39/391] Loss: 0.065 | Acc: 97.793% (5007/5120)\n",
            "train: [79/391] Loss: 0.069 | Acc: 97.715% (10006/10240)\n",
            "train: [119/391] Loss: 0.068 | Acc: 97.663% (15001/15360)\n",
            "train: [159/391] Loss: 0.070 | Acc: 97.505% (19969/20480)\n",
            "train: [199/391] Loss: 0.069 | Acc: 97.559% (24975/25600)\n",
            "train: [239/391] Loss: 0.070 | Acc: 97.536% (29963/30720)\n",
            "train: [279/391] Loss: 0.071 | Acc: 97.503% (34945/35840)\n",
            "train: [319/391] Loss: 0.072 | Acc: 97.483% (39929/40960)\n",
            "train: [359/391] Loss: 0.072 | Acc: 97.485% (44921/46080)\n",
            "val: [39/79] Loss: 0.448 | Acc: 88.906% (4552/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 138\n",
            "train: [39/391] Loss: 0.071 | Acc: 97.461% (4990/5120)\n",
            "train: [79/391] Loss: 0.064 | Acc: 97.744% (10009/10240)\n",
            "train: [119/391] Loss: 0.063 | Acc: 97.734% (15012/15360)\n",
            "train: [159/391] Loss: 0.061 | Acc: 97.861% (20042/20480)\n",
            "train: [199/391] Loss: 0.061 | Acc: 97.859% (25052/25600)\n",
            "train: [239/391] Loss: 0.061 | Acc: 97.855% (30061/30720)\n",
            "train: [279/391] Loss: 0.059 | Acc: 97.924% (35096/35840)\n",
            "train: [319/391] Loss: 0.059 | Acc: 97.949% (40120/40960)\n",
            "train: [359/391] Loss: 0.059 | Acc: 97.930% (45126/46080)\n",
            "val: [39/79] Loss: 0.442 | Acc: 89.434% (4579/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 139\n",
            "train: [39/391] Loss: 0.057 | Acc: 98.086% (5022/5120)\n",
            "train: [79/391] Loss: 0.059 | Acc: 98.047% (10040/10240)\n",
            "train: [119/391] Loss: 0.056 | Acc: 98.132% (15073/15360)\n",
            "train: [159/391] Loss: 0.057 | Acc: 98.096% (20090/20480)\n",
            "train: [199/391] Loss: 0.058 | Acc: 97.973% (25081/25600)\n",
            "train: [239/391] Loss: 0.059 | Acc: 97.910% (30078/30720)\n",
            "train: [279/391] Loss: 0.060 | Acc: 97.907% (35090/35840)\n",
            "train: [319/391] Loss: 0.060 | Acc: 97.947% (40119/40960)\n",
            "train: [359/391] Loss: 0.059 | Acc: 97.986% (45152/46080)\n",
            "val: [39/79] Loss: 0.440 | Acc: 89.453% (4580/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 140\n",
            "train: [39/391] Loss: 0.106 | Acc: 96.738% (4953/5120)\n",
            "train: [79/391] Loss: 0.121 | Acc: 95.879% (9818/10240)\n",
            "train: [119/391] Loss: 0.129 | Acc: 95.534% (14674/15360)\n",
            "train: [159/391] Loss: 0.134 | Acc: 95.288% (19515/20480)\n",
            "train: [199/391] Loss: 0.134 | Acc: 95.258% (24386/25600)\n",
            "train: [239/391] Loss: 0.137 | Acc: 95.186% (29241/30720)\n",
            "train: [279/391] Loss: 0.140 | Acc: 95.067% (34072/35840)\n",
            "train: [319/391] Loss: 0.144 | Acc: 94.893% (38868/40960)\n",
            "train: [359/391] Loss: 0.147 | Acc: 94.785% (43677/46080)\n",
            "val: [39/79] Loss: 0.434 | Acc: 87.598% (4485/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 141\n",
            "train: [39/391] Loss: 0.140 | Acc: 95.234% (4876/5120)\n",
            "train: [79/391] Loss: 0.143 | Acc: 94.893% (9717/10240)\n",
            "train: [119/391] Loss: 0.144 | Acc: 94.935% (14582/15360)\n",
            "train: [159/391] Loss: 0.143 | Acc: 94.917% (19439/20480)\n",
            "train: [199/391] Loss: 0.143 | Acc: 94.910% (24297/25600)\n",
            "train: [239/391] Loss: 0.141 | Acc: 94.899% (29153/30720)\n",
            "train: [279/391] Loss: 0.142 | Acc: 94.852% (33995/35840)\n",
            "train: [319/391] Loss: 0.142 | Acc: 94.858% (38854/40960)\n",
            "train: [359/391] Loss: 0.143 | Acc: 94.844% (43704/46080)\n",
            "val: [39/79] Loss: 0.441 | Acc: 88.066% (4509/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 142\n",
            "train: [39/391] Loss: 0.136 | Acc: 95.137% (4871/5120)\n",
            "train: [79/391] Loss: 0.139 | Acc: 95.000% (9728/10240)\n",
            "train: [119/391] Loss: 0.136 | Acc: 95.085% (14605/15360)\n",
            "train: [159/391] Loss: 0.137 | Acc: 95.078% (19472/20480)\n",
            "train: [199/391] Loss: 0.135 | Acc: 95.160% (24361/25600)\n",
            "train: [239/391] Loss: 0.136 | Acc: 95.146% (29229/30720)\n",
            "train: [279/391] Loss: 0.138 | Acc: 95.070% (34073/35840)\n",
            "train: [319/391] Loss: 0.137 | Acc: 95.125% (38963/40960)\n",
            "train: [359/391] Loss: 0.137 | Acc: 95.130% (43836/46080)\n",
            "val: [39/79] Loss: 0.406 | Acc: 88.281% (4520/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 143\n",
            "train: [39/391] Loss: 0.124 | Acc: 95.352% (4882/5120)\n",
            "train: [79/391] Loss: 0.120 | Acc: 95.674% (9797/10240)\n",
            "train: [119/391] Loss: 0.116 | Acc: 95.814% (14717/15360)\n",
            "train: [159/391] Loss: 0.117 | Acc: 95.767% (19613/20480)\n",
            "train: [199/391] Loss: 0.119 | Acc: 95.711% (24502/25600)\n",
            "train: [239/391] Loss: 0.120 | Acc: 95.628% (29377/30720)\n",
            "train: [279/391] Loss: 0.121 | Acc: 95.647% (34280/35840)\n",
            "train: [319/391] Loss: 0.121 | Acc: 95.618% (39165/40960)\n",
            "train: [359/391] Loss: 0.121 | Acc: 95.647% (44074/46080)\n",
            "val: [39/79] Loss: 0.430 | Acc: 87.891% (4500/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 144\n",
            "train: [39/391] Loss: 0.098 | Acc: 96.543% (4943/5120)\n",
            "train: [79/391] Loss: 0.103 | Acc: 96.377% (9869/10240)\n",
            "train: [119/391] Loss: 0.104 | Acc: 96.367% (14802/15360)\n",
            "train: [159/391] Loss: 0.102 | Acc: 96.445% (19752/20480)\n",
            "train: [199/391] Loss: 0.100 | Acc: 96.531% (24712/25600)\n",
            "train: [239/391] Loss: 0.103 | Acc: 96.367% (29604/30720)\n",
            "train: [279/391] Loss: 0.105 | Acc: 96.297% (34513/35840)\n",
            "train: [319/391] Loss: 0.105 | Acc: 96.267% (39431/40960)\n",
            "train: [359/391] Loss: 0.105 | Acc: 96.322% (44385/46080)\n",
            "val: [39/79] Loss: 0.438 | Acc: 88.652% (4539/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 145\n",
            "train: [39/391] Loss: 0.102 | Acc: 96.406% (4936/5120)\n",
            "train: [79/391] Loss: 0.097 | Acc: 96.699% (9902/10240)\n",
            "train: [119/391] Loss: 0.095 | Acc: 96.738% (14859/15360)\n",
            "train: [159/391] Loss: 0.093 | Acc: 96.685% (19801/20480)\n",
            "train: [199/391] Loss: 0.092 | Acc: 96.730% (24763/25600)\n",
            "train: [239/391] Loss: 0.089 | Acc: 96.882% (29762/30720)\n",
            "train: [279/391] Loss: 0.089 | Acc: 96.889% (34725/35840)\n",
            "train: [319/391] Loss: 0.088 | Acc: 96.929% (39702/40960)\n",
            "train: [359/391] Loss: 0.088 | Acc: 96.912% (44657/46080)\n",
            "val: [39/79] Loss: 0.452 | Acc: 88.750% (4544/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 146\n",
            "train: [39/391] Loss: 0.084 | Acc: 96.875% (4960/5120)\n",
            "train: [79/391] Loss: 0.079 | Acc: 97.314% (9965/10240)\n",
            "train: [119/391] Loss: 0.078 | Acc: 97.279% (14942/15360)\n",
            "train: [159/391] Loss: 0.077 | Acc: 97.241% (19915/20480)\n",
            "train: [199/391] Loss: 0.077 | Acc: 97.262% (24899/25600)\n",
            "train: [239/391] Loss: 0.077 | Acc: 97.236% (29871/30720)\n",
            "train: [279/391] Loss: 0.076 | Acc: 97.266% (34860/35840)\n",
            "train: [319/391] Loss: 0.075 | Acc: 97.288% (39849/40960)\n",
            "train: [359/391] Loss: 0.076 | Acc: 97.231% (44804/46080)\n",
            "val: [39/79] Loss: 0.422 | Acc: 89.297% (4572/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 147\n",
            "train: [39/391] Loss: 0.064 | Acc: 97.715% (5003/5120)\n",
            "train: [79/391] Loss: 0.063 | Acc: 97.861% (10021/10240)\n",
            "train: [119/391] Loss: 0.062 | Acc: 97.865% (15032/15360)\n",
            "train: [159/391] Loss: 0.061 | Acc: 97.847% (20039/20480)\n",
            "train: [199/391] Loss: 0.060 | Acc: 97.871% (25055/25600)\n",
            "train: [239/391] Loss: 0.062 | Acc: 97.839% (30056/30720)\n",
            "train: [279/391] Loss: 0.062 | Acc: 97.846% (35068/35840)\n",
            "train: [319/391] Loss: 0.063 | Acc: 97.834% (40073/40960)\n",
            "train: [359/391] Loss: 0.062 | Acc: 97.867% (45097/46080)\n",
            "val: [39/79] Loss: 0.442 | Acc: 89.629% (4589/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 148\n",
            "train: [39/391] Loss: 0.058 | Acc: 97.969% (5016/5120)\n",
            "train: [79/391] Loss: 0.060 | Acc: 97.910% (10026/10240)\n",
            "train: [119/391] Loss: 0.057 | Acc: 98.027% (15057/15360)\n",
            "train: [159/391] Loss: 0.054 | Acc: 98.101% (20091/20480)\n",
            "train: [199/391] Loss: 0.054 | Acc: 98.066% (25105/25600)\n",
            "train: [239/391] Loss: 0.054 | Acc: 98.096% (30135/30720)\n",
            "train: [279/391] Loss: 0.055 | Acc: 98.069% (35148/35840)\n",
            "train: [319/391] Loss: 0.057 | Acc: 98.015% (40147/40960)\n",
            "train: [359/391] Loss: 0.057 | Acc: 98.008% (45162/46080)\n",
            "val: [39/79] Loss: 0.440 | Acc: 89.629% (4589/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 149\n",
            "train: [39/391] Loss: 0.054 | Acc: 98.105% (5023/5120)\n",
            "train: [79/391] Loss: 0.055 | Acc: 97.969% (10032/10240)\n",
            "train: [119/391] Loss: 0.057 | Acc: 97.943% (15044/15360)\n",
            "train: [159/391] Loss: 0.055 | Acc: 98.047% (20080/20480)\n",
            "train: [199/391] Loss: 0.054 | Acc: 98.027% (25095/25600)\n",
            "train: [239/391] Loss: 0.054 | Acc: 98.053% (30122/30720)\n",
            "train: [279/391] Loss: 0.054 | Acc: 98.078% (35151/35840)\n",
            "train: [319/391] Loss: 0.054 | Acc: 98.096% (40180/40960)\n",
            "train: [359/391] Loss: 0.054 | Acc: 98.095% (45202/46080)\n",
            "val: [39/79] Loss: 0.438 | Acc: 89.531% (4584/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 150\n",
            "train: [39/391] Loss: 0.090 | Acc: 96.797% (4956/5120)\n",
            "train: [79/391] Loss: 0.103 | Acc: 96.357% (9867/10240)\n",
            "train: [119/391] Loss: 0.120 | Acc: 95.807% (14716/15360)\n",
            "train: [159/391] Loss: 0.122 | Acc: 95.688% (19597/20480)\n",
            "train: [199/391] Loss: 0.126 | Acc: 95.562% (24464/25600)\n",
            "train: [239/391] Loss: 0.128 | Acc: 95.524% (29345/30720)\n",
            "train: [279/391] Loss: 0.130 | Acc: 95.455% (34211/35840)\n",
            "train: [319/391] Loss: 0.133 | Acc: 95.303% (39036/40960)\n",
            "train: [359/391] Loss: 0.136 | Acc: 95.143% (43842/46080)\n",
            "val: [39/79] Loss: 0.466 | Acc: 87.070% (4458/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 151\n",
            "train: [39/391] Loss: 0.136 | Acc: 95.273% (4878/5120)\n",
            "train: [79/391] Loss: 0.137 | Acc: 95.186% (9747/10240)\n",
            "train: [119/391] Loss: 0.132 | Acc: 95.299% (14638/15360)\n",
            "train: [159/391] Loss: 0.137 | Acc: 95.098% (19476/20480)\n",
            "train: [199/391] Loss: 0.140 | Acc: 95.016% (24324/25600)\n",
            "train: [239/391] Loss: 0.141 | Acc: 94.958% (29171/30720)\n",
            "train: [279/391] Loss: 0.140 | Acc: 94.986% (34043/35840)\n",
            "train: [319/391] Loss: 0.139 | Acc: 94.978% (38903/40960)\n",
            "train: [359/391] Loss: 0.140 | Acc: 94.948% (43752/46080)\n",
            "val: [39/79] Loss: 0.420 | Acc: 88.262% (4519/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 152\n",
            "train: [39/391] Loss: 0.114 | Acc: 95.801% (4905/5120)\n",
            "train: [79/391] Loss: 0.120 | Acc: 95.684% (9798/10240)\n",
            "train: [119/391] Loss: 0.120 | Acc: 95.736% (14705/15360)\n",
            "train: [159/391] Loss: 0.124 | Acc: 95.532% (19565/20480)\n",
            "train: [199/391] Loss: 0.123 | Acc: 95.547% (24460/25600)\n",
            "train: [239/391] Loss: 0.124 | Acc: 95.508% (29340/30720)\n",
            "train: [279/391] Loss: 0.126 | Acc: 95.463% (34214/35840)\n",
            "train: [319/391] Loss: 0.124 | Acc: 95.518% (39124/40960)\n",
            "train: [359/391] Loss: 0.124 | Acc: 95.523% (44017/46080)\n",
            "val: [39/79] Loss: 0.434 | Acc: 88.496% (4531/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 153\n",
            "train: [39/391] Loss: 0.118 | Acc: 95.703% (4900/5120)\n",
            "train: [79/391] Loss: 0.118 | Acc: 95.762% (9806/10240)\n",
            "train: [119/391] Loss: 0.118 | Acc: 95.814% (14717/15360)\n",
            "train: [159/391] Loss: 0.116 | Acc: 95.913% (19643/20480)\n",
            "train: [199/391] Loss: 0.113 | Acc: 96.012% (24579/25600)\n",
            "train: [239/391] Loss: 0.114 | Acc: 95.970% (29482/30720)\n",
            "train: [279/391] Loss: 0.113 | Acc: 96.002% (34407/35840)\n",
            "train: [319/391] Loss: 0.113 | Acc: 95.991% (39318/40960)\n",
            "train: [359/391] Loss: 0.113 | Acc: 95.996% (44235/46080)\n",
            "val: [39/79] Loss: 0.448 | Acc: 88.730% (4543/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 154\n",
            "train: [39/391] Loss: 0.095 | Acc: 96.562% (4944/5120)\n",
            "train: [79/391] Loss: 0.098 | Acc: 96.562% (9888/10240)\n",
            "train: [119/391] Loss: 0.098 | Acc: 96.589% (14836/15360)\n",
            "train: [159/391] Loss: 0.097 | Acc: 96.597% (19783/20480)\n",
            "train: [199/391] Loss: 0.096 | Acc: 96.652% (24743/25600)\n",
            "train: [239/391] Loss: 0.095 | Acc: 96.699% (29706/30720)\n",
            "train: [279/391] Loss: 0.096 | Acc: 96.674% (34648/35840)\n",
            "train: [319/391] Loss: 0.097 | Acc: 96.633% (39581/40960)\n",
            "train: [359/391] Loss: 0.097 | Acc: 96.615% (44520/46080)\n",
            "val: [39/79] Loss: 0.438 | Acc: 89.043% (4559/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 155\n",
            "train: [39/391] Loss: 0.101 | Acc: 96.426% (4937/5120)\n",
            "train: [79/391] Loss: 0.095 | Acc: 96.562% (9888/10240)\n",
            "train: [119/391] Loss: 0.091 | Acc: 96.706% (14854/15360)\n",
            "train: [159/391] Loss: 0.088 | Acc: 96.870% (19839/20480)\n",
            "train: [199/391] Loss: 0.086 | Acc: 96.949% (24819/25600)\n",
            "train: [239/391] Loss: 0.088 | Acc: 96.885% (29763/30720)\n",
            "train: [279/391] Loss: 0.087 | Acc: 96.897% (34728/35840)\n",
            "train: [319/391] Loss: 0.086 | Acc: 96.929% (39702/40960)\n",
            "train: [359/391] Loss: 0.084 | Acc: 96.988% (44692/46080)\n",
            "val: [39/79] Loss: 0.439 | Acc: 88.965% (4555/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 156\n",
            "train: [39/391] Loss: 0.063 | Acc: 97.676% (5001/5120)\n",
            "train: [79/391] Loss: 0.068 | Acc: 97.549% (9989/10240)\n",
            "train: [119/391] Loss: 0.069 | Acc: 97.520% (14979/15360)\n",
            "train: [159/391] Loss: 0.068 | Acc: 97.563% (19981/20480)\n",
            "train: [199/391] Loss: 0.068 | Acc: 97.527% (24967/25600)\n",
            "train: [239/391] Loss: 0.069 | Acc: 97.549% (29967/30720)\n",
            "train: [279/391] Loss: 0.068 | Acc: 97.520% (34951/35840)\n",
            "train: [319/391] Loss: 0.068 | Acc: 97.542% (39953/40960)\n",
            "train: [359/391] Loss: 0.069 | Acc: 97.515% (44935/46080)\n",
            "val: [39/79] Loss: 0.435 | Acc: 89.219% (4568/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 157\n",
            "train: [39/391] Loss: 0.067 | Acc: 97.715% (5003/5120)\n",
            "train: [79/391] Loss: 0.059 | Acc: 97.988% (10034/10240)\n",
            "train: [119/391] Loss: 0.061 | Acc: 97.956% (15046/15360)\n",
            "train: [159/391] Loss: 0.060 | Acc: 97.910% (20052/20480)\n",
            "train: [199/391] Loss: 0.062 | Acc: 97.840% (25047/25600)\n",
            "train: [239/391] Loss: 0.061 | Acc: 97.861% (30063/30720)\n",
            "train: [279/391] Loss: 0.061 | Acc: 97.891% (35084/35840)\n",
            "train: [319/391] Loss: 0.060 | Acc: 97.903% (40101/40960)\n",
            "train: [359/391] Loss: 0.059 | Acc: 97.951% (45136/46080)\n",
            "val: [39/79] Loss: 0.454 | Acc: 89.590% (4587/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 158\n",
            "train: [39/391] Loss: 0.054 | Acc: 98.203% (5028/5120)\n",
            "train: [79/391] Loss: 0.052 | Acc: 98.271% (10063/10240)\n",
            "train: [119/391] Loss: 0.052 | Acc: 98.262% (15093/15360)\n",
            "train: [159/391] Loss: 0.050 | Acc: 98.291% (20130/20480)\n",
            "train: [199/391] Loss: 0.050 | Acc: 98.281% (25160/25600)\n",
            "train: [239/391] Loss: 0.050 | Acc: 98.271% (30189/30720)\n",
            "train: [279/391] Loss: 0.050 | Acc: 98.276% (35222/35840)\n",
            "train: [319/391] Loss: 0.050 | Acc: 98.267% (40250/40960)\n",
            "train: [359/391] Loss: 0.050 | Acc: 98.244% (45271/46080)\n",
            "val: [39/79] Loss: 0.457 | Acc: 89.551% (4585/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 159\n",
            "train: [39/391] Loss: 0.048 | Acc: 98.340% (5035/5120)\n",
            "train: [79/391] Loss: 0.046 | Acc: 98.389% (10075/10240)\n",
            "train: [119/391] Loss: 0.047 | Acc: 98.359% (15108/15360)\n",
            "train: [159/391] Loss: 0.051 | Acc: 98.271% (20126/20480)\n",
            "train: [199/391] Loss: 0.050 | Acc: 98.277% (25159/25600)\n",
            "train: [239/391] Loss: 0.049 | Acc: 98.314% (30202/30720)\n",
            "train: [279/391] Loss: 0.048 | Acc: 98.337% (35244/35840)\n",
            "train: [319/391] Loss: 0.048 | Acc: 98.362% (40289/40960)\n",
            "train: [359/391] Loss: 0.048 | Acc: 98.336% (45313/46080)\n",
            "val: [39/79] Loss: 0.454 | Acc: 89.492% (4582/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 160\n",
            "train: [39/391] Loss: 0.075 | Acc: 97.148% (4974/5120)\n",
            "train: [79/391] Loss: 0.098 | Acc: 96.641% (9896/10240)\n",
            "train: [119/391] Loss: 0.111 | Acc: 96.159% (14770/15360)\n",
            "train: [159/391] Loss: 0.120 | Acc: 95.835% (19627/20480)\n",
            "train: [199/391] Loss: 0.123 | Acc: 95.730% (24507/25600)\n",
            "train: [239/391] Loss: 0.127 | Acc: 95.566% (29358/30720)\n",
            "train: [279/391] Loss: 0.129 | Acc: 95.466% (34215/35840)\n",
            "train: [319/391] Loss: 0.128 | Acc: 95.488% (39112/40960)\n",
            "train: [359/391] Loss: 0.131 | Acc: 95.358% (43941/46080)\n",
            "val: [39/79] Loss: 0.436 | Acc: 87.578% (4484/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 161\n",
            "train: [39/391] Loss: 0.124 | Acc: 95.430% (4886/5120)\n",
            "train: [79/391] Loss: 0.125 | Acc: 95.400% (9769/10240)\n",
            "train: [119/391] Loss: 0.127 | Acc: 95.371% (14649/15360)\n",
            "train: [159/391] Loss: 0.126 | Acc: 95.430% (19544/20480)\n",
            "train: [199/391] Loss: 0.127 | Acc: 95.379% (24417/25600)\n",
            "train: [239/391] Loss: 0.131 | Acc: 95.260% (29264/30720)\n",
            "train: [279/391] Loss: 0.133 | Acc: 95.209% (34123/35840)\n",
            "train: [319/391] Loss: 0.132 | Acc: 95.286% (39029/40960)\n",
            "train: [359/391] Loss: 0.131 | Acc: 95.339% (43932/46080)\n",
            "val: [39/79] Loss: 0.446 | Acc: 87.949% (4503/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 162\n",
            "train: [39/391] Loss: 0.113 | Acc: 95.938% (4912/5120)\n",
            "train: [79/391] Loss: 0.107 | Acc: 96.201% (9851/10240)\n",
            "train: [119/391] Loss: 0.107 | Acc: 96.211% (14778/15360)\n",
            "train: [159/391] Loss: 0.109 | Acc: 96.089% (19679/20480)\n",
            "train: [199/391] Loss: 0.112 | Acc: 96.023% (24582/25600)\n",
            "train: [239/391] Loss: 0.114 | Acc: 95.921% (29467/30720)\n",
            "train: [279/391] Loss: 0.116 | Acc: 95.831% (34346/35840)\n",
            "train: [319/391] Loss: 0.115 | Acc: 95.845% (39258/40960)\n",
            "train: [359/391] Loss: 0.115 | Acc: 95.866% (44175/46080)\n",
            "val: [39/79] Loss: 0.429 | Acc: 88.691% (4541/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 163\n",
            "train: [39/391] Loss: 0.109 | Acc: 96.035% (4917/5120)\n",
            "train: [79/391] Loss: 0.108 | Acc: 96.299% (9861/10240)\n",
            "train: [119/391] Loss: 0.106 | Acc: 96.452% (14815/15360)\n",
            "train: [159/391] Loss: 0.105 | Acc: 96.475% (19758/20480)\n",
            "train: [199/391] Loss: 0.103 | Acc: 96.504% (24705/25600)\n",
            "train: [239/391] Loss: 0.100 | Acc: 96.595% (29674/30720)\n",
            "train: [279/391] Loss: 0.100 | Acc: 96.574% (34612/35840)\n",
            "train: [319/391] Loss: 0.102 | Acc: 96.521% (39535/40960)\n",
            "train: [359/391] Loss: 0.103 | Acc: 96.450% (44444/46080)\n",
            "val: [39/79] Loss: 0.439 | Acc: 88.672% (4540/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 164\n",
            "train: [39/391] Loss: 0.089 | Acc: 96.973% (4965/5120)\n",
            "train: [79/391] Loss: 0.089 | Acc: 96.738% (9906/10240)\n",
            "train: [119/391] Loss: 0.090 | Acc: 96.732% (14858/15360)\n",
            "train: [159/391] Loss: 0.091 | Acc: 96.738% (19812/20480)\n",
            "train: [199/391] Loss: 0.091 | Acc: 96.762% (24771/25600)\n",
            "train: [239/391] Loss: 0.091 | Acc: 96.758% (29724/30720)\n",
            "train: [279/391] Loss: 0.092 | Acc: 96.724% (34666/35840)\n",
            "train: [319/391] Loss: 0.092 | Acc: 96.729% (39620/40960)\n",
            "train: [359/391] Loss: 0.091 | Acc: 96.771% (44592/46080)\n",
            "val: [39/79] Loss: 0.430 | Acc: 88.711% (4542/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 165\n",
            "train: [39/391] Loss: 0.072 | Acc: 97.207% (4977/5120)\n",
            "train: [79/391] Loss: 0.074 | Acc: 97.168% (9950/10240)\n",
            "train: [119/391] Loss: 0.075 | Acc: 97.201% (14930/15360)\n",
            "train: [159/391] Loss: 0.076 | Acc: 97.148% (19896/20480)\n",
            "train: [199/391] Loss: 0.077 | Acc: 97.176% (24877/25600)\n",
            "train: [239/391] Loss: 0.076 | Acc: 97.240% (29872/30720)\n",
            "train: [279/391] Loss: 0.077 | Acc: 97.241% (34851/35840)\n",
            "train: [319/391] Loss: 0.077 | Acc: 97.244% (39831/40960)\n",
            "train: [359/391] Loss: 0.077 | Acc: 97.242% (44809/46080)\n",
            "val: [39/79] Loss: 0.449 | Acc: 89.199% (4567/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 166\n",
            "train: [39/391] Loss: 0.068 | Acc: 97.637% (4999/5120)\n",
            "train: [79/391] Loss: 0.063 | Acc: 97.852% (10020/10240)\n",
            "train: [119/391] Loss: 0.067 | Acc: 97.754% (15015/15360)\n",
            "train: [159/391] Loss: 0.067 | Acc: 97.715% (20012/20480)\n",
            "train: [199/391] Loss: 0.067 | Acc: 97.754% (25025/25600)\n",
            "train: [239/391] Loss: 0.065 | Acc: 97.773% (30036/30720)\n",
            "train: [279/391] Loss: 0.065 | Acc: 97.804% (35053/35840)\n",
            "train: [319/391] Loss: 0.064 | Acc: 97.830% (40071/40960)\n",
            "train: [359/391] Loss: 0.064 | Acc: 97.812% (45072/46080)\n",
            "val: [39/79] Loss: 0.454 | Acc: 89.336% (4574/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 167\n",
            "train: [39/391] Loss: 0.052 | Acc: 98.164% (5026/5120)\n",
            "train: [79/391] Loss: 0.054 | Acc: 97.939% (10029/10240)\n",
            "train: [119/391] Loss: 0.056 | Acc: 97.962% (15047/15360)\n",
            "train: [159/391] Loss: 0.055 | Acc: 98.008% (20072/20480)\n",
            "train: [199/391] Loss: 0.055 | Acc: 97.984% (25084/25600)\n",
            "train: [239/391] Loss: 0.054 | Acc: 98.018% (30111/30720)\n",
            "train: [279/391] Loss: 0.055 | Acc: 98.019% (35130/35840)\n",
            "train: [319/391] Loss: 0.055 | Acc: 98.013% (40146/40960)\n",
            "train: [359/391] Loss: 0.055 | Acc: 98.008% (45162/46080)\n",
            "val: [39/79] Loss: 0.445 | Acc: 89.688% (4592/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 168\n",
            "train: [39/391] Loss: 0.040 | Acc: 98.711% (5054/5120)\n",
            "train: [79/391] Loss: 0.045 | Acc: 98.428% (10079/10240)\n",
            "train: [119/391] Loss: 0.046 | Acc: 98.379% (15111/15360)\n",
            "train: [159/391] Loss: 0.046 | Acc: 98.403% (20153/20480)\n",
            "train: [199/391] Loss: 0.046 | Acc: 98.395% (25189/25600)\n",
            "train: [239/391] Loss: 0.046 | Acc: 98.405% (30230/30720)\n",
            "train: [279/391] Loss: 0.047 | Acc: 98.382% (35260/35840)\n",
            "train: [319/391] Loss: 0.048 | Acc: 98.359% (40288/40960)\n",
            "train: [359/391] Loss: 0.047 | Acc: 98.372% (45330/46080)\n",
            "val: [39/79] Loss: 0.448 | Acc: 89.707% (4593/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 169\n",
            "train: [39/391] Loss: 0.038 | Acc: 98.809% (5059/5120)\n",
            "train: [79/391] Loss: 0.041 | Acc: 98.564% (10093/10240)\n",
            "train: [119/391] Loss: 0.043 | Acc: 98.464% (15124/15360)\n",
            "train: [159/391] Loss: 0.044 | Acc: 98.433% (20159/20480)\n",
            "train: [199/391] Loss: 0.044 | Acc: 98.449% (25203/25600)\n",
            "train: [239/391] Loss: 0.044 | Acc: 98.454% (30245/30720)\n",
            "train: [279/391] Loss: 0.043 | Acc: 98.510% (35306/35840)\n",
            "train: [319/391] Loss: 0.043 | Acc: 98.506% (40348/40960)\n",
            "train: [359/391] Loss: 0.043 | Acc: 98.511% (45394/46080)\n",
            "val: [39/79] Loss: 0.439 | Acc: 89.863% (4601/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 170\n",
            "train: [39/391] Loss: 0.070 | Acc: 97.637% (4999/5120)\n",
            "train: [79/391] Loss: 0.088 | Acc: 96.924% (9925/10240)\n",
            "train: [119/391] Loss: 0.104 | Acc: 96.367% (14802/15360)\n",
            "train: [159/391] Loss: 0.114 | Acc: 96.060% (19673/20480)\n",
            "train: [199/391] Loss: 0.116 | Acc: 96.027% (24583/25600)\n",
            "train: [239/391] Loss: 0.117 | Acc: 95.983% (29486/30720)\n",
            "train: [279/391] Loss: 0.120 | Acc: 95.801% (34335/35840)\n",
            "train: [319/391] Loss: 0.121 | Acc: 95.769% (39227/40960)\n",
            "train: [359/391] Loss: 0.122 | Acc: 95.781% (44136/46080)\n",
            "val: [39/79] Loss: 0.444 | Acc: 88.418% (4527/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 171\n",
            "train: [39/391] Loss: 0.129 | Acc: 95.293% (4879/5120)\n",
            "train: [79/391] Loss: 0.128 | Acc: 95.283% (9757/10240)\n",
            "train: [119/391] Loss: 0.124 | Acc: 95.449% (14661/15360)\n",
            "train: [159/391] Loss: 0.126 | Acc: 95.366% (19531/20480)\n",
            "train: [199/391] Loss: 0.126 | Acc: 95.387% (24419/25600)\n",
            "train: [239/391] Loss: 0.125 | Acc: 95.472% (29329/30720)\n",
            "train: [279/391] Loss: 0.124 | Acc: 95.533% (34239/35840)\n",
            "train: [319/391] Loss: 0.123 | Acc: 95.586% (39152/40960)\n",
            "train: [359/391] Loss: 0.124 | Acc: 95.569% (44038/46080)\n",
            "val: [39/79] Loss: 0.433 | Acc: 88.223% (4517/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 172\n",
            "train: [39/391] Loss: 0.097 | Acc: 96.562% (4944/5120)\n",
            "train: [79/391] Loss: 0.102 | Acc: 96.475% (9879/10240)\n",
            "train: [119/391] Loss: 0.103 | Acc: 96.484% (14820/15360)\n",
            "train: [159/391] Loss: 0.108 | Acc: 96.328% (19728/20480)\n",
            "train: [199/391] Loss: 0.108 | Acc: 96.273% (24646/25600)\n",
            "train: [239/391] Loss: 0.107 | Acc: 96.270% (29574/30720)\n",
            "train: [279/391] Loss: 0.109 | Acc: 96.222% (34486/35840)\n",
            "train: [319/391] Loss: 0.109 | Acc: 96.162% (39388/40960)\n",
            "train: [359/391] Loss: 0.110 | Acc: 96.107% (44286/46080)\n",
            "val: [39/79] Loss: 0.442 | Acc: 88.438% (4528/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 173\n",
            "train: [39/391] Loss: 0.095 | Acc: 96.621% (4947/5120)\n",
            "train: [79/391] Loss: 0.094 | Acc: 96.689% (9901/10240)\n",
            "train: [119/391] Loss: 0.101 | Acc: 96.426% (14811/15360)\n",
            "train: [159/391] Loss: 0.101 | Acc: 96.396% (19742/20480)\n",
            "train: [199/391] Loss: 0.101 | Acc: 96.406% (24680/25600)\n",
            "train: [239/391] Loss: 0.099 | Acc: 96.494% (29643/30720)\n",
            "train: [279/391] Loss: 0.098 | Acc: 96.540% (34600/35840)\n",
            "train: [319/391] Loss: 0.099 | Acc: 96.506% (39529/40960)\n",
            "train: [359/391] Loss: 0.098 | Acc: 96.497% (44466/46080)\n",
            "val: [39/79] Loss: 0.441 | Acc: 88.984% (4556/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 174\n",
            "train: [39/391] Loss: 0.091 | Acc: 96.992% (4966/5120)\n",
            "train: [79/391] Loss: 0.090 | Acc: 97.021% (9935/10240)\n",
            "train: [119/391] Loss: 0.089 | Acc: 96.960% (14893/15360)\n",
            "train: [159/391] Loss: 0.090 | Acc: 96.865% (19838/20480)\n",
            "train: [199/391] Loss: 0.087 | Acc: 96.945% (24818/25600)\n",
            "train: [239/391] Loss: 0.086 | Acc: 96.963% (29787/30720)\n",
            "train: [279/391] Loss: 0.085 | Acc: 97.012% (34769/35840)\n",
            "train: [319/391] Loss: 0.086 | Acc: 96.943% (39708/40960)\n",
            "train: [359/391] Loss: 0.085 | Acc: 96.992% (44694/46080)\n",
            "val: [39/79] Loss: 0.460 | Acc: 88.965% (4555/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 175\n",
            "train: [39/391] Loss: 0.070 | Acc: 97.324% (4983/5120)\n",
            "train: [79/391] Loss: 0.072 | Acc: 97.383% (9972/10240)\n",
            "train: [119/391] Loss: 0.069 | Acc: 97.572% (14987/15360)\n",
            "train: [159/391] Loss: 0.067 | Acc: 97.622% (19993/20480)\n",
            "train: [199/391] Loss: 0.069 | Acc: 97.586% (24982/25600)\n",
            "train: [239/391] Loss: 0.069 | Acc: 97.552% (29968/30720)\n",
            "train: [279/391] Loss: 0.070 | Acc: 97.489% (34940/35840)\n",
            "train: [319/391] Loss: 0.071 | Acc: 97.461% (39920/40960)\n",
            "train: [359/391] Loss: 0.073 | Acc: 97.405% (44884/46080)\n",
            "val: [39/79] Loss: 0.444 | Acc: 89.043% (4559/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 176\n",
            "train: [39/391] Loss: 0.065 | Acc: 97.773% (5006/5120)\n",
            "train: [79/391] Loss: 0.061 | Acc: 97.861% (10021/10240)\n",
            "train: [119/391] Loss: 0.059 | Acc: 97.936% (15043/15360)\n",
            "train: [159/391] Loss: 0.059 | Acc: 97.930% (20056/20480)\n",
            "train: [199/391] Loss: 0.059 | Acc: 97.930% (25070/25600)\n",
            "train: [239/391] Loss: 0.058 | Acc: 97.975% (30098/30720)\n",
            "train: [279/391] Loss: 0.059 | Acc: 97.930% (35098/35840)\n",
            "train: [319/391] Loss: 0.059 | Acc: 97.915% (40106/40960)\n",
            "train: [359/391] Loss: 0.059 | Acc: 97.930% (45126/46080)\n",
            "val: [39/79] Loss: 0.458 | Acc: 89.648% (4590/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 177\n",
            "train: [39/391] Loss: 0.051 | Acc: 98.262% (5031/5120)\n",
            "train: [79/391] Loss: 0.051 | Acc: 98.301% (10066/10240)\n",
            "train: [119/391] Loss: 0.052 | Acc: 98.210% (15085/15360)\n",
            "train: [159/391] Loss: 0.054 | Acc: 98.159% (20103/20480)\n",
            "train: [199/391] Loss: 0.053 | Acc: 98.203% (25140/25600)\n",
            "train: [239/391] Loss: 0.052 | Acc: 98.229% (30176/30720)\n",
            "train: [279/391] Loss: 0.051 | Acc: 98.220% (35202/35840)\n",
            "train: [319/391] Loss: 0.051 | Acc: 98.220% (40231/40960)\n",
            "train: [359/391] Loss: 0.051 | Acc: 98.214% (45257/46080)\n",
            "val: [39/79] Loss: 0.454 | Acc: 89.688% (4592/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 178\n",
            "train: [39/391] Loss: 0.046 | Acc: 98.418% (5039/5120)\n",
            "train: [79/391] Loss: 0.045 | Acc: 98.477% (10084/10240)\n",
            "train: [119/391] Loss: 0.046 | Acc: 98.509% (15131/15360)\n",
            "train: [159/391] Loss: 0.045 | Acc: 98.525% (20178/20480)\n",
            "train: [199/391] Loss: 0.045 | Acc: 98.480% (25211/25600)\n",
            "train: [239/391] Loss: 0.044 | Acc: 98.467% (30249/30720)\n",
            "train: [279/391] Loss: 0.045 | Acc: 98.474% (35293/35840)\n",
            "train: [319/391] Loss: 0.045 | Acc: 98.469% (40333/40960)\n",
            "train: [359/391] Loss: 0.045 | Acc: 98.474% (45377/46080)\n",
            "val: [39/79] Loss: 0.469 | Acc: 89.785% (4597/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 179\n",
            "train: [39/391] Loss: 0.038 | Acc: 98.750% (5056/5120)\n",
            "train: [79/391] Loss: 0.040 | Acc: 98.594% (10096/10240)\n",
            "train: [119/391] Loss: 0.040 | Acc: 98.600% (15145/15360)\n",
            "train: [159/391] Loss: 0.039 | Acc: 98.647% (20203/20480)\n",
            "train: [199/391] Loss: 0.039 | Acc: 98.695% (25266/25600)\n",
            "train: [239/391] Loss: 0.040 | Acc: 98.675% (30313/30720)\n",
            "train: [279/391] Loss: 0.039 | Acc: 98.669% (35363/35840)\n",
            "train: [319/391] Loss: 0.039 | Acc: 98.662% (40412/40960)\n",
            "train: [359/391] Loss: 0.039 | Acc: 98.668% (45466/46080)\n",
            "val: [39/79] Loss: 0.458 | Acc: 89.688% (4592/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 180\n",
            "train: [39/391] Loss: 0.065 | Acc: 97.656% (5000/5120)\n",
            "train: [79/391] Loss: 0.083 | Acc: 97.021% (9935/10240)\n",
            "train: [119/391] Loss: 0.091 | Acc: 96.719% (14856/15360)\n",
            "train: [159/391] Loss: 0.097 | Acc: 96.577% (19779/20480)\n",
            "train: [199/391] Loss: 0.103 | Acc: 96.402% (24679/25600)\n",
            "train: [239/391] Loss: 0.111 | Acc: 96.100% (29522/30720)\n",
            "train: [279/391] Loss: 0.114 | Acc: 96.018% (34413/35840)\n",
            "train: [319/391] Loss: 0.115 | Acc: 96.008% (39325/40960)\n",
            "train: [359/391] Loss: 0.115 | Acc: 96.007% (44240/46080)\n",
            "val: [39/79] Loss: 0.469 | Acc: 87.754% (4493/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 181\n",
            "train: [39/391] Loss: 0.106 | Acc: 96.172% (4924/5120)\n",
            "train: [79/391] Loss: 0.112 | Acc: 96.162% (9847/10240)\n",
            "train: [119/391] Loss: 0.112 | Acc: 96.094% (14760/15360)\n",
            "train: [159/391] Loss: 0.112 | Acc: 96.084% (19678/20480)\n",
            "train: [199/391] Loss: 0.113 | Acc: 96.012% (24579/25600)\n",
            "train: [239/391] Loss: 0.114 | Acc: 96.009% (29494/30720)\n",
            "train: [279/391] Loss: 0.113 | Acc: 96.016% (34412/35840)\n",
            "train: [319/391] Loss: 0.115 | Acc: 96.001% (39322/40960)\n",
            "train: [359/391] Loss: 0.115 | Acc: 96.003% (44238/46080)\n",
            "val: [39/79] Loss: 0.440 | Acc: 88.438% (4528/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 182\n",
            "train: [39/391] Loss: 0.106 | Acc: 96.309% (4931/5120)\n",
            "train: [79/391] Loss: 0.105 | Acc: 96.260% (9857/10240)\n",
            "train: [119/391] Loss: 0.105 | Acc: 96.315% (14794/15360)\n",
            "train: [159/391] Loss: 0.104 | Acc: 96.333% (19729/20480)\n",
            "train: [199/391] Loss: 0.106 | Acc: 96.246% (24639/25600)\n",
            "train: [239/391] Loss: 0.108 | Acc: 96.159% (29540/30720)\n",
            "train: [279/391] Loss: 0.108 | Acc: 96.091% (34439/35840)\n",
            "train: [319/391] Loss: 0.106 | Acc: 96.208% (39407/40960)\n",
            "train: [359/391] Loss: 0.105 | Acc: 96.289% (44370/46080)\n",
            "val: [39/79] Loss: 0.451 | Acc: 88.418% (4527/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 183\n",
            "train: [39/391] Loss: 0.095 | Acc: 96.953% (4964/5120)\n",
            "train: [79/391] Loss: 0.092 | Acc: 96.855% (9918/10240)\n",
            "train: [119/391] Loss: 0.097 | Acc: 96.667% (14848/15360)\n",
            "train: [159/391] Loss: 0.098 | Acc: 96.636% (19791/20480)\n",
            "train: [199/391] Loss: 0.096 | Acc: 96.695% (24754/25600)\n",
            "train: [239/391] Loss: 0.095 | Acc: 96.755% (29723/30720)\n",
            "train: [279/391] Loss: 0.094 | Acc: 96.738% (34671/35840)\n",
            "train: [319/391] Loss: 0.094 | Acc: 96.758% (39632/40960)\n",
            "train: [359/391] Loss: 0.094 | Acc: 96.732% (44574/46080)\n",
            "val: [39/79] Loss: 0.458 | Acc: 88.262% (4519/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 184\n",
            "train: [39/391] Loss: 0.074 | Acc: 97.422% (4988/5120)\n",
            "train: [79/391] Loss: 0.076 | Acc: 97.285% (9962/10240)\n",
            "train: [119/391] Loss: 0.077 | Acc: 97.240% (14936/15360)\n",
            "train: [159/391] Loss: 0.077 | Acc: 97.217% (19910/20480)\n",
            "train: [199/391] Loss: 0.082 | Acc: 97.059% (24847/25600)\n",
            "train: [239/391] Loss: 0.082 | Acc: 97.051% (29814/30720)\n",
            "train: [279/391] Loss: 0.081 | Acc: 97.107% (34803/35840)\n",
            "train: [319/391] Loss: 0.080 | Acc: 97.173% (39802/40960)\n",
            "train: [359/391] Loss: 0.080 | Acc: 97.153% (44768/46080)\n",
            "val: [39/79] Loss: 0.473 | Acc: 88.652% (4539/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 185\n",
            "train: [39/391] Loss: 0.065 | Acc: 97.637% (4999/5120)\n",
            "train: [79/391] Loss: 0.064 | Acc: 97.734% (10008/10240)\n",
            "train: [119/391] Loss: 0.065 | Acc: 97.695% (15006/15360)\n",
            "train: [159/391] Loss: 0.065 | Acc: 97.690% (20007/20480)\n",
            "train: [199/391] Loss: 0.065 | Acc: 97.684% (25007/25600)\n",
            "train: [239/391] Loss: 0.063 | Acc: 97.760% (30032/30720)\n",
            "train: [279/391] Loss: 0.065 | Acc: 97.720% (35023/35840)\n",
            "train: [319/391] Loss: 0.065 | Acc: 97.705% (40020/40960)\n",
            "train: [359/391] Loss: 0.065 | Acc: 97.711% (45025/46080)\n",
            "val: [39/79] Loss: 0.483 | Acc: 88.828% (4548/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 186\n",
            "train: [39/391] Loss: 0.057 | Acc: 98.145% (5025/5120)\n",
            "train: [79/391] Loss: 0.058 | Acc: 98.096% (10045/10240)\n",
            "train: [119/391] Loss: 0.057 | Acc: 98.047% (15060/15360)\n",
            "train: [159/391] Loss: 0.057 | Acc: 98.066% (20084/20480)\n",
            "train: [199/391] Loss: 0.057 | Acc: 98.062% (25104/25600)\n",
            "train: [239/391] Loss: 0.057 | Acc: 98.066% (30126/30720)\n",
            "train: [279/391] Loss: 0.057 | Acc: 98.072% (35149/35840)\n",
            "train: [319/391] Loss: 0.059 | Acc: 98.047% (40160/40960)\n",
            "train: [359/391] Loss: 0.058 | Acc: 98.073% (45192/46080)\n",
            "val: [39/79] Loss: 0.472 | Acc: 89.238% (4569/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 187\n",
            "train: [39/391] Loss: 0.052 | Acc: 98.223% (5029/5120)\n",
            "train: [79/391] Loss: 0.049 | Acc: 98.301% (10066/10240)\n",
            "train: [119/391] Loss: 0.047 | Acc: 98.379% (15111/15360)\n",
            "train: [159/391] Loss: 0.047 | Acc: 98.369% (20146/20480)\n",
            "train: [199/391] Loss: 0.048 | Acc: 98.348% (25177/25600)\n",
            "train: [239/391] Loss: 0.048 | Acc: 98.369% (30219/30720)\n",
            "train: [279/391] Loss: 0.048 | Acc: 98.326% (35240/35840)\n",
            "train: [319/391] Loss: 0.048 | Acc: 98.354% (40286/40960)\n",
            "train: [359/391] Loss: 0.048 | Acc: 98.368% (45328/46080)\n",
            "val: [39/79] Loss: 0.490 | Acc: 88.711% (4542/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 188\n",
            "train: [39/391] Loss: 0.047 | Acc: 98.281% (5032/5120)\n",
            "train: [79/391] Loss: 0.041 | Acc: 98.496% (10086/10240)\n",
            "train: [119/391] Loss: 0.042 | Acc: 98.542% (15136/15360)\n",
            "train: [159/391] Loss: 0.041 | Acc: 98.579% (20189/20480)\n",
            "train: [199/391] Loss: 0.040 | Acc: 98.594% (25240/25600)\n",
            "train: [239/391] Loss: 0.040 | Acc: 98.623% (30297/30720)\n",
            "train: [279/391] Loss: 0.040 | Acc: 98.636% (35351/35840)\n",
            "train: [319/391] Loss: 0.039 | Acc: 98.638% (40402/40960)\n",
            "train: [359/391] Loss: 0.039 | Acc: 98.657% (45461/46080)\n",
            "val: [39/79] Loss: 0.481 | Acc: 89.043% (4559/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 189\n",
            "train: [39/391] Loss: 0.032 | Acc: 99.023% (5070/5120)\n",
            "train: [79/391] Loss: 0.036 | Acc: 98.828% (10120/10240)\n",
            "train: [119/391] Loss: 0.036 | Acc: 98.789% (15174/15360)\n",
            "train: [159/391] Loss: 0.037 | Acc: 98.711% (20216/20480)\n",
            "train: [199/391] Loss: 0.037 | Acc: 98.699% (25267/25600)\n",
            "train: [239/391] Loss: 0.037 | Acc: 98.743% (30334/30720)\n",
            "train: [279/391] Loss: 0.036 | Acc: 98.750% (35392/35840)\n",
            "train: [319/391] Loss: 0.036 | Acc: 98.738% (40443/40960)\n",
            "train: [359/391] Loss: 0.036 | Acc: 98.754% (45506/46080)\n",
            "val: [39/79] Loss: 0.481 | Acc: 89.141% (4564/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 190\n",
            "train: [39/391] Loss: 0.059 | Acc: 98.008% (5018/5120)\n",
            "train: [79/391] Loss: 0.081 | Acc: 97.207% (9954/10240)\n",
            "train: [119/391] Loss: 0.090 | Acc: 96.816% (14871/15360)\n",
            "train: [159/391] Loss: 0.098 | Acc: 96.572% (19778/20480)\n",
            "train: [199/391] Loss: 0.100 | Acc: 96.496% (24703/25600)\n",
            "train: [239/391] Loss: 0.101 | Acc: 96.416% (29619/30720)\n",
            "train: [279/391] Loss: 0.103 | Acc: 96.350% (34532/35840)\n",
            "train: [319/391] Loss: 0.105 | Acc: 96.250% (39424/40960)\n",
            "train: [359/391] Loss: 0.106 | Acc: 96.198% (44328/46080)\n",
            "val: [39/79] Loss: 0.489 | Acc: 87.344% (4472/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 191\n",
            "train: [39/391] Loss: 0.115 | Acc: 96.055% (4918/5120)\n",
            "train: [79/391] Loss: 0.109 | Acc: 96.123% (9843/10240)\n",
            "train: [119/391] Loss: 0.106 | Acc: 96.211% (14778/15360)\n",
            "train: [159/391] Loss: 0.108 | Acc: 96.147% (19691/20480)\n",
            "train: [199/391] Loss: 0.107 | Acc: 96.172% (24620/25600)\n",
            "train: [239/391] Loss: 0.107 | Acc: 96.162% (29541/30720)\n",
            "train: [279/391] Loss: 0.108 | Acc: 96.138% (34456/35840)\n",
            "train: [319/391] Loss: 0.108 | Acc: 96.157% (39386/40960)\n",
            "train: [359/391] Loss: 0.107 | Acc: 96.181% (44320/46080)\n",
            "val: [39/79] Loss: 0.455 | Acc: 88.047% (4508/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 192\n",
            "train: [39/391] Loss: 0.097 | Acc: 96.680% (4950/5120)\n",
            "train: [79/391] Loss: 0.100 | Acc: 96.445% (9876/10240)\n",
            "train: [119/391] Loss: 0.099 | Acc: 96.452% (14815/15360)\n",
            "train: [159/391] Loss: 0.099 | Acc: 96.479% (19759/20480)\n",
            "train: [199/391] Loss: 0.099 | Acc: 96.512% (24707/25600)\n",
            "train: [239/391] Loss: 0.100 | Acc: 96.481% (29639/30720)\n",
            "train: [279/391] Loss: 0.100 | Acc: 96.465% (34573/35840)\n",
            "train: [319/391] Loss: 0.099 | Acc: 96.453% (39507/40960)\n",
            "train: [359/391] Loss: 0.100 | Acc: 96.432% (44436/46080)\n",
            "val: [39/79] Loss: 0.450 | Acc: 87.910% (4501/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 193\n",
            "train: [39/391] Loss: 0.088 | Acc: 96.895% (4961/5120)\n",
            "train: [79/391] Loss: 0.081 | Acc: 97.119% (9945/10240)\n",
            "train: [119/391] Loss: 0.083 | Acc: 97.148% (14922/15360)\n",
            "train: [159/391] Loss: 0.083 | Acc: 97.148% (19896/20480)\n",
            "train: [199/391] Loss: 0.083 | Acc: 97.117% (24862/25600)\n",
            "train: [239/391] Loss: 0.085 | Acc: 97.080% (29823/30720)\n",
            "train: [279/391] Loss: 0.085 | Acc: 97.023% (34773/35840)\n",
            "train: [319/391] Loss: 0.085 | Acc: 97.029% (39743/40960)\n",
            "train: [359/391] Loss: 0.086 | Acc: 96.964% (44681/46080)\n",
            "val: [39/79] Loss: 0.430 | Acc: 89.102% (4562/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 194\n",
            "train: [39/391] Loss: 0.086 | Acc: 97.031% (4968/5120)\n",
            "train: [79/391] Loss: 0.079 | Acc: 97.305% (9964/10240)\n",
            "train: [119/391] Loss: 0.076 | Acc: 97.344% (14952/15360)\n",
            "train: [159/391] Loss: 0.077 | Acc: 97.285% (19924/20480)\n",
            "train: [199/391] Loss: 0.075 | Acc: 97.398% (24934/25600)\n",
            "train: [239/391] Loss: 0.073 | Acc: 97.480% (29946/30720)\n",
            "train: [279/391] Loss: 0.072 | Acc: 97.511% (34948/35840)\n",
            "train: [319/391] Loss: 0.073 | Acc: 97.498% (39935/40960)\n",
            "train: [359/391] Loss: 0.072 | Acc: 97.487% (44922/46080)\n",
            "val: [39/79] Loss: 0.459 | Acc: 88.770% (4545/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 195\n",
            "train: [39/391] Loss: 0.065 | Acc: 97.363% (4985/5120)\n",
            "train: [79/391] Loss: 0.066 | Acc: 97.383% (9972/10240)\n",
            "train: [119/391] Loss: 0.063 | Acc: 97.617% (14994/15360)\n",
            "train: [159/391] Loss: 0.063 | Acc: 97.632% (19995/20480)\n",
            "train: [199/391] Loss: 0.061 | Acc: 97.754% (25025/25600)\n",
            "train: [239/391] Loss: 0.062 | Acc: 97.760% (30032/30720)\n",
            "train: [279/391] Loss: 0.062 | Acc: 97.743% (35031/35840)\n",
            "train: [319/391] Loss: 0.061 | Acc: 97.766% (40045/40960)\n",
            "train: [359/391] Loss: 0.062 | Acc: 97.737% (45037/46080)\n",
            "val: [39/79] Loss: 0.449 | Acc: 89.414% (4578/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 196\n",
            "train: [39/391] Loss: 0.055 | Acc: 97.949% (5015/5120)\n",
            "train: [79/391] Loss: 0.050 | Acc: 98.096% (10045/10240)\n",
            "train: [119/391] Loss: 0.052 | Acc: 98.112% (15070/15360)\n",
            "train: [159/391] Loss: 0.051 | Acc: 98.232% (20118/20480)\n",
            "train: [199/391] Loss: 0.051 | Acc: 98.223% (25145/25600)\n",
            "train: [239/391] Loss: 0.051 | Acc: 98.242% (30180/30720)\n",
            "train: [279/391] Loss: 0.051 | Acc: 98.251% (35213/35840)\n",
            "train: [319/391] Loss: 0.051 | Acc: 98.240% (40239/40960)\n",
            "train: [359/391] Loss: 0.050 | Acc: 98.260% (45278/46080)\n",
            "val: [39/79] Loss: 0.472 | Acc: 89.258% (4570/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 197\n",
            "train: [39/391] Loss: 0.045 | Acc: 98.496% (5043/5120)\n",
            "train: [79/391] Loss: 0.044 | Acc: 98.564% (10093/10240)\n",
            "train: [119/391] Loss: 0.043 | Acc: 98.548% (15137/15360)\n",
            "train: [159/391] Loss: 0.043 | Acc: 98.569% (20187/20480)\n",
            "train: [199/391] Loss: 0.042 | Acc: 98.586% (25238/25600)\n",
            "train: [239/391] Loss: 0.043 | Acc: 98.574% (30282/30720)\n",
            "train: [279/391] Loss: 0.044 | Acc: 98.521% (35310/35840)\n",
            "train: [319/391] Loss: 0.044 | Acc: 98.472% (40334/40960)\n",
            "train: [359/391] Loss: 0.044 | Acc: 98.505% (45391/46080)\n",
            "val: [39/79] Loss: 0.454 | Acc: 89.238% (4569/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 198\n",
            "train: [39/391] Loss: 0.037 | Acc: 98.867% (5062/5120)\n",
            "train: [79/391] Loss: 0.038 | Acc: 98.750% (10112/10240)\n",
            "train: [119/391] Loss: 0.039 | Acc: 98.724% (15164/15360)\n",
            "train: [159/391] Loss: 0.041 | Acc: 98.652% (20204/20480)\n",
            "train: [199/391] Loss: 0.039 | Acc: 98.676% (25261/25600)\n",
            "train: [239/391] Loss: 0.040 | Acc: 98.665% (30310/30720)\n",
            "train: [279/391] Loss: 0.040 | Acc: 98.677% (35366/35840)\n",
            "train: [319/391] Loss: 0.039 | Acc: 98.682% (40420/40960)\n",
            "train: [359/391] Loss: 0.038 | Acc: 98.676% (45470/46080)\n",
            "val: [39/79] Loss: 0.455 | Acc: 89.766% (4596/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 199\n",
            "train: [39/391] Loss: 0.033 | Acc: 98.867% (5062/5120)\n",
            "train: [79/391] Loss: 0.035 | Acc: 98.779% (10115/10240)\n",
            "train: [119/391] Loss: 0.035 | Acc: 98.757% (15169/15360)\n",
            "train: [159/391] Loss: 0.035 | Acc: 98.809% (20236/20480)\n",
            "train: [199/391] Loss: 0.035 | Acc: 98.820% (25298/25600)\n",
            "train: [239/391] Loss: 0.035 | Acc: 98.828% (30360/30720)\n",
            "train: [279/391] Loss: 0.035 | Acc: 98.797% (35409/35840)\n",
            "train: [319/391] Loss: 0.036 | Acc: 98.799% (40468/40960)\n",
            "train: [359/391] Loss: 0.035 | Acc: 98.789% (45522/46080)\n",
            "val: [39/79] Loss: 0.454 | Acc: 89.473% (4581/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 200\n",
            "train: [39/391] Loss: 0.062 | Acc: 97.891% (5012/5120)\n",
            "train: [79/391] Loss: 0.077 | Acc: 97.324% (9966/10240)\n",
            "train: [119/391] Loss: 0.086 | Acc: 96.999% (14899/15360)\n",
            "train: [159/391] Loss: 0.090 | Acc: 96.816% (19828/20480)\n",
            "train: [199/391] Loss: 0.094 | Acc: 96.723% (24761/25600)\n",
            "train: [239/391] Loss: 0.098 | Acc: 96.579% (29669/30720)\n",
            "train: [279/391] Loss: 0.101 | Acc: 96.417% (34556/35840)\n",
            "train: [319/391] Loss: 0.102 | Acc: 96.418% (39493/40960)\n",
            "train: [359/391] Loss: 0.105 | Acc: 96.332% (44390/46080)\n",
            "val: [39/79] Loss: 0.445 | Acc: 87.832% (4497/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 201\n",
            "train: [39/391] Loss: 0.095 | Acc: 96.719% (4952/5120)\n",
            "train: [79/391] Loss: 0.094 | Acc: 96.553% (9887/10240)\n",
            "train: [119/391] Loss: 0.105 | Acc: 96.348% (14799/15360)\n",
            "train: [159/391] Loss: 0.107 | Acc: 96.270% (19716/20480)\n",
            "train: [199/391] Loss: 0.107 | Acc: 96.305% (24654/25600)\n",
            "train: [239/391] Loss: 0.107 | Acc: 96.276% (29576/30720)\n",
            "train: [279/391] Loss: 0.109 | Acc: 96.172% (34468/35840)\n",
            "train: [319/391] Loss: 0.108 | Acc: 96.155% (39385/40960)\n",
            "train: [359/391] Loss: 0.112 | Acc: 96.063% (44266/46080)\n",
            "val: [39/79] Loss: 0.453 | Acc: 87.676% (4489/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 202\n",
            "train: [39/391] Loss: 0.117 | Acc: 95.820% (4906/5120)\n",
            "train: [79/391] Loss: 0.109 | Acc: 96.250% (9856/10240)\n",
            "train: [119/391] Loss: 0.102 | Acc: 96.406% (14808/15360)\n",
            "train: [159/391] Loss: 0.099 | Acc: 96.509% (19765/20480)\n",
            "train: [199/391] Loss: 0.097 | Acc: 96.535% (24713/25600)\n",
            "train: [239/391] Loss: 0.097 | Acc: 96.520% (29651/30720)\n",
            "train: [279/391] Loss: 0.096 | Acc: 96.546% (34602/35840)\n",
            "train: [319/391] Loss: 0.097 | Acc: 96.504% (39528/40960)\n",
            "train: [359/391] Loss: 0.096 | Acc: 96.506% (44470/46080)\n",
            "val: [39/79] Loss: 0.456 | Acc: 88.516% (4532/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 203\n",
            "train: [39/391] Loss: 0.083 | Acc: 97.129% (4973/5120)\n",
            "train: [79/391] Loss: 0.078 | Acc: 97.383% (9972/10240)\n",
            "train: [119/391] Loss: 0.077 | Acc: 97.428% (14965/15360)\n",
            "train: [159/391] Loss: 0.080 | Acc: 97.354% (19938/20480)\n",
            "train: [199/391] Loss: 0.079 | Acc: 97.430% (24942/25600)\n",
            "train: [239/391] Loss: 0.081 | Acc: 97.363% (29910/30720)\n",
            "train: [279/391] Loss: 0.082 | Acc: 97.288% (34868/35840)\n",
            "train: [319/391] Loss: 0.081 | Acc: 97.305% (39856/40960)\n",
            "train: [359/391] Loss: 0.081 | Acc: 97.316% (44843/46080)\n",
            "val: [39/79] Loss: 0.466 | Acc: 88.750% (4544/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 204\n",
            "train: [39/391] Loss: 0.074 | Acc: 97.422% (4988/5120)\n",
            "train: [79/391] Loss: 0.070 | Acc: 97.471% (9981/10240)\n",
            "train: [119/391] Loss: 0.070 | Acc: 97.428% (14965/15360)\n",
            "train: [159/391] Loss: 0.072 | Acc: 97.451% (19958/20480)\n",
            "train: [199/391] Loss: 0.071 | Acc: 97.438% (24944/25600)\n",
            "train: [239/391] Loss: 0.072 | Acc: 97.435% (29932/30720)\n",
            "train: [279/391] Loss: 0.071 | Acc: 97.480% (34937/35840)\n",
            "train: [319/391] Loss: 0.071 | Acc: 97.478% (39927/40960)\n",
            "train: [359/391] Loss: 0.071 | Acc: 97.474% (44916/46080)\n",
            "val: [39/79] Loss: 0.456 | Acc: 89.277% (4571/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 205\n",
            "train: [39/391] Loss: 0.058 | Acc: 98.203% (5028/5120)\n",
            "train: [79/391] Loss: 0.059 | Acc: 98.135% (10049/10240)\n",
            "train: [119/391] Loss: 0.057 | Acc: 98.138% (15074/15360)\n",
            "train: [159/391] Loss: 0.056 | Acc: 98.193% (20110/20480)\n",
            "train: [199/391] Loss: 0.058 | Acc: 98.156% (25128/25600)\n",
            "train: [239/391] Loss: 0.056 | Acc: 98.180% (30161/30720)\n",
            "train: [279/391] Loss: 0.056 | Acc: 98.161% (35181/35840)\n",
            "train: [319/391] Loss: 0.057 | Acc: 98.169% (40210/40960)\n",
            "train: [359/391] Loss: 0.057 | Acc: 98.151% (45228/46080)\n",
            "val: [39/79] Loss: 0.476 | Acc: 89.121% (4563/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 206\n",
            "train: [39/391] Loss: 0.051 | Acc: 98.145% (5025/5120)\n",
            "train: [79/391] Loss: 0.050 | Acc: 98.262% (10062/10240)\n",
            "train: [119/391] Loss: 0.052 | Acc: 98.242% (15090/15360)\n",
            "train: [159/391] Loss: 0.051 | Acc: 98.267% (20125/20480)\n",
            "train: [199/391] Loss: 0.050 | Acc: 98.285% (25161/25600)\n",
            "train: [239/391] Loss: 0.050 | Acc: 98.314% (30202/30720)\n",
            "train: [279/391] Loss: 0.049 | Acc: 98.343% (35246/35840)\n",
            "train: [319/391] Loss: 0.048 | Acc: 98.369% (40292/40960)\n",
            "train: [359/391] Loss: 0.047 | Acc: 98.372% (45330/46080)\n",
            "val: [39/79] Loss: 0.470 | Acc: 89.492% (4582/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 207\n",
            "train: [39/391] Loss: 0.042 | Acc: 98.535% (5045/5120)\n",
            "train: [79/391] Loss: 0.042 | Acc: 98.535% (10090/10240)\n",
            "train: [119/391] Loss: 0.042 | Acc: 98.574% (15141/15360)\n",
            "train: [159/391] Loss: 0.042 | Acc: 98.584% (20190/20480)\n",
            "train: [199/391] Loss: 0.043 | Acc: 98.559% (25231/25600)\n",
            "train: [239/391] Loss: 0.042 | Acc: 98.600% (30290/30720)\n",
            "train: [279/391] Loss: 0.041 | Acc: 98.602% (35339/35840)\n",
            "train: [319/391] Loss: 0.041 | Acc: 98.608% (40390/40960)\n",
            "train: [359/391] Loss: 0.041 | Acc: 98.605% (45437/46080)\n",
            "val: [39/79] Loss: 0.492 | Acc: 89.180% (4566/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 208\n",
            "train: [39/391] Loss: 0.044 | Acc: 98.477% (5042/5120)\n",
            "train: [79/391] Loss: 0.047 | Acc: 98.438% (10080/10240)\n",
            "train: [119/391] Loss: 0.043 | Acc: 98.581% (15142/15360)\n",
            "train: [159/391] Loss: 0.042 | Acc: 98.584% (20190/20480)\n",
            "train: [199/391] Loss: 0.040 | Acc: 98.590% (25239/25600)\n",
            "train: [239/391] Loss: 0.040 | Acc: 98.610% (30293/30720)\n",
            "train: [279/391] Loss: 0.038 | Acc: 98.703% (35375/35840)\n",
            "train: [319/391] Loss: 0.038 | Acc: 98.723% (40437/40960)\n",
            "train: [359/391] Loss: 0.037 | Acc: 98.759% (45508/46080)\n",
            "val: [39/79] Loss: 0.482 | Acc: 89.355% (4575/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 209\n",
            "train: [39/391] Loss: 0.030 | Acc: 98.965% (5067/5120)\n",
            "train: [79/391] Loss: 0.035 | Acc: 98.867% (10124/10240)\n",
            "train: [119/391] Loss: 0.034 | Acc: 98.939% (15197/15360)\n",
            "train: [159/391] Loss: 0.033 | Acc: 98.926% (20260/20480)\n",
            "train: [199/391] Loss: 0.033 | Acc: 98.926% (25325/25600)\n",
            "train: [239/391] Loss: 0.033 | Acc: 98.910% (30385/30720)\n",
            "train: [279/391] Loss: 0.033 | Acc: 98.909% (35449/35840)\n",
            "train: [319/391] Loss: 0.033 | Acc: 98.904% (40511/40960)\n",
            "train: [359/391] Loss: 0.033 | Acc: 98.906% (45576/46080)\n",
            "val: [39/79] Loss: 0.480 | Acc: 89.453% (4580/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 210\n",
            "train: [39/391] Loss: 0.035 | Acc: 99.004% (5069/5120)\n",
            "train: [79/391] Loss: 0.058 | Acc: 98.047% (10040/10240)\n",
            "train: [119/391] Loss: 0.070 | Acc: 97.565% (14986/15360)\n",
            "train: [159/391] Loss: 0.078 | Acc: 97.280% (19923/20480)\n",
            "train: [199/391] Loss: 0.087 | Acc: 96.945% (24818/25600)\n",
            "train: [239/391] Loss: 0.090 | Acc: 96.875% (29760/30720)\n",
            "train: [279/391] Loss: 0.092 | Acc: 96.755% (34677/35840)\n",
            "train: [319/391] Loss: 0.094 | Acc: 96.672% (39597/40960)\n",
            "train: [359/391] Loss: 0.096 | Acc: 96.632% (44528/46080)\n",
            "val: [39/79] Loss: 0.465 | Acc: 88.301% (4521/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 211\n",
            "train: [39/391] Loss: 0.090 | Acc: 96.836% (4958/5120)\n",
            "train: [79/391] Loss: 0.090 | Acc: 96.875% (9920/10240)\n",
            "train: [119/391] Loss: 0.094 | Acc: 96.660% (14847/15360)\n",
            "train: [159/391] Loss: 0.096 | Acc: 96.597% (19783/20480)\n",
            "train: [199/391] Loss: 0.097 | Acc: 96.555% (24718/25600)\n",
            "train: [239/391] Loss: 0.097 | Acc: 96.598% (29675/30720)\n",
            "train: [279/391] Loss: 0.098 | Acc: 96.501% (34586/35840)\n",
            "train: [319/391] Loss: 0.098 | Acc: 96.526% (39537/40960)\n",
            "train: [359/391] Loss: 0.097 | Acc: 96.547% (44489/46080)\n",
            "val: [39/79] Loss: 0.484 | Acc: 88.555% (4534/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 212\n",
            "train: [39/391] Loss: 0.085 | Acc: 96.875% (4960/5120)\n",
            "train: [79/391] Loss: 0.085 | Acc: 96.924% (9925/10240)\n",
            "train: [119/391] Loss: 0.083 | Acc: 96.960% (14893/15360)\n",
            "train: [159/391] Loss: 0.081 | Acc: 97.046% (19875/20480)\n",
            "train: [199/391] Loss: 0.083 | Acc: 96.961% (24822/25600)\n",
            "train: [239/391] Loss: 0.084 | Acc: 96.924% (29775/30720)\n",
            "train: [279/391] Loss: 0.085 | Acc: 96.906% (34731/35840)\n",
            "train: [319/391] Loss: 0.088 | Acc: 96.812% (39654/40960)\n",
            "train: [359/391] Loss: 0.089 | Acc: 96.795% (44603/46080)\n",
            "val: [39/79] Loss: 0.461 | Acc: 88.105% (4511/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 213\n",
            "train: [39/391] Loss: 0.070 | Acc: 97.656% (5000/5120)\n",
            "train: [79/391] Loss: 0.072 | Acc: 97.441% (9978/10240)\n",
            "train: [119/391] Loss: 0.074 | Acc: 97.422% (14964/15360)\n",
            "train: [159/391] Loss: 0.073 | Acc: 97.407% (19949/20480)\n",
            "train: [199/391] Loss: 0.074 | Acc: 97.359% (24924/25600)\n",
            "train: [239/391] Loss: 0.077 | Acc: 97.272% (29882/30720)\n",
            "train: [279/391] Loss: 0.078 | Acc: 97.288% (34868/35840)\n",
            "train: [319/391] Loss: 0.078 | Acc: 97.278% (39845/40960)\n",
            "train: [359/391] Loss: 0.079 | Acc: 97.246% (44811/46080)\n",
            "val: [39/79] Loss: 0.483 | Acc: 88.477% (4530/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 214\n",
            "train: [39/391] Loss: 0.068 | Acc: 97.930% (5014/5120)\n",
            "train: [79/391] Loss: 0.067 | Acc: 97.822% (10017/10240)\n",
            "train: [119/391] Loss: 0.064 | Acc: 97.910% (15039/15360)\n",
            "train: [159/391] Loss: 0.066 | Acc: 97.876% (20045/20480)\n",
            "train: [199/391] Loss: 0.066 | Acc: 97.820% (25042/25600)\n",
            "train: [239/391] Loss: 0.067 | Acc: 97.773% (30036/30720)\n",
            "train: [279/391] Loss: 0.067 | Acc: 97.729% (35026/35840)\n",
            "train: [319/391] Loss: 0.067 | Acc: 97.695% (40016/40960)\n",
            "train: [359/391] Loss: 0.067 | Acc: 97.684% (45013/46080)\n",
            "val: [39/79] Loss: 0.484 | Acc: 88.633% (4538/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 215\n",
            "train: [39/391] Loss: 0.064 | Acc: 97.656% (5000/5120)\n",
            "train: [79/391] Loss: 0.057 | Acc: 97.842% (10019/10240)\n",
            "train: [119/391] Loss: 0.056 | Acc: 97.910% (15039/15360)\n",
            "train: [159/391] Loss: 0.055 | Acc: 97.969% (20064/20480)\n",
            "train: [199/391] Loss: 0.056 | Acc: 97.938% (25072/25600)\n",
            "train: [239/391] Loss: 0.055 | Acc: 98.014% (30110/30720)\n",
            "train: [279/391] Loss: 0.053 | Acc: 98.089% (35155/35840)\n",
            "train: [319/391] Loss: 0.053 | Acc: 98.088% (40177/40960)\n",
            "train: [359/391] Loss: 0.053 | Acc: 98.090% (45200/46080)\n",
            "val: [39/79] Loss: 0.500 | Acc: 88.945% (4554/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 216\n",
            "train: [39/391] Loss: 0.053 | Acc: 98.047% (5020/5120)\n",
            "train: [79/391] Loss: 0.051 | Acc: 98.252% (10061/10240)\n",
            "train: [119/391] Loss: 0.049 | Acc: 98.288% (15097/15360)\n",
            "train: [159/391] Loss: 0.049 | Acc: 98.354% (20143/20480)\n",
            "train: [199/391] Loss: 0.049 | Acc: 98.312% (25168/25600)\n",
            "train: [239/391] Loss: 0.048 | Acc: 98.343% (30211/30720)\n",
            "train: [279/391] Loss: 0.049 | Acc: 98.315% (35236/35840)\n",
            "train: [319/391] Loss: 0.049 | Acc: 98.308% (40267/40960)\n",
            "train: [359/391] Loss: 0.049 | Acc: 98.322% (45307/46080)\n",
            "val: [39/79] Loss: 0.487 | Acc: 89.141% (4564/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 217\n",
            "train: [39/391] Loss: 0.036 | Acc: 98.770% (5057/5120)\n",
            "train: [79/391] Loss: 0.040 | Acc: 98.643% (10101/10240)\n",
            "train: [119/391] Loss: 0.039 | Acc: 98.665% (15155/15360)\n",
            "train: [159/391] Loss: 0.039 | Acc: 98.677% (20209/20480)\n",
            "train: [199/391] Loss: 0.040 | Acc: 98.617% (25246/25600)\n",
            "train: [239/391] Loss: 0.039 | Acc: 98.636% (30301/30720)\n",
            "train: [279/391] Loss: 0.039 | Acc: 98.630% (35349/35840)\n",
            "train: [319/391] Loss: 0.038 | Acc: 98.662% (40412/40960)\n",
            "train: [359/391] Loss: 0.038 | Acc: 98.681% (45472/46080)\n",
            "val: [39/79] Loss: 0.492 | Acc: 89.434% (4579/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 218\n",
            "train: [39/391] Loss: 0.035 | Acc: 98.926% (5065/5120)\n",
            "train: [79/391] Loss: 0.035 | Acc: 98.857% (10123/10240)\n",
            "train: [119/391] Loss: 0.036 | Acc: 98.815% (15178/15360)\n",
            "train: [159/391] Loss: 0.035 | Acc: 98.833% (20241/20480)\n",
            "train: [199/391] Loss: 0.034 | Acc: 98.859% (25308/25600)\n",
            "train: [239/391] Loss: 0.034 | Acc: 98.890% (30379/30720)\n",
            "train: [279/391] Loss: 0.033 | Acc: 98.917% (35452/35840)\n",
            "train: [319/391] Loss: 0.033 | Acc: 98.931% (40522/40960)\n",
            "train: [359/391] Loss: 0.032 | Acc: 98.913% (45579/46080)\n",
            "val: [39/79] Loss: 0.495 | Acc: 89.453% (4580/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 219\n",
            "train: [39/391] Loss: 0.033 | Acc: 98.945% (5066/5120)\n",
            "train: [79/391] Loss: 0.030 | Acc: 98.994% (10137/10240)\n",
            "train: [119/391] Loss: 0.030 | Acc: 98.965% (15201/15360)\n",
            "train: [159/391] Loss: 0.030 | Acc: 98.984% (20272/20480)\n",
            "train: [199/391] Loss: 0.031 | Acc: 98.988% (25341/25600)\n",
            "train: [239/391] Loss: 0.030 | Acc: 99.020% (30419/30720)\n",
            "train: [279/391] Loss: 0.030 | Acc: 98.996% (35480/35840)\n",
            "train: [319/391] Loss: 0.030 | Acc: 98.994% (40548/40960)\n",
            "train: [359/391] Loss: 0.029 | Acc: 98.969% (45605/46080)\n",
            "val: [39/79] Loss: 0.495 | Acc: 89.785% (4597/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 220\n",
            "train: [39/391] Loss: 0.042 | Acc: 98.672% (5052/5120)\n",
            "train: [79/391] Loss: 0.063 | Acc: 97.881% (10023/10240)\n",
            "train: [119/391] Loss: 0.074 | Acc: 97.480% (14973/15360)\n",
            "train: [159/391] Loss: 0.081 | Acc: 97.173% (19901/20480)\n",
            "train: [199/391] Loss: 0.087 | Acc: 96.965% (24823/25600)\n",
            "train: [239/391] Loss: 0.088 | Acc: 96.911% (29771/30720)\n",
            "train: [279/391] Loss: 0.088 | Acc: 96.864% (34716/35840)\n",
            "train: [319/391] Loss: 0.090 | Acc: 96.812% (39654/40960)\n",
            "train: [359/391] Loss: 0.092 | Acc: 96.762% (44588/46080)\n",
            "val: [39/79] Loss: 0.499 | Acc: 88.242% (4518/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 221\n",
            "train: [39/391] Loss: 0.091 | Acc: 96.934% (4963/5120)\n",
            "train: [79/391] Loss: 0.099 | Acc: 96.543% (9886/10240)\n",
            "train: [119/391] Loss: 0.095 | Acc: 96.673% (14849/15360)\n",
            "train: [159/391] Loss: 0.095 | Acc: 96.685% (19801/20480)\n",
            "train: [199/391] Loss: 0.093 | Acc: 96.754% (24769/25600)\n",
            "train: [239/391] Loss: 0.094 | Acc: 96.689% (29703/30720)\n",
            "train: [279/391] Loss: 0.095 | Acc: 96.643% (34637/35840)\n",
            "train: [319/391] Loss: 0.097 | Acc: 96.550% (39547/40960)\n",
            "train: [359/391] Loss: 0.098 | Acc: 96.495% (44465/46080)\n",
            "val: [39/79] Loss: 0.452 | Acc: 88.438% (4528/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 222\n",
            "train: [39/391] Loss: 0.068 | Acc: 97.520% (4993/5120)\n",
            "train: [79/391] Loss: 0.076 | Acc: 97.285% (9962/10240)\n",
            "train: [119/391] Loss: 0.076 | Acc: 97.279% (14942/15360)\n",
            "train: [159/391] Loss: 0.075 | Acc: 97.324% (19932/20480)\n",
            "train: [199/391] Loss: 0.075 | Acc: 97.316% (24913/25600)\n",
            "train: [239/391] Loss: 0.078 | Acc: 97.279% (29884/30720)\n",
            "train: [279/391] Loss: 0.080 | Acc: 97.229% (34847/35840)\n",
            "train: [319/391] Loss: 0.082 | Acc: 97.104% (39774/40960)\n",
            "train: [359/391] Loss: 0.083 | Acc: 97.066% (44728/46080)\n",
            "val: [39/79] Loss: 0.457 | Acc: 88.574% (4535/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 223\n",
            "train: [39/391] Loss: 0.076 | Acc: 97.246% (4979/5120)\n",
            "train: [79/391] Loss: 0.073 | Acc: 97.373% (9971/10240)\n",
            "train: [119/391] Loss: 0.076 | Acc: 97.279% (14942/15360)\n",
            "train: [159/391] Loss: 0.077 | Acc: 97.314% (19930/20480)\n",
            "train: [199/391] Loss: 0.075 | Acc: 97.371% (24927/25600)\n",
            "train: [239/391] Loss: 0.073 | Acc: 97.438% (29933/30720)\n",
            "train: [279/391] Loss: 0.074 | Acc: 97.453% (34927/35840)\n",
            "train: [319/391] Loss: 0.073 | Acc: 97.454% (39917/40960)\n",
            "train: [359/391] Loss: 0.072 | Acc: 97.470% (44914/46080)\n",
            "val: [39/79] Loss: 0.485 | Acc: 88.730% (4543/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 224\n",
            "train: [39/391] Loss: 0.069 | Acc: 97.715% (5003/5120)\n",
            "train: [79/391] Loss: 0.072 | Acc: 97.490% (9983/10240)\n",
            "train: [119/391] Loss: 0.069 | Acc: 97.591% (14990/15360)\n",
            "train: [159/391] Loss: 0.067 | Acc: 97.661% (20001/20480)\n",
            "train: [199/391] Loss: 0.067 | Acc: 97.637% (24995/25600)\n",
            "train: [239/391] Loss: 0.067 | Acc: 97.666% (30003/30720)\n",
            "train: [279/391] Loss: 0.067 | Acc: 97.626% (34989/35840)\n",
            "train: [319/391] Loss: 0.066 | Acc: 97.651% (39998/40960)\n",
            "train: [359/391] Loss: 0.066 | Acc: 97.669% (45006/46080)\n",
            "val: [39/79] Loss: 0.471 | Acc: 88.711% (4542/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 225\n",
            "train: [39/391] Loss: 0.056 | Acc: 98.105% (5023/5120)\n",
            "train: [79/391] Loss: 0.054 | Acc: 98.213% (10057/10240)\n",
            "train: [119/391] Loss: 0.053 | Acc: 98.301% (15099/15360)\n",
            "train: [159/391] Loss: 0.053 | Acc: 98.223% (20116/20480)\n",
            "train: [199/391] Loss: 0.053 | Acc: 98.172% (25132/25600)\n",
            "train: [239/391] Loss: 0.053 | Acc: 98.171% (30158/30720)\n",
            "train: [279/391] Loss: 0.053 | Acc: 98.175% (35186/35840)\n",
            "train: [319/391] Loss: 0.053 | Acc: 98.196% (40221/40960)\n",
            "train: [359/391] Loss: 0.053 | Acc: 98.194% (45248/46080)\n",
            "val: [39/79] Loss: 0.482 | Acc: 89.180% (4566/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 226\n",
            "train: [39/391] Loss: 0.053 | Acc: 98.105% (5023/5120)\n",
            "train: [79/391] Loss: 0.048 | Acc: 98.398% (10076/10240)\n",
            "train: [119/391] Loss: 0.043 | Acc: 98.490% (15128/15360)\n",
            "train: [159/391] Loss: 0.043 | Acc: 98.535% (20180/20480)\n",
            "train: [199/391] Loss: 0.043 | Acc: 98.516% (25220/25600)\n",
            "train: [239/391] Loss: 0.044 | Acc: 98.477% (30252/30720)\n",
            "train: [279/391] Loss: 0.044 | Acc: 98.446% (35283/35840)\n",
            "train: [319/391] Loss: 0.045 | Acc: 98.418% (40312/40960)\n",
            "train: [359/391] Loss: 0.044 | Acc: 98.433% (45358/46080)\n",
            "val: [39/79] Loss: 0.470 | Acc: 89.102% (4562/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 227\n",
            "train: [39/391] Loss: 0.039 | Acc: 98.789% (5058/5120)\n",
            "train: [79/391] Loss: 0.033 | Acc: 98.926% (10130/10240)\n",
            "train: [119/391] Loss: 0.036 | Acc: 98.802% (15176/15360)\n",
            "train: [159/391] Loss: 0.036 | Acc: 98.755% (20225/20480)\n",
            "train: [199/391] Loss: 0.036 | Acc: 98.738% (25277/25600)\n",
            "train: [239/391] Loss: 0.036 | Acc: 98.730% (30330/30720)\n",
            "train: [279/391] Loss: 0.035 | Acc: 98.778% (35402/35840)\n",
            "train: [319/391] Loss: 0.035 | Acc: 98.777% (40459/40960)\n",
            "train: [359/391] Loss: 0.035 | Acc: 98.759% (45508/46080)\n",
            "val: [39/79] Loss: 0.498 | Acc: 89.023% (4558/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 228\n",
            "train: [39/391] Loss: 0.030 | Acc: 98.945% (5066/5120)\n",
            "train: [79/391] Loss: 0.031 | Acc: 98.945% (10132/10240)\n",
            "train: [119/391] Loss: 0.031 | Acc: 99.010% (15208/15360)\n",
            "train: [159/391] Loss: 0.029 | Acc: 99.038% (20283/20480)\n",
            "train: [199/391] Loss: 0.031 | Acc: 98.977% (25338/25600)\n",
            "train: [239/391] Loss: 0.031 | Acc: 98.968% (30403/30720)\n",
            "train: [279/391] Loss: 0.031 | Acc: 98.993% (35479/35840)\n",
            "train: [319/391] Loss: 0.031 | Acc: 98.987% (40545/40960)\n",
            "train: [359/391] Loss: 0.030 | Acc: 99.017% (45627/46080)\n",
            "val: [39/79] Loss: 0.489 | Acc: 89.277% (4571/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 229\n",
            "train: [39/391] Loss: 0.024 | Acc: 99.141% (5076/5120)\n",
            "train: [79/391] Loss: 0.025 | Acc: 99.160% (10154/10240)\n",
            "train: [119/391] Loss: 0.025 | Acc: 99.141% (15228/15360)\n",
            "train: [159/391] Loss: 0.027 | Acc: 99.106% (20297/20480)\n",
            "train: [199/391] Loss: 0.029 | Acc: 99.051% (25357/25600)\n",
            "train: [239/391] Loss: 0.029 | Acc: 99.040% (30425/30720)\n",
            "train: [279/391] Loss: 0.029 | Acc: 99.051% (35500/35840)\n",
            "train: [319/391] Loss: 0.029 | Acc: 99.038% (40566/40960)\n",
            "train: [359/391] Loss: 0.029 | Acc: 99.043% (45639/46080)\n",
            "val: [39/79] Loss: 0.490 | Acc: 89.492% (4582/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 230\n",
            "train: [39/391] Loss: 0.040 | Acc: 98.477% (5042/5120)\n",
            "train: [79/391] Loss: 0.051 | Acc: 98.154% (10051/10240)\n",
            "train: [119/391] Loss: 0.065 | Acc: 97.760% (15016/15360)\n",
            "train: [159/391] Loss: 0.073 | Acc: 97.534% (19975/20480)\n",
            "train: [199/391] Loss: 0.080 | Acc: 97.277% (24903/25600)\n",
            "train: [239/391] Loss: 0.085 | Acc: 97.093% (29827/30720)\n",
            "train: [279/391] Loss: 0.087 | Acc: 96.995% (34763/35840)\n",
            "train: [319/391] Loss: 0.087 | Acc: 96.987% (39726/40960)\n",
            "train: [359/391] Loss: 0.089 | Acc: 96.903% (44653/46080)\n",
            "val: [39/79] Loss: 0.500 | Acc: 87.656% (4488/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 231\n",
            "train: [39/391] Loss: 0.093 | Acc: 96.699% (4951/5120)\n",
            "train: [79/391] Loss: 0.091 | Acc: 96.826% (9915/10240)\n",
            "train: [119/391] Loss: 0.086 | Acc: 97.025% (14903/15360)\n",
            "train: [159/391] Loss: 0.087 | Acc: 97.021% (19870/20480)\n",
            "train: [199/391] Loss: 0.087 | Acc: 96.957% (24821/25600)\n",
            "train: [239/391] Loss: 0.087 | Acc: 96.989% (29795/30720)\n",
            "train: [279/391] Loss: 0.087 | Acc: 97.015% (34770/35840)\n",
            "train: [319/391] Loss: 0.087 | Acc: 97.009% (39735/40960)\n",
            "train: [359/391] Loss: 0.087 | Acc: 97.033% (44713/46080)\n",
            "val: [39/79] Loss: 0.456 | Acc: 88.789% (4546/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 232\n",
            "train: [39/391] Loss: 0.079 | Acc: 97.207% (4977/5120)\n",
            "train: [79/391] Loss: 0.087 | Acc: 97.012% (9934/10240)\n",
            "train: [119/391] Loss: 0.087 | Acc: 97.090% (14913/15360)\n",
            "train: [159/391] Loss: 0.085 | Acc: 97.075% (19881/20480)\n",
            "train: [199/391] Loss: 0.083 | Acc: 97.129% (24865/25600)\n",
            "train: [239/391] Loss: 0.082 | Acc: 97.152% (29845/30720)\n",
            "train: [279/391] Loss: 0.083 | Acc: 97.104% (34802/35840)\n",
            "train: [319/391] Loss: 0.084 | Acc: 97.051% (39752/40960)\n",
            "train: [359/391] Loss: 0.084 | Acc: 97.023% (44708/46080)\n",
            "val: [39/79] Loss: 0.460 | Acc: 88.730% (4543/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 233\n",
            "train: [39/391] Loss: 0.080 | Acc: 97.051% (4969/5120)\n",
            "train: [79/391] Loss: 0.077 | Acc: 97.197% (9953/10240)\n",
            "train: [119/391] Loss: 0.075 | Acc: 97.331% (14950/15360)\n",
            "train: [159/391] Loss: 0.077 | Acc: 97.275% (19922/20480)\n",
            "train: [199/391] Loss: 0.075 | Acc: 97.332% (24917/25600)\n",
            "train: [239/391] Loss: 0.073 | Acc: 97.386% (29917/30720)\n",
            "train: [279/391] Loss: 0.072 | Acc: 97.419% (34915/35840)\n",
            "train: [319/391] Loss: 0.072 | Acc: 97.427% (39906/40960)\n",
            "train: [359/391] Loss: 0.073 | Acc: 97.398% (44881/46080)\n",
            "val: [39/79] Loss: 0.455 | Acc: 88.594% (4536/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 234\n",
            "train: [39/391] Loss: 0.065 | Acc: 97.812% (5008/5120)\n",
            "train: [79/391] Loss: 0.062 | Acc: 97.812% (10016/10240)\n",
            "train: [119/391] Loss: 0.062 | Acc: 97.812% (15024/15360)\n",
            "train: [159/391] Loss: 0.060 | Acc: 97.812% (20032/20480)\n",
            "train: [199/391] Loss: 0.061 | Acc: 97.773% (25030/25600)\n",
            "train: [239/391] Loss: 0.060 | Acc: 97.773% (30036/30720)\n",
            "train: [279/391] Loss: 0.059 | Acc: 97.815% (35057/35840)\n",
            "train: [319/391] Loss: 0.059 | Acc: 97.839% (40075/40960)\n",
            "train: [359/391] Loss: 0.059 | Acc: 97.828% (45079/46080)\n",
            "val: [39/79] Loss: 0.492 | Acc: 88.535% (4533/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 235\n",
            "train: [39/391] Loss: 0.045 | Acc: 98.418% (5039/5120)\n",
            "train: [79/391] Loss: 0.052 | Acc: 98.193% (10055/10240)\n",
            "train: [119/391] Loss: 0.050 | Acc: 98.190% (15082/15360)\n",
            "train: [159/391] Loss: 0.050 | Acc: 98.179% (20107/20480)\n",
            "train: [199/391] Loss: 0.051 | Acc: 98.195% (25138/25600)\n",
            "train: [239/391] Loss: 0.052 | Acc: 98.193% (30165/30720)\n",
            "train: [279/391] Loss: 0.052 | Acc: 98.192% (35192/35840)\n",
            "train: [319/391] Loss: 0.053 | Acc: 98.164% (40208/40960)\n",
            "train: [359/391] Loss: 0.052 | Acc: 98.173% (45238/46080)\n",
            "val: [39/79] Loss: 0.469 | Acc: 89.023% (4558/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 236\n",
            "train: [39/391] Loss: 0.042 | Acc: 98.672% (5052/5120)\n",
            "train: [79/391] Loss: 0.041 | Acc: 98.691% (10106/10240)\n",
            "train: [119/391] Loss: 0.043 | Acc: 98.568% (15140/15360)\n",
            "train: [159/391] Loss: 0.042 | Acc: 98.569% (20187/20480)\n",
            "train: [199/391] Loss: 0.043 | Acc: 98.555% (25230/25600)\n",
            "train: [239/391] Loss: 0.043 | Acc: 98.555% (30276/30720)\n",
            "train: [279/391] Loss: 0.043 | Acc: 98.566% (35326/35840)\n",
            "train: [319/391] Loss: 0.042 | Acc: 98.582% (40379/40960)\n",
            "train: [359/391] Loss: 0.043 | Acc: 98.544% (45409/46080)\n",
            "val: [39/79] Loss: 0.477 | Acc: 89.512% (4583/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 237\n",
            "train: [39/391] Loss: 0.041 | Acc: 98.711% (5054/5120)\n",
            "train: [79/391] Loss: 0.037 | Acc: 98.760% (10113/10240)\n",
            "train: [119/391] Loss: 0.038 | Acc: 98.743% (15167/15360)\n",
            "train: [159/391] Loss: 0.037 | Acc: 98.765% (20227/20480)\n",
            "train: [199/391] Loss: 0.037 | Acc: 98.781% (25288/25600)\n",
            "train: [239/391] Loss: 0.037 | Acc: 98.770% (30342/30720)\n",
            "train: [279/391] Loss: 0.036 | Acc: 98.770% (35399/35840)\n",
            "train: [319/391] Loss: 0.037 | Acc: 98.718% (40435/40960)\n",
            "train: [359/391] Loss: 0.037 | Acc: 98.715% (45488/46080)\n",
            "val: [39/79] Loss: 0.476 | Acc: 89.746% (4595/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 238\n",
            "train: [39/391] Loss: 0.032 | Acc: 99.043% (5071/5120)\n",
            "train: [79/391] Loss: 0.032 | Acc: 99.014% (10139/10240)\n",
            "train: [119/391] Loss: 0.031 | Acc: 98.984% (15204/15360)\n",
            "train: [159/391] Loss: 0.030 | Acc: 98.979% (20271/20480)\n",
            "train: [199/391] Loss: 0.031 | Acc: 98.953% (25332/25600)\n",
            "train: [239/391] Loss: 0.030 | Acc: 98.997% (30412/30720)\n",
            "train: [279/391] Loss: 0.030 | Acc: 98.954% (35465/35840)\n",
            "train: [319/391] Loss: 0.029 | Acc: 98.984% (40544/40960)\n",
            "train: [359/391] Loss: 0.030 | Acc: 98.982% (45611/46080)\n",
            "val: [39/79] Loss: 0.484 | Acc: 89.980% (4607/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 239\n",
            "train: [39/391] Loss: 0.028 | Acc: 98.984% (5068/5120)\n",
            "train: [79/391] Loss: 0.029 | Acc: 98.955% (10133/10240)\n",
            "train: [119/391] Loss: 0.029 | Acc: 98.971% (15202/15360)\n",
            "train: [159/391] Loss: 0.029 | Acc: 99.004% (20276/20480)\n",
            "train: [199/391] Loss: 0.029 | Acc: 99.004% (25345/25600)\n",
            "train: [239/391] Loss: 0.029 | Acc: 99.027% (30421/30720)\n",
            "train: [279/391] Loss: 0.028 | Acc: 99.057% (35502/35840)\n",
            "train: [319/391] Loss: 0.028 | Acc: 99.045% (40569/40960)\n",
            "train: [359/391] Loss: 0.029 | Acc: 99.015% (45626/46080)\n",
            "val: [39/79] Loss: 0.473 | Acc: 89.863% (4601/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 240\n",
            "train: [39/391] Loss: 0.050 | Acc: 98.301% (5033/5120)\n",
            "train: [79/391] Loss: 0.057 | Acc: 98.115% (10047/10240)\n",
            "train: [119/391] Loss: 0.071 | Acc: 97.637% (14997/15360)\n",
            "train: [159/391] Loss: 0.080 | Acc: 97.290% (19925/20480)\n",
            "train: [199/391] Loss: 0.084 | Acc: 97.133% (24866/25600)\n",
            "train: [239/391] Loss: 0.085 | Acc: 97.090% (29826/30720)\n",
            "train: [279/391] Loss: 0.086 | Acc: 97.031% (34776/35840)\n",
            "train: [319/391] Loss: 0.087 | Acc: 96.995% (39729/40960)\n",
            "train: [359/391] Loss: 0.087 | Acc: 96.984% (44690/46080)\n",
            "val: [39/79] Loss: 0.498 | Acc: 88.008% (4506/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 241\n",
            "train: [39/391] Loss: 0.097 | Acc: 96.660% (4949/5120)\n",
            "train: [79/391] Loss: 0.090 | Acc: 96.797% (9912/10240)\n",
            "train: [119/391] Loss: 0.091 | Acc: 96.803% (14869/15360)\n",
            "train: [159/391] Loss: 0.089 | Acc: 96.807% (19826/20480)\n",
            "train: [199/391] Loss: 0.090 | Acc: 96.801% (24781/25600)\n",
            "train: [239/391] Loss: 0.089 | Acc: 96.836% (29748/30720)\n",
            "train: [279/391] Loss: 0.090 | Acc: 96.814% (34698/35840)\n",
            "train: [319/391] Loss: 0.088 | Acc: 96.873% (39679/40960)\n",
            "train: [359/391] Loss: 0.088 | Acc: 96.862% (44634/46080)\n",
            "val: [39/79] Loss: 0.475 | Acc: 88.203% (4516/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 242\n",
            "train: [39/391] Loss: 0.074 | Acc: 97.539% (4994/5120)\n",
            "train: [79/391] Loss: 0.070 | Acc: 97.578% (9992/10240)\n",
            "train: [119/391] Loss: 0.071 | Acc: 97.624% (14995/15360)\n",
            "train: [159/391] Loss: 0.070 | Acc: 97.568% (19982/20480)\n",
            "train: [199/391] Loss: 0.075 | Acc: 97.391% (24932/25600)\n",
            "train: [239/391] Loss: 0.075 | Acc: 97.376% (29914/30720)\n",
            "train: [279/391] Loss: 0.075 | Acc: 97.374% (34899/35840)\n",
            "train: [319/391] Loss: 0.075 | Acc: 97.366% (39881/40960)\n",
            "train: [359/391] Loss: 0.076 | Acc: 97.363% (44865/46080)\n",
            "val: [39/79] Loss: 0.465 | Acc: 88.672% (4540/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 243\n",
            "train: [39/391] Loss: 0.072 | Acc: 97.480% (4991/5120)\n",
            "train: [79/391] Loss: 0.067 | Acc: 97.637% (9998/10240)\n",
            "train: [119/391] Loss: 0.066 | Acc: 97.630% (14996/15360)\n",
            "train: [159/391] Loss: 0.068 | Acc: 97.598% (19988/20480)\n",
            "train: [199/391] Loss: 0.068 | Acc: 97.598% (24985/25600)\n",
            "train: [239/391] Loss: 0.068 | Acc: 97.562% (29971/30720)\n",
            "train: [279/391] Loss: 0.069 | Acc: 97.561% (34966/35840)\n",
            "train: [319/391] Loss: 0.069 | Acc: 97.542% (39953/40960)\n",
            "train: [359/391] Loss: 0.068 | Acc: 97.559% (44955/46080)\n",
            "val: [39/79] Loss: 0.471 | Acc: 89.023% (4558/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 244\n",
            "train: [39/391] Loss: 0.057 | Acc: 98.027% (5019/5120)\n",
            "train: [79/391] Loss: 0.053 | Acc: 98.125% (10048/10240)\n",
            "train: [119/391] Loss: 0.055 | Acc: 98.027% (15057/15360)\n",
            "train: [159/391] Loss: 0.055 | Acc: 98.042% (20079/20480)\n",
            "train: [199/391] Loss: 0.057 | Acc: 98.016% (25092/25600)\n",
            "train: [239/391] Loss: 0.055 | Acc: 98.031% (30115/30720)\n",
            "train: [279/391] Loss: 0.056 | Acc: 97.997% (35122/35840)\n",
            "train: [319/391] Loss: 0.056 | Acc: 97.986% (40135/40960)\n",
            "train: [359/391] Loss: 0.056 | Acc: 97.967% (45143/46080)\n",
            "val: [39/79] Loss: 0.483 | Acc: 89.160% (4565/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 245\n",
            "train: [39/391] Loss: 0.047 | Acc: 98.613% (5049/5120)\n",
            "train: [79/391] Loss: 0.043 | Acc: 98.613% (10098/10240)\n",
            "train: [119/391] Loss: 0.044 | Acc: 98.542% (15136/15360)\n",
            "train: [159/391] Loss: 0.047 | Acc: 98.481% (20169/20480)\n",
            "train: [199/391] Loss: 0.046 | Acc: 98.473% (25209/25600)\n",
            "train: [239/391] Loss: 0.046 | Acc: 98.431% (30238/30720)\n",
            "train: [279/391] Loss: 0.047 | Acc: 98.390% (35263/35840)\n",
            "train: [319/391] Loss: 0.048 | Acc: 98.340% (40280/40960)\n",
            "train: [359/391] Loss: 0.048 | Acc: 98.329% (45310/46080)\n",
            "val: [39/79] Loss: 0.481 | Acc: 88.926% (4553/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 246\n",
            "train: [39/391] Loss: 0.035 | Acc: 98.691% (5053/5120)\n",
            "train: [79/391] Loss: 0.035 | Acc: 98.672% (10104/10240)\n",
            "train: [119/391] Loss: 0.037 | Acc: 98.652% (15153/15360)\n",
            "train: [159/391] Loss: 0.040 | Acc: 98.608% (20195/20480)\n",
            "train: [199/391] Loss: 0.038 | Acc: 98.668% (25259/25600)\n",
            "train: [239/391] Loss: 0.039 | Acc: 98.659% (30308/30720)\n",
            "train: [279/391] Loss: 0.038 | Acc: 98.669% (35363/35840)\n",
            "train: [319/391] Loss: 0.038 | Acc: 98.652% (40408/40960)\n",
            "train: [359/391] Loss: 0.038 | Acc: 98.678% (45471/46080)\n",
            "val: [39/79] Loss: 0.492 | Acc: 89.316% (4573/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 247\n",
            "train: [39/391] Loss: 0.031 | Acc: 98.965% (5067/5120)\n",
            "train: [79/391] Loss: 0.029 | Acc: 98.955% (10133/10240)\n",
            "train: [119/391] Loss: 0.031 | Acc: 98.932% (15196/15360)\n",
            "train: [159/391] Loss: 0.031 | Acc: 98.945% (20264/20480)\n",
            "train: [199/391] Loss: 0.031 | Acc: 98.910% (25321/25600)\n",
            "train: [239/391] Loss: 0.031 | Acc: 98.903% (30383/30720)\n",
            "train: [279/391] Loss: 0.031 | Acc: 98.909% (35449/35840)\n",
            "train: [319/391] Loss: 0.031 | Acc: 98.901% (40510/40960)\n",
            "train: [359/391] Loss: 0.031 | Acc: 98.891% (45569/46080)\n",
            "val: [39/79] Loss: 0.492 | Acc: 89.727% (4594/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 248\n",
            "train: [39/391] Loss: 0.034 | Acc: 98.867% (5062/5120)\n",
            "train: [79/391] Loss: 0.030 | Acc: 99.004% (10138/10240)\n",
            "train: [119/391] Loss: 0.029 | Acc: 99.010% (15208/15360)\n",
            "train: [159/391] Loss: 0.028 | Acc: 99.082% (20292/20480)\n",
            "train: [199/391] Loss: 0.028 | Acc: 99.094% (25368/25600)\n",
            "train: [239/391] Loss: 0.027 | Acc: 99.076% (30436/30720)\n",
            "train: [279/391] Loss: 0.027 | Acc: 99.102% (35518/35840)\n",
            "train: [319/391] Loss: 0.027 | Acc: 99.087% (40586/40960)\n",
            "train: [359/391] Loss: 0.027 | Acc: 99.099% (45665/46080)\n",
            "val: [39/79] Loss: 0.497 | Acc: 89.961% (4606/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 249\n",
            "train: [39/391] Loss: 0.019 | Acc: 99.453% (5092/5120)\n",
            "train: [79/391] Loss: 0.022 | Acc: 99.258% (10164/10240)\n",
            "train: [119/391] Loss: 0.024 | Acc: 99.134% (15227/15360)\n",
            "train: [159/391] Loss: 0.023 | Acc: 99.194% (20315/20480)\n",
            "train: [199/391] Loss: 0.023 | Acc: 99.188% (25392/25600)\n",
            "train: [239/391] Loss: 0.023 | Acc: 99.176% (30467/30720)\n",
            "train: [279/391] Loss: 0.024 | Acc: 99.169% (35542/35840)\n",
            "train: [319/391] Loss: 0.024 | Acc: 99.167% (40619/40960)\n",
            "train: [359/391] Loss: 0.024 | Acc: 99.167% (45696/46080)\n",
            "val: [39/79] Loss: 0.496 | Acc: 90.000% (4608/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 250\n",
            "train: [39/391] Loss: 0.042 | Acc: 98.496% (5043/5120)\n",
            "train: [79/391] Loss: 0.063 | Acc: 97.764% (10011/10240)\n",
            "train: [119/391] Loss: 0.072 | Acc: 97.500% (14976/15360)\n",
            "train: [159/391] Loss: 0.076 | Acc: 97.319% (19931/20480)\n",
            "train: [199/391] Loss: 0.080 | Acc: 97.219% (24888/25600)\n",
            "train: [239/391] Loss: 0.083 | Acc: 97.096% (29828/30720)\n",
            "train: [279/391] Loss: 0.086 | Acc: 96.970% (34754/35840)\n",
            "train: [319/391] Loss: 0.086 | Acc: 96.958% (39714/40960)\n",
            "train: [359/391] Loss: 0.086 | Acc: 96.955% (44677/46080)\n",
            "val: [39/79] Loss: 0.468 | Acc: 88.516% (4532/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 251\n",
            "train: [39/391] Loss: 0.082 | Acc: 97.090% (4971/5120)\n",
            "train: [79/391] Loss: 0.077 | Acc: 97.227% (9956/10240)\n",
            "train: [119/391] Loss: 0.079 | Acc: 97.168% (14925/15360)\n",
            "train: [159/391] Loss: 0.080 | Acc: 97.148% (19896/20480)\n",
            "train: [199/391] Loss: 0.081 | Acc: 97.109% (24860/25600)\n",
            "train: [239/391] Loss: 0.083 | Acc: 97.057% (29816/30720)\n",
            "train: [279/391] Loss: 0.082 | Acc: 97.084% (34795/35840)\n",
            "train: [319/391] Loss: 0.083 | Acc: 97.056% (39754/40960)\n",
            "train: [359/391] Loss: 0.083 | Acc: 97.007% (44701/46080)\n",
            "val: [39/79] Loss: 0.481 | Acc: 88.164% (4514/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 252\n",
            "train: [39/391] Loss: 0.071 | Acc: 97.363% (4985/5120)\n",
            "train: [79/391] Loss: 0.069 | Acc: 97.471% (9981/10240)\n",
            "train: [119/391] Loss: 0.070 | Acc: 97.428% (14965/15360)\n",
            "train: [159/391] Loss: 0.068 | Acc: 97.549% (19978/20480)\n",
            "train: [199/391] Loss: 0.070 | Acc: 97.555% (24974/25600)\n",
            "train: [239/391] Loss: 0.070 | Acc: 97.559% (29970/30720)\n",
            "train: [279/391] Loss: 0.070 | Acc: 97.517% (34950/35840)\n",
            "train: [319/391] Loss: 0.071 | Acc: 97.466% (39922/40960)\n",
            "train: [359/391] Loss: 0.072 | Acc: 97.431% (44896/46080)\n",
            "val: [39/79] Loss: 0.470 | Acc: 88.477% (4530/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 253\n",
            "train: [39/391] Loss: 0.068 | Acc: 97.734% (5004/5120)\n",
            "train: [79/391] Loss: 0.067 | Acc: 97.773% (10012/10240)\n",
            "train: [119/391] Loss: 0.065 | Acc: 97.819% (15025/15360)\n",
            "train: [159/391] Loss: 0.064 | Acc: 97.837% (20037/20480)\n",
            "train: [199/391] Loss: 0.064 | Acc: 97.762% (25027/25600)\n",
            "train: [239/391] Loss: 0.065 | Acc: 97.738% (30025/30720)\n",
            "train: [279/391] Loss: 0.064 | Acc: 97.771% (35041/35840)\n",
            "train: [319/391] Loss: 0.066 | Acc: 97.717% (40025/40960)\n",
            "train: [359/391] Loss: 0.066 | Acc: 97.721% (45030/46080)\n",
            "val: [39/79] Loss: 0.471 | Acc: 88.867% (4550/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 254\n",
            "train: [39/391] Loss: 0.057 | Acc: 98.047% (5020/5120)\n",
            "train: [79/391] Loss: 0.055 | Acc: 98.154% (10051/10240)\n",
            "train: [119/391] Loss: 0.053 | Acc: 98.151% (15076/15360)\n",
            "train: [159/391] Loss: 0.054 | Acc: 98.154% (20102/20480)\n",
            "train: [199/391] Loss: 0.055 | Acc: 98.109% (25116/25600)\n",
            "train: [239/391] Loss: 0.055 | Acc: 98.079% (30130/30720)\n",
            "train: [279/391] Loss: 0.055 | Acc: 98.089% (35155/35840)\n",
            "train: [319/391] Loss: 0.055 | Acc: 98.103% (40183/40960)\n",
            "train: [359/391] Loss: 0.056 | Acc: 98.058% (45185/46080)\n",
            "val: [39/79] Loss: 0.488 | Acc: 88.613% (4537/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 255\n",
            "train: [39/391] Loss: 0.044 | Acc: 98.496% (5043/5120)\n",
            "train: [79/391] Loss: 0.044 | Acc: 98.477% (10084/10240)\n",
            "train: [119/391] Loss: 0.045 | Acc: 98.477% (15126/15360)\n",
            "train: [159/391] Loss: 0.046 | Acc: 98.418% (20156/20480)\n",
            "train: [199/391] Loss: 0.045 | Acc: 98.480% (25211/25600)\n",
            "train: [239/391] Loss: 0.044 | Acc: 98.490% (30256/30720)\n",
            "train: [279/391] Loss: 0.043 | Acc: 98.521% (35310/35840)\n",
            "train: [319/391] Loss: 0.044 | Acc: 98.513% (40351/40960)\n",
            "train: [359/391] Loss: 0.044 | Acc: 98.492% (45385/46080)\n",
            "val: [39/79] Loss: 0.477 | Acc: 89.609% (4588/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 256\n",
            "train: [39/391] Loss: 0.046 | Acc: 98.496% (5043/5120)\n",
            "train: [79/391] Loss: 0.040 | Acc: 98.613% (10098/10240)\n",
            "train: [119/391] Loss: 0.040 | Acc: 98.594% (15144/15360)\n",
            "train: [159/391] Loss: 0.038 | Acc: 98.657% (20205/20480)\n",
            "train: [199/391] Loss: 0.038 | Acc: 98.676% (25261/25600)\n",
            "train: [239/391] Loss: 0.038 | Acc: 98.698% (30320/30720)\n",
            "train: [279/391] Loss: 0.037 | Acc: 98.739% (35388/35840)\n",
            "train: [319/391] Loss: 0.037 | Acc: 98.743% (40445/40960)\n",
            "train: [359/391] Loss: 0.037 | Acc: 98.741% (45500/46080)\n",
            "val: [39/79] Loss: 0.496 | Acc: 89.316% (4573/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 257\n",
            "train: [39/391] Loss: 0.037 | Acc: 98.848% (5061/5120)\n",
            "train: [79/391] Loss: 0.036 | Acc: 98.896% (10127/10240)\n",
            "train: [119/391] Loss: 0.034 | Acc: 98.978% (15203/15360)\n",
            "train: [159/391] Loss: 0.033 | Acc: 98.989% (20273/20480)\n",
            "train: [199/391] Loss: 0.032 | Acc: 98.992% (25342/25600)\n",
            "train: [239/391] Loss: 0.032 | Acc: 99.004% (30414/30720)\n",
            "train: [279/391] Loss: 0.033 | Acc: 98.979% (35474/35840)\n",
            "train: [319/391] Loss: 0.031 | Acc: 99.031% (40563/40960)\n",
            "train: [359/391] Loss: 0.032 | Acc: 99.015% (45626/46080)\n",
            "val: [39/79] Loss: 0.501 | Acc: 89.336% (4574/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 258\n",
            "train: [39/391] Loss: 0.026 | Acc: 99.336% (5086/5120)\n",
            "train: [79/391] Loss: 0.028 | Acc: 99.170% (10155/10240)\n",
            "train: [119/391] Loss: 0.027 | Acc: 99.115% (15224/15360)\n",
            "train: [159/391] Loss: 0.028 | Acc: 99.067% (20289/20480)\n",
            "train: [199/391] Loss: 0.028 | Acc: 99.059% (25359/25600)\n",
            "train: [239/391] Loss: 0.028 | Acc: 99.017% (30418/30720)\n",
            "train: [279/391] Loss: 0.028 | Acc: 99.046% (35498/35840)\n",
            "train: [319/391] Loss: 0.028 | Acc: 99.062% (40576/40960)\n",
            "train: [359/391] Loss: 0.027 | Acc: 99.071% (45652/46080)\n",
            "val: [39/79] Loss: 0.505 | Acc: 89.785% (4597/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 259\n",
            "train: [39/391] Loss: 0.028 | Acc: 99.062% (5072/5120)\n",
            "train: [79/391] Loss: 0.024 | Acc: 99.209% (10159/10240)\n",
            "train: [119/391] Loss: 0.026 | Acc: 99.115% (15224/15360)\n",
            "train: [159/391] Loss: 0.025 | Acc: 99.146% (20305/20480)\n",
            "train: [199/391] Loss: 0.024 | Acc: 99.172% (25388/25600)\n",
            "train: [239/391] Loss: 0.024 | Acc: 99.173% (30466/30720)\n",
            "train: [279/391] Loss: 0.024 | Acc: 99.169% (35542/35840)\n",
            "train: [319/391] Loss: 0.024 | Acc: 99.165% (40618/40960)\n",
            "train: [359/391] Loss: 0.024 | Acc: 99.162% (45694/46080)\n",
            "val: [39/79] Loss: 0.498 | Acc: 89.844% (4600/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 260\n",
            "train: [39/391] Loss: 0.043 | Acc: 98.477% (5042/5120)\n",
            "train: [79/391] Loss: 0.058 | Acc: 97.998% (10035/10240)\n",
            "train: [119/391] Loss: 0.070 | Acc: 97.598% (14991/15360)\n",
            "train: [159/391] Loss: 0.076 | Acc: 97.358% (19939/20480)\n",
            "train: [199/391] Loss: 0.080 | Acc: 97.223% (24889/25600)\n",
            "train: [239/391] Loss: 0.081 | Acc: 97.171% (29851/30720)\n",
            "train: [279/391] Loss: 0.082 | Acc: 97.137% (34814/35840)\n",
            "train: [319/391] Loss: 0.083 | Acc: 97.092% (39769/40960)\n",
            "train: [359/391] Loss: 0.083 | Acc: 97.081% (44735/46080)\n",
            "val: [39/79] Loss: 0.498 | Acc: 87.930% (4502/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 261\n",
            "train: [39/391] Loss: 0.085 | Acc: 96.953% (4964/5120)\n",
            "train: [79/391] Loss: 0.084 | Acc: 97.041% (9937/10240)\n",
            "train: [119/391] Loss: 0.086 | Acc: 96.992% (14898/15360)\n",
            "train: [159/391] Loss: 0.084 | Acc: 97.085% (19883/20480)\n",
            "train: [199/391] Loss: 0.084 | Acc: 97.066% (24849/25600)\n",
            "train: [239/391] Loss: 0.082 | Acc: 97.119% (29835/30720)\n",
            "train: [279/391] Loss: 0.081 | Acc: 97.188% (34832/35840)\n",
            "train: [319/391] Loss: 0.080 | Acc: 97.209% (39817/40960)\n",
            "train: [359/391] Loss: 0.080 | Acc: 97.222% (44800/46080)\n",
            "val: [39/79] Loss: 0.481 | Acc: 88.574% (4535/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 262\n",
            "train: [39/391] Loss: 0.074 | Acc: 97.227% (4978/5120)\n",
            "train: [79/391] Loss: 0.071 | Acc: 97.373% (9971/10240)\n",
            "train: [119/391] Loss: 0.071 | Acc: 97.409% (14962/15360)\n",
            "train: [159/391] Loss: 0.071 | Acc: 97.393% (19946/20480)\n",
            "train: [199/391] Loss: 0.072 | Acc: 97.438% (24944/25600)\n",
            "train: [239/391] Loss: 0.073 | Acc: 97.422% (29928/30720)\n",
            "train: [279/391] Loss: 0.072 | Acc: 97.436% (34921/35840)\n",
            "train: [319/391] Loss: 0.073 | Acc: 97.454% (39917/40960)\n",
            "train: [359/391] Loss: 0.072 | Acc: 97.459% (44909/46080)\n",
            "val: [39/79] Loss: 0.483 | Acc: 88.066% (4509/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 263\n",
            "train: [39/391] Loss: 0.076 | Acc: 97.324% (4983/5120)\n",
            "train: [79/391] Loss: 0.075 | Acc: 97.383% (9972/10240)\n",
            "train: [119/391] Loss: 0.071 | Acc: 97.552% (14984/15360)\n",
            "train: [159/391] Loss: 0.073 | Acc: 97.432% (19954/20480)\n",
            "train: [199/391] Loss: 0.072 | Acc: 97.465% (24951/25600)\n",
            "train: [239/391] Loss: 0.070 | Acc: 97.546% (29966/30720)\n",
            "train: [279/391] Loss: 0.069 | Acc: 97.584% (34974/35840)\n",
            "train: [319/391] Loss: 0.069 | Acc: 97.559% (39960/40960)\n",
            "train: [359/391] Loss: 0.068 | Acc: 97.582% (44966/46080)\n",
            "val: [39/79] Loss: 0.457 | Acc: 88.926% (4553/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 264\n",
            "train: [39/391] Loss: 0.057 | Acc: 97.988% (5017/5120)\n",
            "train: [79/391] Loss: 0.051 | Acc: 98.213% (10057/10240)\n",
            "train: [119/391] Loss: 0.051 | Acc: 98.203% (15084/15360)\n",
            "train: [159/391] Loss: 0.053 | Acc: 98.203% (20112/20480)\n",
            "train: [199/391] Loss: 0.054 | Acc: 98.160% (25129/25600)\n",
            "train: [239/391] Loss: 0.054 | Acc: 98.138% (30148/30720)\n",
            "train: [279/391] Loss: 0.054 | Acc: 98.114% (35164/35840)\n",
            "train: [319/391] Loss: 0.054 | Acc: 98.105% (40184/40960)\n",
            "train: [359/391] Loss: 0.053 | Acc: 98.149% (45227/46080)\n",
            "val: [39/79] Loss: 0.484 | Acc: 89.375% (4576/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 265\n",
            "train: [39/391] Loss: 0.046 | Acc: 98.359% (5036/5120)\n",
            "train: [79/391] Loss: 0.048 | Acc: 98.320% (10068/10240)\n",
            "train: [119/391] Loss: 0.048 | Acc: 98.340% (15105/15360)\n",
            "train: [159/391] Loss: 0.047 | Acc: 98.384% (20149/20480)\n",
            "train: [199/391] Loss: 0.048 | Acc: 98.367% (25182/25600)\n",
            "train: [239/391] Loss: 0.048 | Acc: 98.333% (30208/30720)\n",
            "train: [279/391] Loss: 0.047 | Acc: 98.348% (35248/35840)\n",
            "train: [319/391] Loss: 0.046 | Acc: 98.403% (40306/40960)\n",
            "train: [359/391] Loss: 0.045 | Acc: 98.438% (45360/46080)\n",
            "val: [39/79] Loss: 0.486 | Acc: 89.492% (4582/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 266\n",
            "train: [39/391] Loss: 0.040 | Acc: 98.496% (5043/5120)\n",
            "train: [79/391] Loss: 0.037 | Acc: 98.672% (10104/10240)\n",
            "train: [119/391] Loss: 0.036 | Acc: 98.711% (15162/15360)\n",
            "train: [159/391] Loss: 0.035 | Acc: 98.789% (20232/20480)\n",
            "train: [199/391] Loss: 0.034 | Acc: 98.809% (25295/25600)\n",
            "train: [239/391] Loss: 0.034 | Acc: 98.805% (30353/30720)\n",
            "train: [279/391] Loss: 0.035 | Acc: 98.809% (35413/35840)\n",
            "train: [319/391] Loss: 0.035 | Acc: 98.809% (40472/40960)\n",
            "train: [359/391] Loss: 0.036 | Acc: 98.765% (45511/46080)\n",
            "val: [39/79] Loss: 0.484 | Acc: 89.336% (4574/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 267\n",
            "train: [39/391] Loss: 0.024 | Acc: 99.141% (5076/5120)\n",
            "train: [79/391] Loss: 0.025 | Acc: 99.160% (10154/10240)\n",
            "train: [119/391] Loss: 0.027 | Acc: 99.102% (15222/15360)\n",
            "train: [159/391] Loss: 0.028 | Acc: 99.053% (20286/20480)\n",
            "train: [199/391] Loss: 0.028 | Acc: 99.027% (25351/25600)\n",
            "train: [239/391] Loss: 0.028 | Acc: 99.020% (30419/30720)\n",
            "train: [279/391] Loss: 0.028 | Acc: 99.043% (35497/35840)\n",
            "train: [319/391] Loss: 0.028 | Acc: 99.055% (40573/40960)\n",
            "train: [359/391] Loss: 0.028 | Acc: 99.058% (45646/46080)\n",
            "val: [39/79] Loss: 0.496 | Acc: 89.629% (4589/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 268\n",
            "train: [39/391] Loss: 0.025 | Acc: 99.219% (5080/5120)\n",
            "train: [79/391] Loss: 0.026 | Acc: 99.111% (10149/10240)\n",
            "train: [119/391] Loss: 0.027 | Acc: 99.108% (15223/15360)\n",
            "train: [159/391] Loss: 0.027 | Acc: 99.102% (20296/20480)\n",
            "train: [199/391] Loss: 0.027 | Acc: 99.094% (25368/25600)\n",
            "train: [239/391] Loss: 0.026 | Acc: 99.111% (30447/30720)\n",
            "train: [279/391] Loss: 0.027 | Acc: 99.093% (35515/35840)\n",
            "train: [319/391] Loss: 0.026 | Acc: 99.111% (40596/40960)\n",
            "train: [359/391] Loss: 0.026 | Acc: 99.136% (45682/46080)\n",
            "val: [39/79] Loss: 0.503 | Acc: 89.805% (4598/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 269\n",
            "train: [39/391] Loss: 0.025 | Acc: 99.121% (5075/5120)\n",
            "train: [79/391] Loss: 0.024 | Acc: 99.199% (10158/10240)\n",
            "train: [119/391] Loss: 0.025 | Acc: 99.193% (15236/15360)\n",
            "train: [159/391] Loss: 0.025 | Acc: 99.194% (20315/20480)\n",
            "train: [199/391] Loss: 0.024 | Acc: 99.199% (25395/25600)\n",
            "train: [239/391] Loss: 0.024 | Acc: 99.225% (30482/30720)\n",
            "train: [279/391] Loss: 0.023 | Acc: 99.222% (35561/35840)\n",
            "train: [319/391] Loss: 0.023 | Acc: 99.238% (40648/40960)\n",
            "train: [359/391] Loss: 0.023 | Acc: 99.238% (45729/46080)\n",
            "val: [39/79] Loss: 0.500 | Acc: 89.727% (4594/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 270\n",
            "train: [39/391] Loss: 0.035 | Acc: 98.906% (5064/5120)\n",
            "train: [79/391] Loss: 0.050 | Acc: 98.262% (10062/10240)\n",
            "train: [119/391] Loss: 0.063 | Acc: 97.793% (15021/15360)\n",
            "train: [159/391] Loss: 0.069 | Acc: 97.593% (19987/20480)\n",
            "train: [199/391] Loss: 0.073 | Acc: 97.457% (24949/25600)\n",
            "train: [239/391] Loss: 0.076 | Acc: 97.344% (29904/30720)\n",
            "train: [279/391] Loss: 0.076 | Acc: 97.352% (34891/35840)\n",
            "train: [319/391] Loss: 0.076 | Acc: 97.307% (39857/40960)\n",
            "train: [359/391] Loss: 0.077 | Acc: 97.250% (44813/46080)\n",
            "val: [39/79] Loss: 0.493 | Acc: 88.418% (4527/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 271\n",
            "train: [39/391] Loss: 0.074 | Acc: 97.383% (4986/5120)\n",
            "train: [79/391] Loss: 0.077 | Acc: 97.324% (9966/10240)\n",
            "train: [119/391] Loss: 0.075 | Acc: 97.324% (14949/15360)\n",
            "train: [159/391] Loss: 0.076 | Acc: 97.266% (19920/20480)\n",
            "train: [199/391] Loss: 0.077 | Acc: 97.211% (24886/25600)\n",
            "train: [239/391] Loss: 0.078 | Acc: 97.214% (29864/30720)\n",
            "train: [279/391] Loss: 0.076 | Acc: 97.277% (34864/35840)\n",
            "train: [319/391] Loss: 0.076 | Acc: 97.273% (39843/40960)\n",
            "train: [359/391] Loss: 0.077 | Acc: 97.244% (44810/46080)\n",
            "val: [39/79] Loss: 0.473 | Acc: 88.789% (4546/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 272\n",
            "train: [39/391] Loss: 0.066 | Acc: 97.812% (5008/5120)\n",
            "train: [79/391] Loss: 0.062 | Acc: 97.910% (10026/10240)\n",
            "train: [119/391] Loss: 0.064 | Acc: 97.917% (15040/15360)\n",
            "train: [159/391] Loss: 0.063 | Acc: 97.891% (20048/20480)\n",
            "train: [199/391] Loss: 0.066 | Acc: 97.801% (25037/25600)\n",
            "train: [239/391] Loss: 0.066 | Acc: 97.780% (30038/30720)\n",
            "train: [279/391] Loss: 0.067 | Acc: 97.718% (35022/35840)\n",
            "train: [319/391] Loss: 0.068 | Acc: 97.688% (40013/40960)\n",
            "train: [359/391] Loss: 0.067 | Acc: 97.737% (45037/46080)\n",
            "val: [39/79] Loss: 0.473 | Acc: 89.355% (4575/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 273\n",
            "train: [39/391] Loss: 0.060 | Acc: 97.969% (5016/5120)\n",
            "train: [79/391] Loss: 0.057 | Acc: 97.920% (10027/10240)\n",
            "train: [119/391] Loss: 0.058 | Acc: 97.904% (15038/15360)\n",
            "train: [159/391] Loss: 0.057 | Acc: 97.974% (20065/20480)\n",
            "train: [199/391] Loss: 0.058 | Acc: 97.973% (25081/25600)\n",
            "train: [239/391] Loss: 0.057 | Acc: 97.995% (30104/30720)\n",
            "train: [279/391] Loss: 0.057 | Acc: 98.008% (35126/35840)\n",
            "train: [319/391] Loss: 0.057 | Acc: 98.005% (40143/40960)\n",
            "train: [359/391] Loss: 0.059 | Acc: 97.945% (45133/46080)\n",
            "val: [39/79] Loss: 0.490 | Acc: 88.984% (4556/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 274\n",
            "train: [39/391] Loss: 0.055 | Acc: 97.988% (5017/5120)\n",
            "train: [79/391] Loss: 0.055 | Acc: 98.047% (10040/10240)\n",
            "train: [119/391] Loss: 0.055 | Acc: 98.112% (15070/15360)\n",
            "train: [159/391] Loss: 0.054 | Acc: 98.149% (20101/20480)\n",
            "train: [199/391] Loss: 0.055 | Acc: 98.109% (25116/25600)\n",
            "train: [239/391] Loss: 0.054 | Acc: 98.177% (30160/30720)\n",
            "train: [279/391] Loss: 0.054 | Acc: 98.150% (35177/35840)\n",
            "train: [319/391] Loss: 0.053 | Acc: 98.162% (40207/40960)\n",
            "train: [359/391] Loss: 0.053 | Acc: 98.155% (45230/46080)\n",
            "val: [39/79] Loss: 0.496 | Acc: 88.691% (4541/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 275\n",
            "train: [39/391] Loss: 0.049 | Acc: 98.301% (5033/5120)\n",
            "train: [79/391] Loss: 0.047 | Acc: 98.359% (10072/10240)\n",
            "train: [119/391] Loss: 0.045 | Acc: 98.372% (15110/15360)\n",
            "train: [159/391] Loss: 0.044 | Acc: 98.398% (20152/20480)\n",
            "train: [199/391] Loss: 0.045 | Acc: 98.363% (25181/25600)\n",
            "train: [239/391] Loss: 0.044 | Acc: 98.398% (30228/30720)\n",
            "train: [279/391] Loss: 0.044 | Acc: 98.415% (35272/35840)\n",
            "train: [319/391] Loss: 0.043 | Acc: 98.491% (40342/40960)\n",
            "train: [359/391] Loss: 0.041 | Acc: 98.546% (45410/46080)\n",
            "val: [39/79] Loss: 0.500 | Acc: 89.316% (4573/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 276\n",
            "train: [39/391] Loss: 0.032 | Acc: 98.906% (5064/5120)\n",
            "train: [79/391] Loss: 0.033 | Acc: 98.809% (10118/10240)\n",
            "train: [119/391] Loss: 0.033 | Acc: 98.854% (15184/15360)\n",
            "train: [159/391] Loss: 0.033 | Acc: 98.867% (20248/20480)\n",
            "train: [199/391] Loss: 0.035 | Acc: 98.824% (25299/25600)\n",
            "train: [239/391] Loss: 0.034 | Acc: 98.848% (30366/30720)\n",
            "train: [279/391] Loss: 0.035 | Acc: 98.853% (35429/35840)\n",
            "train: [319/391] Loss: 0.034 | Acc: 98.867% (40496/40960)\n",
            "train: [359/391] Loss: 0.034 | Acc: 98.887% (45567/46080)\n",
            "val: [39/79] Loss: 0.494 | Acc: 89.492% (4582/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 277\n",
            "train: [39/391] Loss: 0.033 | Acc: 99.023% (5070/5120)\n",
            "train: [79/391] Loss: 0.029 | Acc: 99.014% (10139/10240)\n",
            "train: [119/391] Loss: 0.029 | Acc: 99.017% (15209/15360)\n",
            "train: [159/391] Loss: 0.029 | Acc: 99.009% (20277/20480)\n",
            "train: [199/391] Loss: 0.030 | Acc: 98.973% (25337/25600)\n",
            "train: [239/391] Loss: 0.030 | Acc: 98.971% (30404/30720)\n",
            "train: [279/391] Loss: 0.029 | Acc: 98.996% (35480/35840)\n",
            "train: [319/391] Loss: 0.029 | Acc: 98.994% (40548/40960)\n",
            "train: [359/391] Loss: 0.029 | Acc: 99.026% (45631/46080)\n",
            "val: [39/79] Loss: 0.495 | Acc: 89.492% (4582/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 278\n",
            "train: [39/391] Loss: 0.027 | Acc: 99.121% (5075/5120)\n",
            "train: [79/391] Loss: 0.023 | Acc: 99.229% (10161/10240)\n",
            "train: [119/391] Loss: 0.022 | Acc: 99.271% (15248/15360)\n",
            "train: [159/391] Loss: 0.023 | Acc: 99.272% (20331/20480)\n",
            "train: [199/391] Loss: 0.024 | Acc: 99.219% (25400/25600)\n",
            "train: [239/391] Loss: 0.024 | Acc: 99.209% (30477/30720)\n",
            "train: [279/391] Loss: 0.023 | Acc: 99.210% (35557/35840)\n",
            "train: [319/391] Loss: 0.023 | Acc: 99.211% (40637/40960)\n",
            "train: [359/391] Loss: 0.023 | Acc: 99.232% (45726/46080)\n",
            "val: [39/79] Loss: 0.499 | Acc: 89.551% (4585/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 279\n",
            "train: [39/391] Loss: 0.025 | Acc: 99.238% (5081/5120)\n",
            "train: [79/391] Loss: 0.023 | Acc: 99.287% (10167/10240)\n",
            "train: [119/391] Loss: 0.023 | Acc: 99.212% (15239/15360)\n",
            "train: [159/391] Loss: 0.024 | Acc: 99.219% (20320/20480)\n",
            "train: [199/391] Loss: 0.024 | Acc: 99.230% (25403/25600)\n",
            "train: [239/391] Loss: 0.023 | Acc: 99.251% (30490/30720)\n",
            "train: [279/391] Loss: 0.024 | Acc: 99.216% (35559/35840)\n",
            "train: [319/391] Loss: 0.024 | Acc: 99.231% (40645/40960)\n",
            "train: [359/391] Loss: 0.024 | Acc: 99.225% (45723/46080)\n",
            "val: [39/79] Loss: 0.493 | Acc: 89.648% (4590/5120)\n",
            "Learing rate:  0.001\n",
            "\n",
            "Epoch: 280\n",
            "train: [39/391] Loss: 0.031 | Acc: 98.848% (5061/5120)\n",
            "train: [79/391] Loss: 0.039 | Acc: 98.623% (10099/10240)\n",
            "train: [119/391] Loss: 0.046 | Acc: 98.379% (15111/15360)\n",
            "train: [159/391] Loss: 0.055 | Acc: 98.032% (20077/20480)\n",
            "train: [199/391] Loss: 0.060 | Acc: 97.863% (25053/25600)\n",
            "train: [239/391] Loss: 0.064 | Acc: 97.728% (30022/30720)\n",
            "train: [279/391] Loss: 0.067 | Acc: 97.637% (34993/35840)\n",
            "train: [319/391] Loss: 0.068 | Acc: 97.600% (39977/40960)\n",
            "train: [359/391] Loss: 0.070 | Acc: 97.528% (44941/46080)\n",
            "val: [39/79] Loss: 0.476 | Acc: 88.359% (4524/5120)\n",
            "Learing rate:  0.0009757729755661011\n",
            "\n",
            "Epoch: 281\n",
            "train: [39/391] Loss: 0.065 | Acc: 97.617% (4998/5120)\n",
            "train: [79/391] Loss: 0.074 | Acc: 97.539% (9988/10240)\n",
            "train: [119/391] Loss: 0.077 | Acc: 97.428% (14965/15360)\n",
            "train: [159/391] Loss: 0.074 | Acc: 97.554% (19979/20480)\n",
            "train: [199/391] Loss: 0.073 | Acc: 97.605% (24987/25600)\n",
            "train: [239/391] Loss: 0.072 | Acc: 97.594% (29981/30720)\n",
            "train: [279/391] Loss: 0.072 | Acc: 97.575% (34971/35840)\n",
            "train: [319/391] Loss: 0.072 | Acc: 97.554% (39958/40960)\n",
            "train: [359/391] Loss: 0.073 | Acc: 97.511% (44933/46080)\n",
            "val: [39/79] Loss: 0.492 | Acc: 88.652% (4539/5120)\n",
            "Learing rate:  0.000905463412215599\n",
            "\n",
            "Epoch: 282\n",
            "train: [39/391] Loss: 0.059 | Acc: 97.930% (5014/5120)\n",
            "train: [79/391] Loss: 0.058 | Acc: 97.920% (10027/10240)\n",
            "train: [119/391] Loss: 0.059 | Acc: 97.943% (15044/15360)\n",
            "train: [159/391] Loss: 0.060 | Acc: 97.876% (20045/20480)\n",
            "train: [199/391] Loss: 0.061 | Acc: 97.848% (25049/25600)\n",
            "train: [239/391] Loss: 0.060 | Acc: 97.865% (30064/30720)\n",
            "train: [279/391] Loss: 0.061 | Acc: 97.835% (35064/35840)\n",
            "train: [319/391] Loss: 0.062 | Acc: 97.793% (40056/40960)\n",
            "train: [359/391] Loss: 0.062 | Acc: 97.767% (45051/46080)\n",
            "val: [39/79] Loss: 0.480 | Acc: 89.004% (4557/5120)\n",
            "Learing rate:  0.0007959536998847742\n",
            "\n",
            "Epoch: 283\n",
            "train: [39/391] Loss: 0.053 | Acc: 98.027% (5019/5120)\n",
            "train: [79/391] Loss: 0.058 | Acc: 97.979% (10033/10240)\n",
            "train: [119/391] Loss: 0.055 | Acc: 98.079% (15065/15360)\n",
            "train: [159/391] Loss: 0.056 | Acc: 98.096% (20090/20480)\n",
            "train: [199/391] Loss: 0.067 | Acc: 97.742% (25022/25600)\n",
            "train: [239/391] Loss: 0.070 | Acc: 97.630% (29992/30720)\n",
            "train: [279/391] Loss: 0.070 | Acc: 97.598% (34979/35840)\n",
            "train: [319/391] Loss: 0.071 | Acc: 97.534% (39950/40960)\n",
            "train: [359/391] Loss: 0.071 | Acc: 97.554% (44953/46080)\n",
            "val: [39/79] Loss: 0.453 | Acc: 89.160% (4565/5120)\n",
            "Learing rate:  0.000657963412215599\n",
            "\n",
            "Epoch: 284\n",
            "train: [39/391] Loss: 0.054 | Acc: 98.340% (5035/5120)\n",
            "train: [79/391] Loss: 0.057 | Acc: 98.096% (10045/10240)\n",
            "train: [119/391] Loss: 0.053 | Acc: 98.210% (15085/15360)\n",
            "train: [159/391] Loss: 0.053 | Acc: 98.218% (20115/20480)\n",
            "train: [199/391] Loss: 0.053 | Acc: 98.191% (25137/25600)\n",
            "train: [239/391] Loss: 0.052 | Acc: 98.232% (30177/30720)\n",
            "train: [279/391] Loss: 0.051 | Acc: 98.245% (35211/35840)\n",
            "train: [319/391] Loss: 0.051 | Acc: 98.237% (40238/40960)\n",
            "train: [359/391] Loss: 0.051 | Acc: 98.216% (45258/46080)\n",
            "val: [39/79] Loss: 0.472 | Acc: 89.551% (4585/5120)\n",
            "Learing rate:  0.000505\n",
            "\n",
            "Epoch: 285\n",
            "train: [39/391] Loss: 0.040 | Acc: 98.652% (5051/5120)\n",
            "train: [79/391] Loss: 0.041 | Acc: 98.623% (10099/10240)\n",
            "train: [119/391] Loss: 0.041 | Acc: 98.639% (15151/15360)\n",
            "train: [159/391] Loss: 0.039 | Acc: 98.701% (20214/20480)\n",
            "train: [199/391] Loss: 0.040 | Acc: 98.668% (25259/25600)\n",
            "train: [239/391] Loss: 0.040 | Acc: 98.662% (30309/30720)\n",
            "train: [279/391] Loss: 0.040 | Acc: 98.652% (35357/35840)\n",
            "train: [319/391] Loss: 0.040 | Acc: 98.630% (40399/40960)\n",
            "train: [359/391] Loss: 0.040 | Acc: 98.663% (45464/46080)\n",
            "val: [39/79] Loss: 0.492 | Acc: 89.805% (4598/5120)\n",
            "Learing rate:  0.0003520365877844011\n",
            "\n",
            "Epoch: 286\n",
            "train: [39/391] Loss: 0.032 | Acc: 99.082% (5073/5120)\n",
            "train: [79/391] Loss: 0.032 | Acc: 98.994% (10137/10240)\n",
            "train: [119/391] Loss: 0.033 | Acc: 98.913% (15193/15360)\n",
            "train: [159/391] Loss: 0.033 | Acc: 98.887% (20252/20480)\n",
            "train: [199/391] Loss: 0.033 | Acc: 98.852% (25306/25600)\n",
            "train: [239/391] Loss: 0.034 | Acc: 98.828% (30360/30720)\n",
            "train: [279/391] Loss: 0.034 | Acc: 98.825% (35419/35840)\n",
            "train: [319/391] Loss: 0.035 | Acc: 98.813% (40474/40960)\n",
            "train: [359/391] Loss: 0.034 | Acc: 98.824% (45538/46080)\n",
            "val: [39/79] Loss: 0.488 | Acc: 89.688% (4592/5120)\n",
            "Learing rate:  0.00021404630011522585\n",
            "\n",
            "Epoch: 287\n",
            "train: [39/391] Loss: 0.028 | Acc: 99.004% (5069/5120)\n",
            "train: [79/391] Loss: 0.026 | Acc: 99.121% (10150/10240)\n",
            "train: [119/391] Loss: 0.026 | Acc: 99.115% (15224/15360)\n",
            "train: [159/391] Loss: 0.027 | Acc: 99.092% (20294/20480)\n",
            "train: [199/391] Loss: 0.028 | Acc: 99.078% (25364/25600)\n",
            "train: [239/391] Loss: 0.028 | Acc: 99.066% (30433/30720)\n",
            "train: [279/391] Loss: 0.028 | Acc: 99.074% (35508/35840)\n",
            "train: [319/391] Loss: 0.028 | Acc: 99.067% (40578/40960)\n",
            "train: [359/391] Loss: 0.028 | Acc: 99.069% (45651/46080)\n",
            "val: [39/79] Loss: 0.498 | Acc: 89.980% (4607/5120)\n",
            "Learing rate:  0.00010453658778440107\n",
            "\n",
            "Epoch: 288\n",
            "train: [39/391] Loss: 0.021 | Acc: 99.297% (5084/5120)\n",
            "train: [79/391] Loss: 0.024 | Acc: 99.180% (10156/10240)\n",
            "train: [119/391] Loss: 0.024 | Acc: 99.173% (15233/15360)\n",
            "train: [159/391] Loss: 0.023 | Acc: 99.185% (20313/20480)\n",
            "train: [199/391] Loss: 0.024 | Acc: 99.180% (25390/25600)\n",
            "train: [239/391] Loss: 0.024 | Acc: 99.189% (30471/30720)\n",
            "train: [279/391] Loss: 0.023 | Acc: 99.216% (35559/35840)\n",
            "train: [319/391] Loss: 0.023 | Acc: 99.211% (40637/40960)\n",
            "train: [359/391] Loss: 0.023 | Acc: 99.230% (45725/46080)\n",
            "val: [39/79] Loss: 0.498 | Acc: 90.117% (4614/5120)\n",
            "Learing rate:  3.4227024433899005e-05\n",
            "\n",
            "Epoch: 289\n",
            "train: [39/391] Loss: 0.026 | Acc: 99.062% (5072/5120)\n",
            "train: [79/391] Loss: 0.023 | Acc: 99.248% (10163/10240)\n",
            "train: [119/391] Loss: 0.022 | Acc: 99.310% (15254/15360)\n",
            "train: [159/391] Loss: 0.022 | Acc: 99.312% (20339/20480)\n",
            "train: [199/391] Loss: 0.021 | Acc: 99.348% (25433/25600)\n",
            "train: [239/391] Loss: 0.021 | Acc: 99.355% (30522/30720)\n",
            "train: [279/391] Loss: 0.020 | Acc: 99.355% (35609/35840)\n",
            "train: [319/391] Loss: 0.021 | Acc: 99.319% (40681/40960)\n",
            "train: [359/391] Loss: 0.021 | Acc: 99.312% (45763/46080)\n",
            "val: [39/79] Loss: 0.496 | Acc: 90.391% (4628/5120)\n",
            "Saving..\n",
            ".pth saved\n",
            "Learing rate:  0.001\n",
            "best accurary 90.530000%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms         \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "batch_size = 128\n",
        "root_dir = 'drive/app/cifar10/'\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)),   \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
        "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
        "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
        "])\n",
        "\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root=root_dir,\n",
        "                                        train=True, \n",
        "                                        download=True, \n",
        "                                        transform=transform_train)\n",
        "testset = torchvision.datasets.CIFAR10(root=root_dir,\n",
        "                                       train=False, \n",
        "                                       download=True, \n",
        "                                       transform=transform_test)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=True, \n",
        "                                          num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(testset, \n",
        "                                         batch_size=batch_size, \n",
        "                                         shuffle=False, \n",
        "                                         num_workers=2)\n",
        "\n",
        "#classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "#def sigmoid(x):\n",
        "#    return 1 / (1 + np.exp(-x))\n",
        "#def Swish(x):\n",
        "#    return x * sigmoid(x)\n",
        "class Swish(nn.Module):\n",
        "    def __init__(self, inplace: bool=False):\n",
        "        super().__init__()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.inplace=inplace\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.sigmoid(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    '''expand + depthwise + pointwise + squeeze-excitation'''\n",
        "\n",
        "    def __init__(self, in_planes, out_planes, expansion, stride):\n",
        "        super(Block, self).__init__()\n",
        "        self.stride = stride\n",
        "\n",
        "        planes = expansion * in_planes\n",
        "        self.conv1 = nn.Conv2d( in_planes, planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, groups=planes, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d( planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_planes)\n",
        "        self.swish=Swish(inplace=True)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride == 1 and in_planes != out_planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, out_planes, kernel_size=1,\n",
        "                          stride=1, padding=0, bias=False),\n",
        "                nn.BatchNorm2d(out_planes),\n",
        "            )\n",
        "\n",
        "        # SE layers\n",
        "        self.fc1 = nn.Conv2d(out_planes, out_planes//16, kernel_size=1)\n",
        "        self.fc2 = nn.Conv2d(out_planes//16, out_planes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #out = F.relu(self.bn1(self.conv1(x)))\n",
        "        #out = F.gelu(self.bn1(self.conv1(x)))\n",
        "        #out = nn.SiLU(self.bn1(self.conv1(x)))\n",
        "        out = self.swish(self.bn1(self.conv1(x)))\n",
        "        #out = F.relu(self.bn2(self.conv2(out)))\n",
        "        #out = F.gelu(self.bn2(self.conv2(out)))\n",
        "        out = self.swish(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        shortcut = self.shortcut(x) if self.stride == 1 else out\n",
        "        # Squeeze-Excitation\n",
        "        w = F.avg_pool2d(out, out.size(2))\n",
        "        #w = F.relu(self.fc1(w))\n",
        "        #w = F.gelu(self.fc1(w))\n",
        "        w = self.swish(self.fc1(w))\n",
        "        w = self.fc2(w).sigmoid()\n",
        "        out = out * w + shortcut\n",
        "        return out\n",
        "\n",
        "\n",
        "class EfficientNet(nn.Module):\n",
        "    def __init__(self, cfg, num_classes=10):\n",
        "        super(EfficientNet, self).__init__()\n",
        "        self.cfg = cfg\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.layers = self._make_layers(in_planes=32)\n",
        "        self.linear = nn.Linear(cfg[-1][1], num_classes)\n",
        "        self.swish=Swish(inplace=True)\n",
        "\n",
        "    def _make_layers(self, in_planes):\n",
        "        layers = []\n",
        "        for expansion, out_planes, num_blocks, stride in self.cfg:\n",
        "            strides = [stride] + [1]*(num_blocks-1)\n",
        "            for stride in strides:\n",
        "                layers.append(Block(in_planes, out_planes, expansion, stride))\n",
        "                in_planes = out_planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #out = F.relu(self.bn1(self.conv1(x)))\n",
        "        #out = F.gelu(self.bn1(self.conv1(x)))\n",
        "        out = self.swish(self.bn1(self.conv1(x)))\n",
        "        out = self.layers(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def EfficientNetB0():\n",
        "    # (expansion, out_planes, num_blocks, stride)\n",
        "    cfg = [(1,  16, 1, 2),\n",
        "           (6,  24, 2, 1),\n",
        "           (6,  40, 2, 2),\n",
        "           (6,  80, 3, 2),\n",
        "           (6, 112, 3, 1),\n",
        "           (6, 192, 4, 2),\n",
        "           (6, 320, 1, 2)]\n",
        "    return EfficientNet(cfg)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "net = EfficientNetB0()\n",
        "net = net.to(device)\n",
        "# Load checkpoint.\n",
        "#assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
        "best_acc = 0.0\n",
        "start_epoch = 0\n",
        "if os.path.exists('./checkpoint/EfficientNet.pth'):\n",
        "    checkpoint = torch.load('./checkpoint/EfficientNet.pth')\n",
        "    net.load_state_dict(checkpoint['net'])\n",
        "    best_acc = checkpoint['acc']\n",
        "    start_epoch = checkpoint['epoch']\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9, weight_decay=5e-4)\n",
        "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
        "\n",
        "#scheduler = lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.1)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1, eta_min=0.00001)\n",
        "\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        if batch_idx % 40 == 39:\n",
        "            print('train: [%d/%d] Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                % (batch_idx, len(trainloader), train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "def test(epoch):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            if batch_idx % 40 == 39:\n",
        "                print('val: [%d/%d] Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                    % (batch_idx, len(testloader), test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "    # Save checkpoint.\n",
        "    acc = 100.*correct/total\n",
        "    if acc > best_acc:\n",
        "        print('Saving..')\n",
        "        state = {\n",
        "            'net': net.state_dict(),\n",
        "            'acc': acc,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        if not os.path.isdir('checkpoint'):\n",
        "            os.mkdir('checkpoint')\n",
        "        torch.save(state, './checkpoint/EfficientNet.pth')\n",
        "        print(\".pth saved\")\n",
        "        best_acc = acc\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "#for epoch in range(start_epoch, start_epoch+1):\n",
        "for epoch in range(start_epoch, 290):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    scheduler.step()\n",
        "    print(\"Learing rate: \",get_lr(optimizer))\n",
        "\n",
        "print(\"best accurary %f%%\"%best_acc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxQRM8FuVAjE",
        "outputId": "e55a38a2-0e11-4816-a96d-570ca97da31c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Colab_Test\n",
        "!git clone https://Terizea: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@github.com/Terizea/EfficientNet-CIFAR10.git ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhshldTxiGUL",
        "outputId": "4e18c89e-8482-41f5-f414-ad4061e41cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab_Test\n",
            "Cloning into '.'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (3/3), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Colab_Test/\n",
        "!git add EfficientNet_test1_90.53_backup"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_CfwelAlDYm",
        "outputId": "69ef5295-ac7c-44e1-9767-53a48207796e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n",
            "/content/drive/MyDrive/Colab_Test\n",
            "fatal: not a git repository (or any parent up to mount point /content)\n",
            "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd\n",
        "!git add EfficientNet_test1_90.53_backup"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyPvpNgJmXvj",
        "outputId": "1e8f6c55-53b0-450c-e5f1-7d57683bee35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any parent up to mount point /content)\n",
            "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "EfficientNet_test1_90.53의 사본",
      "provenance": [],
      "mount_file_id": "1obbYrtNNZ514UUux2fmAEks1rZaP-9Kj",
      "authorship_tag": "ABX9TyMPcpfzsl3bVNPivLBpNSUG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "910d1f54b70c4d50a114c92154e675ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b7a80f95dde54de698d00ac1c9ac2dad",
              "IPY_MODEL_6c16c16634c4443b929f6017b9a904db",
              "IPY_MODEL_2a461138a32345b687b4760a5a060356"
            ],
            "layout": "IPY_MODEL_cc0279d3c53c40b7933b3fcc6aba975f"
          }
        },
        "b7a80f95dde54de698d00ac1c9ac2dad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95c67122963e43a8a3c2453b9497cefe",
            "placeholder": "​",
            "style": "IPY_MODEL_ee4d99d2af2c405e8c536f4fd293dde2",
            "value": ""
          }
        },
        "6c16c16634c4443b929f6017b9a904db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58d592630acd4c66852b676f54ced6e5",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c91995eac94a4f7eab8995b06449ad07",
            "value": 170498071
          }
        },
        "2a461138a32345b687b4760a5a060356": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fa7a98e7f484eb2b3dd442ae4685197",
            "placeholder": "​",
            "style": "IPY_MODEL_39acdd793f4443aaa25bf6ab7ea98d90",
            "value": " 170499072/? [00:02&lt;00:00, 83820752.23it/s]"
          }
        },
        "cc0279d3c53c40b7933b3fcc6aba975f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95c67122963e43a8a3c2453b9497cefe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee4d99d2af2c405e8c536f4fd293dde2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58d592630acd4c66852b676f54ced6e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c91995eac94a4f7eab8995b06449ad07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9fa7a98e7f484eb2b3dd442ae4685197": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39acdd793f4443aaa25bf6ab7ea98d90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}